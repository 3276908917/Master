\chapter{Discussion and Conclusion}
\label{chap: disc_and_conc}

\section{Tightness of the Priors Used}
\label{sec: prior_woes}

With this section I would like to revisit the specific values for the priors,
that I only briefly mentioned back in section~\ref{sec: build_lhc}.

First of all, from a purely practical consideration, expanding the priors was
not feasible due to the high incidence of unsolvable cells.

But second, this may not be a significant limitation to the utility of the
emulators introduced here, because they are already quite wide compared to
current state-of-the-art parameter inferences. \textcolor{green}{CITATIONS}.

As of 19.06.23, ``COMET'' is the default for the emulator. It is the most 
restrictive of the three options and was implemented in order to totally 
eliminate the problem of unsolvable cells, allowing us to train our 
demonstration emulator over an LHS without any significant gaps.

Our hope was that the narrow parameter ranges would furthermore help the 
demonstration emulator to achieve high accuracy--in principal, success here
means that we can simply ``scale up'' the approach of this work by
simultaneously expanding the priors as well as the total number of training
samples. Unfortunately, it is not clear if we can scale up the emulator past
the point at which unsolvable cells begin to appear. Since Latin hypercube
sampling is designed to evenly sample a space, unsolvable cells certainly
indicate that parts of the parameter space lack representation in the training
data. In these regions, our emulator will be forced to interpolate across
large gaps, or worse, extrapolate (if the unsolvable cells occur at the edges
of the parameter space \textcolor{orange}{This is something that I should have
shown... i.e. with plots}).
% Andrea recommends a plot coloring points by the "extremeness" in our sample
% space.

We predict that increasing the total number of cells will only marginally
reduce the issue of unsolvable cells \textcolor{orange}{This is something that 
I should have shown... i.e. with plots}). We can imagine the subspace of
solvable points as some hypervolume within a hypercube determined by our
priors, and the emulator's training coverage as an approximation of this 
hypervolume with small hypercubes whose size is determined by the separation 
between points in the sample. In the ideal case, the space of solvable points 
is the same as the Latin hypercube. When this is not so, we can at least
reduce the error associated with our approximation of the space of solvable
points by shrinking the hypercubes we use in our approximation (i.e. by
increasing the total number of points in our sample). \textcolor{orange}{
To give a sense of the marginal nature of this error reduction, we can
consider how small our hypercubes already are. For simplicity, let's examine
just one axis of the hypercube. With 5000 samples in the ``MEGA'' priors,
the length of the training coverage MOST STRONGLY DETERMINED BY ONE HYPERCUBE
IS: UNFINISHED THOUGHT}

It seems reasonable to think that unsolvable cells indicate extreme regions of 
the parameter space, rather than isolated holes. Therefore, it would be 
misleading to claim that the final ``MEGA'' emulator corresponds to, for 
example, any prior ranges in table 00A; in truth, the emulator would 
correspond to a potentially (this is a dangerous word and opens you up to hard 
questions) complicated shape inscribed within the six-dimensional rectangular 
hyperprism.

CAMB does not allow for negative redshifts, although it prove be 
interesting to try to resolve this issue with a code that does allow negative 
redshifts, such as CLASS. \textcolor{orange}{Re-iterate the discussion from
section~\ref{sec: boltzmann_intro}: why didnâ€™t we use CLASS for this project?}

\section{Minimum Separation of the Training LHC}
\label{sec: error_from_lhc}

What is the impact of the minimum separation? Surely the minimum separation
should be a proxy for the evenness of the coverage of the space of
cosmologies. Therefore, we expect the error variance to increase much more
dramatically than, say, the average bias.

How would we
be able to quantify the error due to this? We could try to compare the
emulator performance trained on hyper cubes of various minimum distances.

\section{Resolution of the k Axis}

This might go better in the CassL section, but I think I ought to motivate the 
decision to use length-300 arrays.

\section{Number of Training Samples}
\label{sec: num_samples}

\textcolor{blue}{Justify choice of 5000 samples for each: maybe we can make a
trend plot showing diminishing returns in test error?}

% This might go better in the CassL section, but I think I ought to motivate 
% the decision to use 5000 training arrays.

\textcolor{orange}{I'll have to concede that the results of this section are 
not entirely comprehensive; we didn't train any emulators over the 
uncertainties of analogous validation hypercubes. All comparisons here use the 
simpler pipeline of just two data sets, training and testing.}


\section{Linear Sampling in Different Parameters}

We also tried sampling in $\sigma_{12}^2$ as well as $\sqrt{\sigma_{12}}$.
Unfortunately, we were unable to conclude anything about the effectiveness of
these strategies--there appears to have been some mistake in our code, such
that the errors are much larger than can be explained on account of poor
sampling.

See figures~\ref{fig: sigsquare_sample} and~\ref{fig: sigroot_sample} for
illustrations of the problem. In a future work, it would be helpful to
investigate these problems further. We may find that a different sampling
strategy will more efficiently reduce the deltas that we see in our emulator.

\section{Summary of the Paper}

What was the main objective of this thesis? What were the key results of this 
work? Why are they important? How do these results compare with results from 
papers on similar subjects?

\section{Emulation over Uncertainties}

\textcolor{blue}{This content should be subsumed into future work, since we
didn't manage to complete this feature in time.}

For a further step of accuracy, we can add a third data set to our pipeline
and introduce a second layer of emulation.

Up to this point, our pipeline has included a training set and a testing set.
If we add a validation set, then we can train a second emulator over the
errors associated with the first emulator's performance on this validation
set.

Within the current (as of \textcolor{orange}{24.08.2023}) setup of
Cassandra-Linear, we typically generate two Latin hypercubes for each
emulator. The first represents our training set and an emulator cannot be
produced without it. The second one represents our validation set.

\textcolor{orange}{In a future
release of Cassandra-Linear, this set will be used to train an ``uncertainty''
emulator, which will train over the errors from the main emulator in order to
provide the user with an uncertainty estimate for any cosmology located within
the space of priors over which the main emulator was trained.} 

\textcolor{orange}{However, this functionality has not yet been implemented. 
Currently, we are 
using the validation hypercube more as a test hypercube: we compute the 
uncertainties at discrete points and examine these uncertainties (e.g. with
scatterplots and histograms) to assess the performance of the emulator.}


\section{Future Work}
\label{sec: future_work}

Here I will talk about what kinds of questions we estimate will be most 
fruitful for further inquiries about this topic and this code.

%s Theoretical inquiries

Is there a theoretically perfect LHC generator?

%s CLASS versus CAMB again

In order to expand the priors, it would be helpful to investigate why CAMB 
does not allow negative redshifts, in case this can be adapted. Alternatively, 
CLASS \textcolor{green}{allows} negative redshifts, so it may be worthwhile to 
repeat the work of this thesis using power spectra from CLASS. This would 
involve familiarization with a new platform, however, and so this escapes the 
scope of this thesis. Remember that negative redshifts would be a helpful 
feature for this work because it would allow us to investigate much broader 
priors.

% Inquiries using different technologies

Should we use a neural network instead of a Gaussian process for our emulator? 
AndreaP and Alex Eggemeier are already on the job: they are converting COMET 
to a neural network approach. We recommend that the reader follow future COMET 
papers for investigations into this question.

GP's allow natural propagation of
uncertainty in predictions to the final posterior distribution; neural
networks lack this feature. At the same time, NNs provide larger speedups 
\textcolor{green}{CITATIONS}. We performed limited testing of a neural network
but did not allocate enough time to arrive at conclusive / quantitative
comparisons.

Symbolic regression for the perfect $A_s$ formula?
As mentioned in section~\ref{sec: fit_testing}.
Of course, we would 
like to reiterate that, though we have applied a symbolic regression approach 
in order to independently justify our analytical expression from
chapter~\ref{chap: A_s}, this by no means guarantees that we have found the 
optimal analytical expression.

%s Code improvements

The code should be expanded with documentation and unit tests. Also, the
user interface script is still in progress.

To simplify the user experience, this two-emulator solution lives ``under the
hood'' and by default \textcolor{orange}{will be} hidden behind an interface
which automatically queries the correct emulator given some user-input
cosmology.

We want to build an emulator over the validation set in order to quote an 
uncertainty to the user and to try to further correct for errors? I'm not sure 
I understood that last part, we would have to ask Ariel again.

Extend the two-emulator solution to create emulators with different mass
hierarchies. Mass hierarchy should also be an option to specify in the
scenario file, like: degenerate, normal, inverted, both normal and inverted. 
Remember to scale the mnu correctly to bridge the gap between degenerate and
normal that we observed when playing around.

\textcolor{orange}{The integration of 
multiple emulators into one user-facing script can be further exploited with, 
for example, different emulators for different neutrino mass hierarchies.}

(For example, $\sigma_{12}^2$ sampling--although we did not
get it to work, the infrastructure is still there for whomever comes along in
the future. This suggestion builds on the concepts of
section~\ref{sec: lhc_outline}.)

