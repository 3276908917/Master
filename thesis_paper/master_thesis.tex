\documentclass[11pt]{article}
\usepackage{reports}

% The following block isn't actually used on the output.
% But it's easier to simply fill it out than to try
% to edit the rather fragile reports.sty file.
\newcommand*{\instr}{Ariel S\'{a}nchez}
\newcommand*{\term}{31.07.2023}
\newcommand*{\coursenum}{Master's Thesis}
\newcommand*{\hwnum}{Redshift Dependence of Neutrino Damping on the Power
Spectrum}


\usepackage{pdfpages}
\usepackage{bm}
\usepackage{listings}
\usepackage{titling}

\newcounter{biblatexAvailable}
\setcounter{biblatexAvailable}{0}

\IfFileExists{biblatex.sty} {
    \usepackage[
        backend=biber,
        style=alphabetic,
        sorting=ynt
    ]{biblatex}
    \addbibresource{master_thesis.bib}
}

\newcommand{\cbib}[1]{
    \IfFileExists{biblatex.sty} {
        \cite{#1}
    }{
        (citation ``#1'' cannot be linked in the current environment)
    }
}

\graphicspath{{./res/}}

\begin{document}

\fontsize{12}{15}

\begin{center}
Lukas Finkbeiner: Master's Thesis
\end{center}

This is a master's thesis. I hope to defend it to your satisfaction.

\tableofcontents

\begin{centering}
\section{Introduction: overarching themes and goals}
\end{centering}

\footnote{
    Some of my ideas for this section strike me as perhaps too introductory.
    The master's thesis is supposed to read like a scientific paper, right?
    But those never start off by talking about LSS nor why it's interesting,
    they just jump right into the novel techniques.
}

We want to study the large-scale structure of the Universe. From our
observations thereof, we want to find the best-fit parameters, such as
the Universe's density in cold dark matter.

If we have a particular cosmology (some specific configuration of these various
parameters) that we would like to explore, it would be helpful to know what
this Universe looks like in the ways that correspond to the capabilities of
our instruments. For example, for any given proposed model, we should like to
know what the power spectrum\footnote{It's a little weird that I'm assuming
familiarity with the power spectrum, given the simplicity used elsewhere.
But I think it's okay.} would look like
in such a Universe so that we
can compare it to the power spectrum that we measure in our own.

To output these kinds of observables based on some proposed model, we task
Boltzmann solvers.

\begin{centering}
\section{Boltzmann Solvers and CAMB}
\end{centering}

This is just a theoretical discussion of these solvers. The hands-on stuff
comes a couple sections later.

Talk about CAMB and CLASS. Are there any important ways in which they
distinguish themselves? Why do we use both of them in modern papers? Anyway,
the reason that we don't use CMBFast (as well as other older codes?) is
because it is no longer maintained.

\begin{centering}
\section{Emulators}
\end{centering}


In order to constrain cosmological parameters based on the application of this
method to \textcolor{red}{LSS data sets}\footnote{
	Vague--you should instead mention the precise names of the data sets,
	and you should even have a dedicated section of the paper wherein you
	summarize the goals, instrumentation, targets, and state-of-the-art
	analyses of these data sets
}, we must call a Boltzmann solver for each point in our parameter space. This
multitude of calls represents an enormous computational, and therefore time,
cost.

Therefore, we would like to build a machine that can predict what the power
spectrum should look like (for any given combination of the input parameters)
based on a relatively small number of input simulations. Emulators have been
recently  proposed as such a solution.

\textit{What is an emulator? How do we train it?} (Arico et al. 2021, Mancini
et al. 2021).

Emulators interpolate across a high-dimensional parameter space. The primary
limitation is that the emulator has to be built with every possible parameter
in mind that an end-user could wish to vary. There are a large number of
different cosmological parameters discussed in the modern literature. The
solution, proposed as evolution mapping, consists in categorizing the
different cosmological parameters in terms of their impacts on the power
spectrum. By clearly expressing degeneracies and dependencies in this way, we
can capture a large number of hypothetical parameters with a relatively simple
and unchanging emulator. \textcolor{orange}{This is just my impression of the
project so far! This is a fairly foundational paragraph to the kind of work
that we are trying to do, so we should get Ariel's feedback on this
formulation very early on in the thesis-writing process!}

Now, I would like to briefly introduce the particular emulation suite with
which this paper will deal.

Now, for a point of suspense: there are so many different cosmological
parameters that we investigate. What is the point of building an emulator if
you just have to train a new one each time you take interest in a new
parameter? This is supposed to be a segue into evolution mapping, a proposed
solution to this problem.

\begin{centering}
\section{Sigma 12 and Evolution Mapping}
\end{centering}

\footnote{
    I have two primary objectives for this section: explain the unit system
    we are using (ditch $h$ factor because it messes up everything--but
    only briefly summarize the main arguments of Sanchez 2020), and briefly
    summarize why we can funnel all of the evolution parameters through
    $\sigma_{12}$ in this way. Unfortunately, this second objective will
    almost certainly require you to bust out a few equations, and even to
    manipulate them a little to tease out relations essential to this paper.
}

Conventional emulator calibration entails the historical units of Mpc / $h$,
but if we use instead units of Mpc, then we can \textcolor{red}{???} 

(S\'{a}nchez et al, 2021) proposes to divide up the full set of cosmological
parameters into two categories: \textit{evolution} parameters $\mathcal{O}_E$
(such as $\omega_b$, $\omega_c$, and $\eta_s$)
affect the amplitude of the power spectrum at a particular redshift, while
\textit{shape} parameters $\mathcal{O}_S$
(such as $\omega_\kappa$, $\omega_\text{DE}$, w(a))
affect the shape of the power
spectrum. \textcolor{orange}{One paper made it sound like shape parameters
affect both shape and amplitude. Was it one of the S\'{a}nchez papers? I think
I misunderstood it, but we have to confirm that we can now understand what was
meant.\footnote{
    I'm certain that I misread this second description, for the following.
    The main \textit{problem} with neutrinos (i.e. why they \textit{don't}
    play nicely with the categories that I am trying to describe here)
    is that they affect \textit{both} shape and evolution! As far as we know,
    the physical density in neutrinos is the \textit{only} parameter that
    is poorly behaved in this way.
}
Why does it help to have these categories in the first place?}\footnote{
    As I understand it, we split the parameters up this way because
    one category of parameters (I think the evolution parameters)
    can be entirely funneled through just one parameter
    (I think it's $\sigma_{12}$, but obviously we should double check that)
}

Why is this scheme important? Evolution mapping\footnote{I think we need to
double check our language here, I think evolution mapping refers to more than
just the categorization scheme} greatly simplifies the emulator
scheme, because we can
funnel all of the evolution parameters through $\sigma_{12}$, so we've
collapsed that entire category to just one parameter.

Now, for the hiccup: this scheme is broken by one parameter, the Universe's
density in neutrinos. (In the next section: why this is so and what we can do
about it.)

\begin{centering}
\section{Neutrinos and Their Cosmological Impact}
\end{centering}

\footnote{
    Here I am violating my traditional rule of ordering sections by
    increasing complexity, because I consider the $\sigma_{12}$ argument
    decidedly more esoteric than a summary of the relevant points of neutrino
    physics. I think we should stick with this structure though, in the
    interest of narrative. Specifically, neutrinos represent a wrench in the
    plan. For that development to make sense, we first need to establish what
    the ``plan'' is: evolution mapping.
}

From Kiakotou 2008: ``Neutrinos with masses on the eV scale or below will be a
hot component of the dark matter and will free-stream out of overdensities and
thus wipe out small-scale structures.''

Remember what the point of this section is: why is $\omega_\nu$ bad for the
evolution mapping scheme? Because neutrinos exhibit redshift-dependent
damping of the power-spectrum.
Why? I \textcolor{orange}{think} that it's because all neutrinos start off as
relativistic particles in the early Universe, acting as a type of radiation.
But as the Universe continues to expand and cool, the neutrinos behave
increasingly like dark matter.
In this way, the physical density in neutrinos impacts both the shape and the
evolution. What I don't understand is why we say that this is so when we
only ever consider the shape of the power spectrum at one redshift. By
``shape'' I was assuming that S\'{a}nchez meant the shape of $P(k)$ versus
$k$ but if $\omega_\nu$ is really a problem, then shape should also refer
to $P(k)$ through all values $z$? I don't quite understand this, but
\textcolor{green}{maybe we can ask Ariel}.


\begin{centering}
\section{CAMB, Initial Setup}
\end{centering}

CAMB is a Fortran code with a Python wrapper\footnote{
\url{https://github.com/cmbant/CAMB}
}which we will be using for the
entirety of this project.

\footnote{
    At this point, we've built suspense in the reader: we have a goal and
    we have an idea about how to get to that goal. Therefore, now is the
    appropriate time to divert into laying the technical groundwork which
    will allow us to explore our idea through concrete implementations and
    to test our idea with plots and numbers.
}

To introduce the reader to the scope of CAMB, we will now introduce
some basic simulated power spectra along with a summary of the dynamic
parameters which will be of greatest interest to us.

Neutrino mass hierarchy: the options are degenerate, normal, and inverted.
The CAMB documentation annotates this parameter as ``(1 or 2 eigenstate
approximation),'' but this is somewhat unclear. Is the degenerate
hierarchy the single mass eigenstate approximation? Do both normal
and inverted hierarchies involve two eigenstates?

\begin{centering}
\section{Dumpster}
\end{centering}

\footnote{
    The following is a giant dumpster (\textit{NOT} a graveyard) for notes
    that I took on paper, as well as possible directions in which to take the
    report. I hope to eventually organize these into a coherent story.
}

\textit{What is the MCMC?} Monte Carlo Markov Chain is a way of exploring this
parameter space in the interests of finding the best fit parameters, right?
Should this definition come
when we're trying to use our emulator to make conclusions about our data?
So, the paper that Ariel gave me on this talks about fitting to the WMAP
CMB data with MCMC running on some likelihood function. Is there ever a point
at which I'm actually making a conclusion about the world, or am I just
constructing a tool here by studying the redshift dependence of neutrinos? 

\begin{centering}
\section{Meta / Lessons Learned}
\end{centering}

\footnote{The reason I am putting this content here rather than in pretty
notes: I think that these lessons might actually be a great guide for writing
the paper, and I could talk about the intuition behind them.
}

\cbib{Kiakotou}

\IfFileExists{biblatex.sty} {
    \printbibliography
}

\end{document}
