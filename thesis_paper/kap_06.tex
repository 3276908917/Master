\chapter{Conclusion}
\label{chap: conclusion}

Massive neutrinos represent a challenge for evolution-mapping emulators
because their damping of the power spectrum depends on redshift.
This redshift dependence results in $\omega_\nu$ impacting both the
amplitude and the shape of the linear-theory cold-matter power spectrum.
This behavior precludes exclusive categorization as either a shape or
evolution parameter.

We find that two simple adjustments to evolution mapping can extend its
efficacy to the case of massive neutrinos. By recasting the evolution mapping
relation in terms of $\tilde{\sigma}_12}$ rather than $\sigma_{12}$--that is,
in terms of the MEMNeC amplitude rather than the desired cosmology's
amplitude--we are able to circumvent the impact of $\omega_\nu$ on the
amplitude of the power spectrum, particularly on large scales. Furthermore,
we find that $A_s$ is not exclusively an evolution parameter but contains
information about the impact of massive neutrinos on the small-scale
power spectrum. By including it as a parameter over which we train the
emulator, we find that the small-scale accuracy significantly improves.

We introduce a new code, the Python package Cassandra-Linear, to construct
and test an emulator of the linear-theory cold-matter power spectrum based on
these extensions to the evolution mapping recipe. We offer a detailed survey
of the capabilities of this code so that interested readers can build and
experiment with their own emulators.

We compare the accuracies of two emulators trained using this code: one
including $A_s$ and $\omega_\nu$, and the other featuring no massive 
neutrinos. We find that the massless-neutrino emulator performs significantly
better when the number of samples is held fixed, but argue that this
comparison is unfair. We show that for the majority of scales and the majority
of cosmologies, the level of error of our massive-neutrino emulator is
comparable to that of CAMB, the Boltzmann solver we use to produce the
training spectra with which we train the emulator.

FUTURE WORK: SEE SPECIFICALLY HOW THE DISCREPANCY BETWEEN THE TWO EMULATORS
DECREASES AS WE INCREASE THE NUMBER OF SAMPLES $N_s$: IF WE HOLD $N_s$ FIXED
FOR THE MASSLESS EMULATOR, CAN WE EVER CONSTRUCT A MASSIVE EMULATOR OF HIGHER
ACCURACY?

``The goal of this project is to produce an emulator of the linear-theory
power spectrum based on evolution mapping'' (A. G. S\'{a}nchez, private
communication). An emulator is a multi-dimensional function
produced by interpolation across many training points $(x, y)$. The power
spectrum can be thought of as a statistical description of the ``clumpiness''
of matter in the Universe. Evolution mapping is a technique for simplifying
the parameter space by identifying many cosmological parameters as basically
the same in their impact on the power spectrum.

The evolution mapping scheme of \cbib{San21} has already achieved success in
emulators of the nonlinear power spectrum. However, massive neutrinos do not
fit neatly into the scheme. Consequently, emulators implementing evolution
mapping have typically fixed the physical density of the Universe in 
neutrinos, $\omega_\nu$, to zero.

This work seeks to extend the evolution mapping scheme to massive-neutrino 
cosmologies through expansion of the parameter space over which the emulator
is trained. In particular, we find that the scalar mode amplitude $A_s$, which
is traditionally treated as an evolution parameter and therefore excluded from 
the parameter space. can be used to quantify the suppression of structure 
growth due to massive neutrinos.

We find that, by including $A_s$ in our parameter space, we can successfully 
train over the physical density of the universe in massive neutrinos. We 
introduce a new emulation code, Cassandra-Linear (CL), which combines this 
expanded parameter space with evolution
mapping. We include various error statistics and show that the emulator
performs roughly at the level of error associated with CAMB itself, which is
guaranteed to be no worse than 0.1\% \cbib{Seljak}.

\textcolor{orange}{Why does this work? Before, the shape impact of
$\omega_\nu$ 
was $z$-dependent. In other words, if we held $\omega_nu$ constant and varied
$z$, the power spectrum would not vary only in overall amplitude. However, if
we fix $\omega_\nu$ and $A_s$, $z$ becomes an evolution parameter again. Why
does fixing $A_s$ accomplish this? That's something for the theorists to 
figure out!}

% Just use one big block of text, don't split into sections

What was the main objective of this thesis? What were the key results of this 
work? Why are they important? How do these results compare with results from 
papers on similar subjects?

\section{Emulation over Uncertainties}

\textcolor{blue}{This content should be subsumed into future work, since we
didn't manage to complete this feature in time.}

For a further step of accuracy, we can add a third data set to our pipeline
and introduce a second layer of emulation.

Up to this point, our pipeline has included a training set and a testing set.
If we add a validation set, then we can train a second emulator over the
errors associated with the first emulator's performance on this validation
set.

Within the current (as of \textcolor{orange}{24.08.2023}) setup of
Cassandra-Linear, we typically generate two Latin hypercubes for each
emulator. The first represents our training set and an emulator cannot be
produced without it. The second one represents our validation set.

\textcolor{orange}{In a future
release of Cassandra-Linear, this set will be used to train an ``uncertainty''
emulator, which will train over the errors from the main emulator in order to
provide the user with an uncertainty estimate for any cosmology located within
the space of priors over which the main emulator was trained.} 

\textcolor{orange}{However, this functionality has not yet been implemented. 
Currently, we are 
using the validation hypercube more as a test hypercube: we compute the 
uncertainties at discrete points and examine these uncertainties (e.g. with
scatterplots and histograms) to assess the performance of the emulator.}


\section{Future Work}
\label{sec: future_work}

Here I will talk about what kinds of questions we estimate will be most 
fruitful for further inquiries about this topic and this code.

%s Theoretical inquiries

Is there a theoretically perfect LHC generator?

%s CLASS versus CAMB again

In order to expand the priors, it would be helpful to investigate why CAMB 
does not allow negative redshifts, in case this can be adapted. Alternatively, 
CLASS \textcolor{green}{allows} negative redshifts, so it may be worthwhile to 
repeat the work of this thesis using power spectra from CLASS. This would 
involve familiarization with a new platform, however, and so this escapes the 
scope of this thesis. Remember that negative redshifts would be a helpful 
feature for this work because it would allow us to investigate much broader 
priors.

% Inquiries using different technologies

Should we use a neural network instead of a Gaussian process for our emulator? 
AndreaP and Alex Eggemeier are already on the job: they are converting COMET 
to a neural network approach. We recommend that the reader follow future COMET 
papers for investigations into this question.

GP's allow natural propagation of
uncertainty in predictions to the final posterior distribution; neural
networks lack this feature. At the same time, NNs provide larger speedups 
\textcolor{green}{CITATIONS}. We performed limited testing of a neural network
but did not allocate enough time to arrive at conclusive / quantitative
comparisons.

Symbolic regression for the perfect $A_s$ formula?
As mentioned in section~\ref{sec: fit_testing}.
Of course, we would 
like to reiterate that, though we have applied a symbolic regression approach 
in order to independently justify our analytical expression from
chapter~\ref{chap: A_s}, this by no means guarantees that we have found the 
optimal analytical expression.

%s Code improvements

The code should be expanded with documentation and unit tests. Also, the
user interface script is still in progress.

To simplify the user experience, this two-emulator solution lives ``under the
hood'' and by default \textcolor{orange}{will be} hidden behind an interface
which automatically queries the correct emulator given some user-input
cosmology.

We want to build an emulator over the validation set in order to quote an 
uncertainty to the user and to try to further correct for errors? I'm not sure 
I understood that last part, we would have to ask Ariel again.

Extend the two-emulator solution to create emulators with different mass
hierarchies. Mass hierarchy should also be an option to specify in the
scenario file, like: degenerate, normal, inverted, both normal and inverted. 
Remember to scale the mnu correctly to bridge the gap between degenerate and
normal that we observed when playing around.

\textcolor{orange}{The integration of 
multiple emulators into one user-facing script can be further exploited with, 
for example, different emulators for different neutrino mass hierarchies.}

(For example, $\sigma_{12}^2$ sampling--although we did not
get it to work, the infrastructure is still there for whomever comes along in
the future. This suggestion builds on the concepts of
section~\ref{sec: lhc_outline}.)

