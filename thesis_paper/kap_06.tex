\chapter{Conclusion}
\label{chap: conclusion}

Massive neutrinos represent a challenge for evolution-mapping emulators
because their damping of the power spectrum depends on redshift.
This redshift dependence results in $\omega_\nu$ impacting both the
amplitude and the shape of the linear-theory cold-matter power spectrum.
This behavior precludes exclusive categorization as either a shape or
evolution parameter.

We find that two simple adjustments to evolution mapping can extend its
efficacy to the case of massive neutrinos. By recasting the evolution mapping
relation in terms of $\tilde{\sigma}_12$ rather than $\sigma_{12}$--that is,
in terms of the MEMNeC amplitude rather than the desired cosmology's
amplitude--we are able to circumvent the impact of $\omega_\nu$ on the
amplitude of the power spectrum, particularly on large scales. Furthermore,
we find that $A_s$ is not exclusively an evolution parameter but contains
information about the impact of massive neutrinos on the small-scale
power spectrum. By including it as a parameter over which we train the
emulator, we find that the small-scale accuracy significantly improves.

We introduce a new code, the Python package Cassandra-Linear, to construct
and test an emulator of the linear-theory cold-matter power spectrum based on
these extensions to the evolution mapping recipe. We offer a detailed survey
of the capabilities of this code so that interested readers can build and
experiment with their own emulators.

We compare the accuracies of two emulators trained using this code: one
including $A_s$ and $\omega_\nu$, and the other featuring no massive 
neutrinos. We find that the massless-neutrino emulator performs significantly
better when the number of samples is held fixed. Nevertheless, we show that
the $3\sigma$ confidence interval is below the 0.1\% error level. This means
the error of our massive-neutrino emulator is
comparable to that of CAMB, the Boltzmann solver we use to produce the
training spectra used for training.

We explore the impact of $s^*$, $N_k$, and $N_s$ on the performance of the
emulator and find...

In conclusion...

% Just use one big block of text, don't split into sections

\textcolor{red}{How do these results compare with results from 
papers on similar subjects?}

%s FUTURE WORK

To conclude this work, we here identify several promising paths to advancing
both the Cassandra-Linear code as well as the scientific analyses in
chapter~\ref{chap: results}. The potential improvements are
numerous, but we here concentrate on the most important
suggestions.\footnote{For smaller suggestions, refer to the GitHub issues 
page: \url{https://github.com/3276908917/Master/issues}}.

%s Section 1: science improvements

Based on the results of sections~\ref{sec: error_from_lhc} and~\ref{sec: 
num_samples}, it would behoove future inquiries to investigate whether we
can in some way compensate for the different impact of $N_s$ on the massive
and massless emulators so that their accuracies may be directly compared.
Such a comparison would help us to understand the error associated
specifically with the evolution mapping extensions introduced in
chapter~\ref{chap: A_s}. Even if a direct comparison method cannot be
established, it would be useful to know how much larger $N_s$ would have to
be for a massive emulator to reach equivalent levels of error.

Based on the results of section~\ref{sec: error_from_lhc},
\textcolor{blue}{we do not consider LHC improvement to be a significant
priority}. But if it were... we would look through the theory. Can we build
a generator of perfect, or at least much  better, LHCs?

%s Section 2: code improvements. Segue offered by the uncertainty emulator.

We want to build an emulator over the validation set in order to quote an 
uncertainty to the user and to try to further correct for errors? I'm not sure 
I understood that last part, we would have to ask Ariel again.
\
For a further step of accuracy, we can add a third data set to our pipeline
and introduce a second layer of emulation. Up to this point, our pipeline has 
included a training set and a testing set.
If we add a validation set, then we can train a second emulator over the
errors associated with the first emulator's performance on this validation
set.
\
Within the current (as of \textcolor{orange}{24.08.2023}) setup of
Cassandra-Linear, we typically generate two Latin hypercubes for each
emulator. The first represents our training set and an emulator cannot be
produced without it. The second one represents our validation set.
\
\textcolor{orange}{In a future
release of Cassandra-Linear, this set will be used to train an ``uncertainty''
emulator, which will train over the errors from the main emulator in order to
provide the user with an uncertainty estimate for any cosmology located within
the space of priors over which the main emulator was trained.} 
\
\textcolor{orange}{However, this functionality has not yet been implemented. 
Currently, we are 
using the validation hypercube more as a test hypercube: we compute the 
uncertainties at discrete points and examine these uncertainties (e.g. with
scatterplots and histograms) to assess the performance of the emulator.}

\textcolor{orange}{The integration of 
multiple emulators into one user-facing script can be further exploited with, 
for example, different emulators for different neutrino mass hierarchies.}
Extend the two-emulator solution to create emulators with different mass
hierarchies. Mass hierarchy should also be an option to specify in the
scenario file, like: degenerate, normal, inverted, both normal and inverted. 
Remember to scale the mnu correctly to bridge the gap between degenerate and
normal that we observed when playing around.

(For example, $\sigma_{12}^2$ sampling--although we did not
get it to work, the infrastructure is still there for whomever comes along in
the future. This suggestion builds on the concepts of
section~\ref{sec: lhc_outline}.)
\
We also tried sampling in $\sigma_{12}^2$ as well as $\sqrt{\sigma_{12}}$.
Unfortunately, we were unable to conclude anything about the effectiveness of
these strategies--there appears to have been some mistake in our code, such
that the errors are much larger than can be explained on account of poor
sampling.
\
See figures~\ref{fig: sigsquare_sample} and~\ref{fig: sigroot_sample} for
illustrations of the problem. In a future work, it would be helpful to
investigate these problems further. We may find that a different sampling
strategy will more efficiently reduce the deltas that we see in our emulator.

%s Section 3: use of different technologies

%s CLASS versus CAMB again

Although our priors should already suffice for most conventional parameter
inference studies, we believe that wider priors would be of benefit to those
seeking to understand exotic cosmologies. The chief obstacle to 
implementation of wider priors in CL is CAMB's requirement that $z \geq 0$,
which limits the range of cosmologies we can probe. 
Negative redshifts do not violate any conditions in the equations of
cosmological evolution, so it could prove fruitful to investigate why CAMB 
has this requirement; if the code could be easily amended, the results of
this work could be extended to broader priors. Alternatively,
CLASS \textcolor{green}{allows} negative redshifts, so it may be worthwhile 
to repeat the work of this thesis using power spectra from CLASS.

Should we use a neural network instead of a Gaussian process for our emulator? 
AndreaP and Alex Eggemeier are already on the job: they are converting COMET 
to a neural network approach. We recommend that the reader follow future COMET 
papers for investigations into this question.
\
GP's allow natural propagation of
uncertainty in predictions to the final posterior distribution; neural
networks lack this feature. At the same time, NNs provide larger speedups 
\textcolor{green}{CITATIONS}. We performed limited testing of a neural network
but did not allocate enough time to arrive at conclusive / quantitative
comparisons.


% Inquiries using different technologies

%s Code improvements

%%% Unprofessional to mention the following here:
\begin{comment}
The code should be expanded with documentation and unit tests. Also, the
user interface script is still in progress.

To simplify the user experience, this two-emulator solution lives ``under the
hood'' and by default \textcolor{orange}{will be} hidden behind an interface
which automatically queries the correct emulator given some user-input
cosmology.
\end{comment}
%%%

%%% Way too much detail for this thesis, but I'm glad that I at least thought
%%% about these things.
\begin{comment}
Our hope was that the narrow parameter ranges would furthermore help the 
demonstration emulator to achieve high accuracy--in principal, success here
means that we can simply ``scale up'' the approach of this work by
simultaneously expanding the priors as well as the total number of training
samples. Unfortunately, it is not clear if we can scale up the emulator past
the point at which unsolvable cells begin to appear. Since Latin hypercube
sampling is designed to evenly sample a space, unsolvable cells certainly
indicate that parts of the parameter space lack representation in the training
data. In these regions, our emulator will be forced to interpolate across
large gaps, or worse, extrapolate (if the unsolvable cells occur at the edges
of the parameter space \textcolor{orange}{This is something that I should have
shown... i.e. with plots}).
% Andrea recommends a plot coloring points by the "extremeness" in our sample
% space.

We predict that increasing the total number of cells will only marginally
reduce the issue of unsolvable cells \textcolor{orange}{This is something that 
I should have shown... i.e. with plots}). We can imagine the subspace of
solvable points as some hypervolume within a hypercube determined by our
priors, and the emulator's training coverage as an approximation of this 
hypervolume with small hypercubes whose size is determined by the separation 
between points in the sample. In the ideal case, the space of solvable points 
is the same as the Latin hypercube. When this is not so, we can at least
reduce the error associated with our approximation of the space of solvable
points by shrinking the hypercubes we use in our approximation (i.e. by
increasing the total number of points in our sample). \textcolor{orange}{
To give a sense of the marginal nature of this error reduction, we can
consider how small our hypercubes already are. For simplicity, let's examine
just one axis of the hypercube. With 5000 samples in the ``MEGA'' priors,
the length of the training coverage MOST STRONGLY DETERMINED BY ONE HYPERCUBE
IS: UNFINISHED THOUGHT}

It seems reasonable to think that unsolvable cells indicate extreme regions of 
the parameter space, rather than isolated holes. Therefore, it would be 
misleading to claim that the final ``MEGA'' emulator corresponds to, for 
example, any prior ranges in table 00A; in truth, the emulator would 
correspond to a potentially (this is a dangerous word and opens you up to hard 
questions) complicated shape inscribed within the six-dimensional rectangular 
hyperprism.
\end{comment}
%%%