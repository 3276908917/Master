\chapter{Conclusion}
\label{chap: conclusion}

Massive neutrinos represent a challenge for evolution-mapping emulators
because their damping of the power spectrum depends on redshift.
This redshift dependence results in $\omega_\nu$ impacting both the
amplitude and the shape of the linear-theory cold-matter power spectrum.
This behavior precludes exclusive categorization as either a shape or
evolution parameter.

We find that two simple adjustments to evolution mapping can extend its
efficacy to the case of massive neutrinos. By recasting the evolution mapping
relation in terms of $\tilde{\sigma}_12$ rather than $\sigma_{12}$--that is,
in terms of the MEMNeC amplitude rather than the desired cosmology's
amplitude--we are able to circumvent the impact of $\omega_\nu$ on the
amplitude of the power spectrum, particularly on large scales. Furthermore,
we find that $A_s$ is not exclusively an evolution parameter but contains
information about the impact of massive neutrinos on the small-scale
power spectrum. By including it as a parameter over which we train the
emulator, we find that the small-scale accuracy significantly improves.

We introduce a new code, the Python package Cassandra-Linear, to construct
and test an emulator of the linear-theory cold-matter power spectrum based on
these extensions to the evolution mapping recipe. We offer a detailed survey
of the capabilities of this code so that interested readers can build and
experiment with their own emulators.

We compare the accuracies of two emulators trained using this code: one
including $A_s$ and $\omega_\nu$, and the other featuring no massive 
neutrinos. We find that the massless-neutrino emulator performs significantly
better when the number of samples is held fixed. Nevertheless, we show that
the $3\sigma$ confidence interval is below the 0.1\% error level. This means
the error of our massive-neutrino emulator is
comparable to that of CAMB, the Boltzmann solver we use to produce the
training spectra used for training.

We explore the impact of $s^*$, $N_k$, and $N_s$ on the performance of the
emulator and find...

In conclusion...

% Just use one big block of text, don't split into sections

\textcolor{red}{How do these results compare with results from 
papers on similar subjects?}

%s FUTURE WORK

To conclude this work, we identify several promising paths to advancing
both the Cassandra-Linear code as well as the scientific analyses in
chapter~\ref{chap: results}. The potential improvements are
numerous, but we here concentrate on the most important
suggestions.\footnote{For smaller suggestions, refer to the GitHub issues 
page: \url{https://github.com/3276908917/Master/issues}}.

%s Section 1: science improvements

It would behoove future inquiries to follow up on
sections~\ref{sec: error_from_lhc} and~\ref{sec: num_samples} by investigating 
whether we
can in some way compensate for the different impact of $N_s$ on the massive
and massless emulators so that their accuracies may be directly compared.
Such a comparison would help us to understand the error associated
specifically with the evolution mapping extensions introduced in
chapter~\ref{chap: A_s}. Even if a direct comparison method cannot be
established, it would be useful to know how much larger $N_s$ would have to
be for a massive emulator to reach equivalent levels of error.

% I can't touch this paragraph until the results are in
Based on the results of section~\ref{sec: error_from_lhc},
\textcolor{blue}{we do not consider LHC improvement to be a significant
priority}. But if it were... we would look through the theory. Can we build
a generator of perfect, or at least much  better, LHCs?

After finding that our emulator requires orders of magnitude less
time to produce a power spectrum than CAMB, we ceased to consider the precise
time cost. Instead, we compared different emulators exclusively on the
basis of accuracy. It would be helpful to compare the computation times of the
the various emulators. The large-$N_s$ samples, for example, lead to larger
emulators. The $N_s = 3000$ emulator is 219 MB, while the $N_s = 7000$
emulator is 1.5 GB. Do larger file sizes also mean slower emulators?

%s Section 2: code improvements. Segue offered by the uncertainty emulator.

The most important next step for the Cassandra-Linear code is the
extension of the emulator pipeline with a third data set, which we call the
\textit{validation} set. From the validation data we will build a validation
emulator, which will estimate the emulator error for any given cosmology.
We expect that combination of this information with the original emulator
predictions will achieve greater performance for the vast majority of
cosmologies. However, this will double the
computational cost both of building and querying the emulator.

We used a two-emulator approach analyzed in \ref{sec: 2emu_improvement},
which extended the range of applicability of our emulator. We can extend this
approach to account for discrete aspects of the cosmology. In particular, 
one could train two emulators of different mass hierarchies, inverted and
normal. The CAMB settings to change are in principle simple (see
section~\ref{sec: neutrino_settings}), but some work would need to be done to
see how the \texttt{mnu} parameter would need to be modified so that the
results are directly comparable with those cited here, which make use of the
degenerate mass hierarchy.

Our code would be more versatile if the user were allowed to specify a
distribution for each parameter in $\matr{X}$. For example, the emulator
performance appears to depend on $\tilde{sigma}_{12}$,
even if only for the trivial
reason that percent error depends on the amplitude of the true quantity.
Therefore, we were curious if the emulators would perform better by
sampling $\sqrt{\tilde{\sigma}}_{12}$ rather than $\tilde{\sigma}_{12}$.
In principle, the change should be fairly simple, but we encountered an
unknown error during the implementation and the feature is incomplete.

%s Section 3: use of different technologies

%s CLASS versus CAMB again

Although our priors should already suffice for most conventional parameter
inference studies, we believe that wider priors would be of benefit to those
seeking to understand exotic cosmologies. The chief obstacle to 
implementation of wider priors in CL is CAMB's requirement that $z \geq 0$,
which limits the range of cosmologies we can probe. 
Negative redshifts do not violate any conditions in the equations of
cosmological evolution, so it could prove fruitful to investigate why CAMB 
has this requirement; if the code could be easily amended, the results of
this work could be extended to broader priors. Alternatively,
CLASS \textcolor{green}{allows} negative redshifts, so it may be worthwhile 
to repeat the work of this thesis using power spectra from CLASS.

Lastly, we suggest the promulgation of quantitative results comparing the
performance of GPRs and neural networks. COMET is an example of a GPR-based
project that is switching to neural networks. It would be useful to consider
whether the emulators showcased here could be made more effective simply by
implementing a different machine learning setup. 

% Inquiries using different technologies

%s Code improvements

%%% Unprofessional to mention the following here:
\begin{comment}
The code should be expanded with documentation and unit tests. Also, the
user interface script is still in progress.

To simplify the user experience, this two-emulator solution lives ``under the
hood'' and by default \textcolor{orange}{will be} hidden behind an interface
which automatically queries the correct emulator given some user-input
cosmology.
\end{comment}
%%%

%%% Way too much detail for this thesis, but I'm glad that I at least thought
%%% about these things.
\begin{comment}
Our hope was that the narrow parameter ranges would furthermore help the 
demonstration emulator to achieve high accuracy--in principal, success here
means that we can simply ``scale up'' the approach of this work by
simultaneously expanding the priors as well as the total number of training
samples. Unfortunately, it is not clear if we can scale up the emulator past
the point at which unsolvable cells begin to appear. Since Latin hypercube
sampling is designed to evenly sample a space, unsolvable cells certainly
indicate that parts of the parameter space lack representation in the training
data. In these regions, our emulator will be forced to interpolate across
large gaps, or worse, extrapolate (if the unsolvable cells occur at the edges
of the parameter space \textcolor{orange}{This is something that I should have
shown... i.e. with plots}).
% Andrea recommends a plot coloring points by the "extremeness" in our sample
% space.

We predict that increasing the total number of cells will only marginally
reduce the issue of unsolvable cells \textcolor{orange}{This is something that 
I should have shown... i.e. with plots}). We can imagine the subspace of
solvable points as some hypervolume within a hypercube determined by our
priors, and the emulator's training coverage as an approximation of this 
hypervolume with small hypercubes whose size is determined by the separation 
between points in the sample. In the ideal case, the space of solvable points 
is the same as the Latin hypercube. When this is not so, we can at least
reduce the error associated with our approximation of the space of solvable
points by shrinking the hypercubes we use in our approximation (i.e. by
increasing the total number of points in our sample). \textcolor{orange}{
To give a sense of the marginal nature of this error reduction, we can
consider how small our hypercubes already are. For simplicity, let's examine
just one axis of the hypercube. With 5000 samples in the ``MEGA'' priors,
the length of the training coverage MOST STRONGLY DETERMINED BY ONE HYPERCUBE
IS: UNFINISHED THOUGHT}

It seems reasonable to think that unsolvable cells indicate extreme regions of 
the parameter space, rather than isolated holes. Therefore, it would be 
misleading to claim that the final ``MEGA'' emulator corresponds to, for 
example, any prior ranges in table 00A; in truth, the emulator would 
correspond to a potentially (this is a dangerous word and opens you up to hard 
questions) complicated shape inscribed within the six-dimensional rectangular 
hyperprism.
\end{comment}
%%%