\chapter{Conceptual Outline of Our Emulator Code and Pipeline}

\textcolor{blue}{The point of this section is to introduce the reader to the
basic structure of the emulator pipeline. Most of the discussion will follow
from a flow chart showing the various conceptual pieces of the emulator. Also
in this section, I will introduce various choices made (e.g. 5000 training
spectra, 300 points for each spectrum) WITHOUT justifying any of them--later,
in chapter~\ref{chap: disc_and_conc}, we'll justify these decisions and
talk about the consequences of alternate settings.}

Now that we have a solution which allows integration of massive neutrinos
into the evolution mapping framework, we are ready to plan and build the
machinery which leads to an emulator. CL represents a full emulation pipeline from the generation of training and 
testing data to the plotting of error statistics associated with
the emulation. This chapter will concentrate on a high-level and accessible 
description of the pipeline, with motivations behind several key components.
For a more detailed and technical breakdown of the implementation of the steps
mentioned here, refer to chapter~\ref{chap: implementation}.


\section{LHSs}
\label{sec: lhc_outline}

The LHSs give the values of the cosmological parameters and therefore may be 
thought of as the $\matr{X}$ in our regression problem, while the 
corresponding CAMB power spectra may be thought of as the $\matr{Y}$. Each
row $\bm{x}$ in $\matr{X}$ will describe a different cosmology, and each
row $\bm{y}$ in $\matr{Y}$ will be the power spectrum for that cosmology.

%%% I just don't think anyone is really going to be confused by this...
\begin{comment}
to
emphasize the roles of these data in the training and testing of the emulator. 
However, we will continue to use the term ``unit'' as introduced here, and 
the reader should keep in mind that a unit $\matr{X}_u$ therefore does not
indicate an identity matrix.}
\end{comment}

% Intro sentence?

We will generate our LHSs so that their values range from zero to unity.
In continuity with the language of section~\ref{sec: lhc_theory}, we will 
refer to such LHSs as \textit{unit} LHSs. These unit LHSs are saved as
\texttt{npy} files and later rescaled according to priors in order to
specify the training
domain in each parameter. This two-step process, first of generating a unit
LHC and then rescaling it, allows us to build many different emulators from a 
single LHS file. That is, rescaling the same LHS in different ways allows us
to cut down on the number of LHSs that we need to generate.

% One reason this goes here: priors show up in almost every script,
% so there isn't a great cass-L section in which to put this.

CL accomplishes this rescaling through the use of priors files.
CL is packaged with three pairs of priors files (the pairing will be explained
in section~\ref{sec: 2emu_intro}). The parameter domains for each pair are
described in tables~\ref{tab: MEGA_priors},~\ref{tab: CLASSIC_priors},
and~\ref{tab: COMET_priors}. The three pairs of files share the same priors in
$\omega_\nu$, [0, 0.01], and $\sigma_{12}$, [0.2, 1], and so these are
omitted. \textcolor{red}{Is it okay for me to omit like that or should I just
repeat the information?}

\begin{table}[ht!]
\centering
\begin{tabular}{l|l|l}
\hline
Parameter & Minimum Value & Maximum Value \\ \hline
$\omega_b$ & 0.005 & 0.28 \\
$\omega_c$ & 0.001 & 0.99 \\
$n_s$ & 0.7 & 1.3 \\
$A_s$\footnotemark & $5.003 \cdot 10^{-10}$ & $1.484 \cdot 10^{-8}$  \\
\end{tabular}
 \cprotect\caption[``MEGA'' priors]{``MEGA'' priors. The range of allowed
 	values is significantly larger than the modern $3\sigma$ intervals
 	for these parameters. It was created from the most extreme values used
 	in table 1 of \citet{Mancini} and table 1 of \citet{Arico}, even including
 	CMB emulators. 
 	\textcolor{red}{Maybe since we didn't end up using this prior file, I
 	should scrap it from the discussion?}}
 \label{tab: MEGA_priors}
\end{table}

\begin{table}[ht!]
\centering
\begin{tabular}{l|l|l}
\hline
Parameter & Minimum Value & Maximum Value \\ \hline
$\omega_b$ & 0.01875 & 0.02625 \\
$\omega_c$ & 0.05 & 0.255 \\
$n_s$ & 0.84 & 1.1 \\
$A_s$\footnotemark & $1.049 \cdot 10^{-9}$ & $4.990 \cdot 10^{-9}$  \\
\end{tabular}
	\cprotect\caption[``CLASSIC'' priors]{``CLASSIC'' priors. Despite
 	significantly smaller ranges than table~\ref{tab: MEGA_priors}, this
 	parameter space is still easily large enough to accommodate modern
 	statistical analyses. \textcolor{green}{citation.}}
 \label{tab: CLASSIC_priors}
\end{table}

\begin{table}[ht!]
\centering
\begin{tabular}{l|l|l}
\hline
Parameter & Minimum Value & Maximum Value \\ \hline
$\omega_b$ & 0.0205 & 0.02415 \\
$\omega_c$ & 0.085 & 0.155 \\
$n_s$ & 0.92 & 1.01 \\
$A_s$\footnotemark & $1.049 \cdot 10^{-9}$ & $4.990 \cdot 10^{-9}$  \\
\end{tabular}
	\cprotect\caption[``COMET'' priors]{``COMET'' priors.
	These values are significantly more restrictive than in
	tables~\ref{tab: MEGA_priors} and~\ref{tab: CLASSIC_priors},
	but the parameter space should still be large enough to accommodate
	most modern statistical analyses. \textcolor{green}{citation.}
	These priors were taken from table 2 of \citet{Eggemeier}.}
 \label{tab: COMET_priors}
\end{table}

\textcolor{orange}{Talk a little more about the prior ranges, what are their
values in words?}

At first, we used the highly ambitious ``MEGA'' set of priors
(table~\ref{tab: MEGA_priors}), whose values we decided based on the recent 
emulator papers from \citet{Mancini} and \citet{Arico}.
However, during the process of generating training data, 
we found that CAMB was frequently unable to compute the power spectrum for
all the
resulting cosmologies (this issue will be explained in
section~\ref{sec: generate_emu_data}). We do not recommend the use of this
file because the priors are not representative of the ranges appearing in the
training set.

The ``CLASSIC'' priors (table~\ref{tab: CLASSIC_priors}) are mostly solvable, 
but only the ``COMET'' priors (table~\ref{tab: COMET_priors}) are consistently
entirely solvable. For this reason and for the cheaper (by number of samples) 
accuracy that comes with tighter priors, the COMET priors are the defaults for 
CL.

The user is welcome to write sets of priors suitable to their applications.
To do so, one need only create a new prior file. The format is very simple, 
but must be followed carefully for the file to be read correctly. We
recommend copying one of the existing priors files as a template. 

As one final note on the general implementation of LHSs in CL, we note that
all axes of our \textit{unit} LHSs run \textit{uniformly} over the interval
[0, 1]. However, the rescaled 
parameter domains can be more flexible. For example, the fourth axis
of an LHS
describes $\sigma_{12}$ in the default implementation of the code. We can also
interpret the axis as a set of $\sigma_{12}^2$ values instead,
in case we wish to
increase our sampling in larger values of $\sigma_{12}$ over the same
interval. Unfortunately, this non-uniform sampling does not seem to be
correctly implemented
in the current (as of 2 October 2023) version of the code. Nonetheless, as we
will repeat in section~\ref{sec: future_work},
the feature is in principle simple to implement.


\section{Pipeline}
\label{sec: flow_chart}

% Disgusting hack to get verbatim functionality in tabel

To digestibly visualize the overall structure of emulation with CL, we
include figure~\ref{fig: flow_chart}.
This section will explain the main steps in the emulation pipeline by
elaborating on this flow chart with the aid of the abbreviations listed in
table~\ref{tab: script_summary}.

\textcolor{red}{The following paragraph is not very easy to read, but I'm not
sure how to better structure it.}

First, the user needs to decide on the desired properties and domain of the
emulator. Most of these properties are currently set within a ``scenario
file.'' Over what range in each parameter should the emulator be trained?
The user should specify these with a priors file (see section~\ref{sec: 
lhc_outline}) and reference it in the scenario file. At how many points in
$k$ should CL predict the power spectrum? By default, we pick $N_k = 300$. 
Where are the training and testing 
LHSs stored? When using \texttt{lhs} to generate the training and testing
LHSs, remaining questions are answered: how even does the LHS need to be
(e.g. threshold value for $s^*$)? How many samples should each data set have?
By default, we pick $N_s = 5000$.

\texttt{ui} processes the scenario file and loads the LHSs and priors into
memory, packaging them into a data dictionary. Before submitting this
dictionary to \texttt{te}, \texttt{ui} requests the power spectra from
\texttt{ged}, which constitutes our $\matr{Y}$.\footnote{\texttt{ged} also 
produces the rescale parameters files which are not
necessary for building the emulator but which contain useful diagnostic
information when verifying the correctness of the code. To learn about rescale
parameters, refer to section~\ref{sec: generate_emu_data}}

At this point, we have $\matr{X}_\text{train}$, $\matr{X}_\text{test}$, 
$\matr{Y}_\text{train}$, and $\matr{Y}_\text{test}$. These elements are
organized into a data dictionary and passed to \texttt{te}, which instantiates
a new emulator trainer object. This trainer in turn instantiates an emulator 
object and calculates some simple error metrics according to the provided
test set.

\textcolor{orange}{The emulator should also compute training errors!! This
would be a really easy way to expand the code in a meaningful way.}

\begin{table}[ht!]
\centering
\begin{tabular}{l|c|l}
\hline
Script & Abbreviation & purpose \\ \hline
\Verb|lhs.py| & \texttt{lhs} & Creates unit LHSs. \\
\Verb|generate_emu_data.py| & \texttt{ged} & Computes spectra for an LHS.\\
\Verb|camb_interface.py| & \texttt{ci} & Computes an individual spectrum. \\
\Verb|train_emu.py| & \texttt{te} & Trains and tests emulator. \\
\Verb|user_interface.py| & \texttt{ui} & Bridges
	\texttt{lhs}, \texttt{ged}, and \texttt{te} scripts. \\
\Verb|utils.py| & \texttt{utils} & Miscellaneous utility functions. 
\end{tabular}
 \cprotect\caption[Summary of CL Scripts]{\textcolor{red}{Should I axe
 	the last column or could it be appropriate for this thesis?}}
 \label{tab: script_summary}
\end{table}

% The following plots were generated with h_units_bad.ipynb
\begin{figure}[ht!]
    \centering
 	\includesvg[width=\textwidth]{chap4/flow_chart}
 	\caption[CL Flow Chart]{Flow chart broadly describing the structure
 		of the
 		code as it pertains to constructing an emulator. Dotted lines
 		indicate connections still in development as of 2 October 2023.
 		Double lines indicate references of one file in another.
 		\Verb|utils| contains code which is frequently used throughout
 		the entire pipeline. However, due to its miscellaneous nature, its
 		inclusion in this flow chart would lead to unnecessary clutter.}
 	\label{fig: flow_chart}
\end{figure}

\section{Creation of Separate Emulators}
\label{sec: 2emu_intro}

\textcolor{orange}{One consequence of this section for this chapter: our
priors files come in pairs!}

\textcolor{orange}{These prior files come in pairs so that the user may choose 
between a
massless- and a massive-neutrino emulator: the prior name need only be
coupled with the ending} \verb|_no_nu| or \verb|_with_nu|, respectively.

One potential weakness of \textcolor{orange}{this} emulator outline emerges 
from the prior range for the physical density in neutrinos. The lower bound 
($\omega_\nu = 0$) of this range for the physical density in neutrinos is 
highly firm; any negative value would be unphysical. However, this 
lower bound is also inclusive in the sense that the massless neutrino case is  
of interest to us. GP predictions are, naturally, at their 
strongest for points surrounded with training samples. Since the massless 
neutrino case represents the end of a parameter range, there will by 
definition be no samples in the $\omega_\nu < 0$ direction with which to
interpolate. Indeed, due to the nature of random sampling (including both 
Latin hypercube and simple random sampling), we do not expect \textit{any}
of our
samples to be located exactly at $\omega_\nu = 0$.
Therefore, we expect the massless-neutrino case to be a slight
\textit{extrapolation} from our training data, which is dangerous for 
accuracy.

%%% Footnote just opens you up to even worse questions
\begin{comment}
\footnote{\textcolor{orange}{What should I say to people who complain: 
``why not just manually add massless-neutrino samples to the original training 
set? Why not simply have one emulator trained over a more diverse training 
set?'' I think it would be better if we simply focus on the $\omega_\nu = 0$
case not being adequately captured by interpolation, since we don't have any
samples on the ``left'' side of $\omega_\nu = 0$.}}
\end{comment}
%%%

To address this unique problem among our parameter ranges, we train two
independent CL emulators. The primary 
emulator is trained over the aforementioned prior range,
$\omega_\nu \in [0, 0.01]$, and $A_s$ depending on the prior file used
(see tables~\ref{tab: MEGA_priors},~\ref{tab: CLASSIC_priors},
and~\ref{tab: COMET_priors}). The secondary emulator is specifically trained
to handle the massless neutrino case. In 
practice, this entails the same prior ranges for the parameters $\omega_b$, 
$\omega_c$, $n_s$, and $\sigma_{12}$. However, here we no longer need $A_s$;
as discussed in chapter~\ref{chap: A_s}, $A_s$ helps to characterize the
structure growth suppression induced by massive neutrinos. Besides this,
$A_s$ behaves as an evolution parameter, and we are already specifying the 
amplitude of the power spectrum through $\sigma_{12}$. Therefore, the
dimension of our LHS decreases from six to four.

To train this massless-neutrino emulator, we use the same pipeline introduced
in section~\ref{sec: flow_chart} and the same code as described in 
chapter~\ref{chap: implementation}. Since the
massless-neutrino emulator is trained over just four parameters, and since we 
nevertheless continue to train over 5000 samples, we expect the
massless-neutrino emulator to be universally more accurate when compared
against an analogous test cube. We also expect our massless-neutrino emulator
to perform better since it will not suffer from errors associated with missing
parameters. Recall from section~\ref{sec: fit_testing} that $A_s$ cannot be
enough to fully characterize the $\el$ values for different cosmologies. This
error will of course only affect the massive-neutrino emulator.