\chapter{Cassandra-Linear: a Python Package}

This will be a rather long, dry, and technical section with subsections based on each of the core scripts making up the Python package that I have been developing. It will in some ways paraphrase and summarize the documentation, explaining the basic use of the package as well as important limitations. Of course, fairly early on in this section, there will be a footnote linking to my GitHub repository, which will be made public once I'm ready to hand in this thesis.

Even if my code doesn't end up in a bigger repository, I nevertheless think that it's important to the scientific process that I describe in detail the code that I have written. Besides, if any readers want to experiment specifically with the ideas discussed in this thesis, they may find my code more accessible because it is, in a sense, "single-purpose"--that is to say, written almost exclusively to investigate the topics of this paper.

It only takes a couple of sentences, but we also need to describe how to install the package.


\section{Building the Latin Hypercube}
\label{sec: build_lhc}

% lhc.py

\textcolor{blue}{This section will briefly recapitulate some of the Latin hypercube sampling
explanation from the introduction. Then we will talk about how this sample is
procured in practice, and what our ``solution'' (inelegant though it may be)
is to obtaining a sample with a decent minimum separation between the points
(remember to briefly repeat why this is important).}

In this section, we will discuss the building of LHSs, which within the
Cassandra-linear is handled by the \verb|lhs.py| script.
By ``LHSs,'' we are here referring exclusively to the
sample of cosmologies for which we want to compute power spectra. In other 
words, this section elucidates our procedure for the sampling of cosmological 
parameters given a set of priors. The resultant array of cosmological
configurations will act as the $X$ data set when we train our emulator. For 
the construction of the $Y$ set, see section~\ref{sec: generate_emu_data}.

Each entry in our LHS will be either a four- or six-dimensional vector, 
depending on the emulator's support for massive neutrinos. When building a
massless-neutrino emulator, a single LHS entry will describe the $\omega_b$,
$\omega_\text{CDM}$, $n_s$, and $\sigma_{12}$ values for that cosmology in 
this order. When building a massive-neutrino emulator, a single LHS entry will 
additionally describe the $A_s$ and $\omega_\nu$ values for that cosmology, 
again in this order.

To get started generating LHSs, we integrated demonstration code written by 
Daniel Farrow \text{red}{(and who else?) add these people to the Danksagung} 
during the CAKE 2021 workshop. We begin by invoking the \verb|lhs| function of
\verb|pyDOE2| a number of times. Each time we get a random new unit LHC, whose
spacing we analyze with the \verb|cdist| function from
\verb|scipy.spatial.distance|.  Recall from section~\ref{sec: lhc_theory} that
our sample more evenly covers the sample space if the minimum separation 
between the sample points increases. \textcolor{green}{Uneven separations will
result in excess error depending on the location of the cosmology in parameter
space}. Therefore, from among the numerous calls of \verb|lhs| we select the
hypercube sample with the largest minimum separation.

In order to maximize the number of random hypercubes generated per unit of
wall time, we have written a multithreaded function called
\verb|multithread_unit_LHC_builder| \textcolor{orange}{Keep your eye on this
one, I'm thinking about condensing lhc.py to just a couple of functions.}
Through the function parameter \verb|num_workers| the user is free to assign
any number of CPU threads to the task, so in principle the computer can be
wholly dedicated to the optimization of the LHS.  

As of the current (\textcolor{orange}{24.08.2023}) version of the
Cassandra-Linear code, this brute-force approach is the method used to build
the Latin hypercube samples eventually used to train the emulators. To 
understand the computational and wall-time costs associated with our
brute-force solution, we include plots XXY and XXY.

Remember from section~\ref{sec: lhc_theory} that we have an equation for the
best possible minimum separation that we can achieve with a Latin hypercube
of some given dimension and total number of samples. Let us consider a massless-neutrino emulator (a total of four dimensions per cosmology vector)
trained over five-thousand CAMB spectra. If we plug in $d = 4$ and $N=5000$,
we get XXY. Similarly, for our massive-neutrino emulator (a total of six 
dimensions per cosmology) trained over the same number of spectra, we find a 
theoretical best minimum-separation of XXY. 

It is based on figures XXY and XXY that we may claim that our approach is
inefficient and not well-suited to the approach of finding LHCs with large
minimum separations. As mentioned in section~\ref{sec: lhc_theory},
we value a high minimum separation because it means we are sampling the
space of cosmologies evenly rather than oversampling particular regions.
Therefore, we expect the consequence of a low minimum separation in the LHC to
be a higher error variance as well as a slightly higher average bias.

% One could argue that the variance in oversampled regions will decrease to
% compensate, right? I need to clarify that the decrease in variance in the
% oversampled regions would not be able to compensate the increased variance
% in the undersampled regions, because the space of power spectra is
% continuous, so closely spaced points reveal less about the true function
% than well spaced points.

To appreciate the impact of this approach's limitations on the performance of
the resultant emulator, we direct the reader to
section~\ref{sec: error_from_lhc}.

%! Maybe we should move get_param_ranges to lhc.py. It would be more thematically consistent if the note on priors appeared in this section

Are there more efficient techniques than this brute force random generation?
We discuss this later. 


\section{Interfacing with CAMB}

% camb\_interface.py

This section won't actually introduce much that the reader hasn't already encountered in the section "CAMB, initial setup." This will primarily be a summary of the ``correct'' settings with a couple of new sections describing the particular functions in this script.

\section{Integrating Evolution Mapping}
\label{sec: generate_emu_data}

% generate\_emu\_data.py

This section will describe the process going from a Latin hypercube to a set of CAMB power spectra. We will describe how an array of six parameter values is fleshed out into a params object understood by CAMB. We will describe the procedure of modifying $h$ and $z$ until we arrive roughly at the $\sigma_{12}$ value that we desire, as well as the process of writing the \textit{actual} $\sigma_{12}$ value that we obtained back to the original hypercube. Why is this important? Because the Latin hypercube is used again later in the user\_interface.py script for the purpose of training the Gaussian emulator object.

We'll also describe the switch from a rescaled Latin hypercube to a unit Latin hypercube that we later interpret differently according to the specified priors.

The priors that we use correspond to those currently used by COMET. \textcolor{orange}{Should I spend any time defending this choice here, or should I put all of the defense in section~\ref{sec: priors}?}


\section{Training the Emulator}
\label{sec: emu_training}

% train\_emu.py

This section will focus on the particular lines of GPy that we used, as well
as the various data-cleaning and normalization statements that we used.
Normalization will be a really important topic. I want to explain some of
the theory behind why the emulator performs poorly with values outside of this
[0, 1] range.

I don't know how to justify this kernel that I got from Alex via AndreaP...

\section{Accessing and Using the Emulator}

% user\_interface.py

The content of this section is still relatively uncertain. Since I am still tweaking some of the emulator's settings, I haven't spent too much time on a script dedicated purely to simplifying the interaction between the user and the emulator \textit{object} itself.

Anyway, the hope is to provide some clear and simple descriptions of what functions the user should turn to in order to get started predicting power spectra using the results of this thesis. The functions can either create a new emulator object and train it or load an existing one with easy-to-understand functions for interacting with the object.

Here, we will talk about the details of the training as well as the importance of normalization. Since this is still technical, it may not be appropriate for a UI script. So, I am considering breaking out a new script (and therefore a new section), maybe called ``train\_emu.py.''
