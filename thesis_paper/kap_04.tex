\chapter{Conceptual Outline of Our Emulator Code and Pipeline}

\textcolor{blue}{The point of this section is to introduce the reader to the
basic structure of the emulator pipeline. Most of the discussion will follow
from a flow chart showing the various conceptual pieces of the emulator. Also
in this section, I will introduce various choices made (e.g. 5000 training
spectra, 300 points for each spectrum) WITHOUT justifying any of them--later,
in chapter~\ref{chap: disc_and_conc}, we'll justify these decisions and
talk about the consequences of alternate settings.}

CL represents a full emulation pipeline from the generation of training and 
testing data to the plotting of error statistics associated with
the emulation.

% We're gonna need a flow chart for this puppy. That should be priority number
% one, because the writing should constantly refer back to it.

\section{Pipeline and Design Choices}
\label{sec: flow_chart}

% Disgusting hack to get verbatim functionality in tabel



\begin{comment}

\SaveVerb{lhs}|lhs.py|
\SaveVerb{ged}|generate_emu_data.py|
\SaveVerb{ci}|camb_interface.py|
\SaveVerb{te}|train_emu.py|
\SaveVerb{ui}|user_interface.py|

\end{comment}

% The following plots were generated with h_units_bad.ipynb
\begin{figure}[ht!]
    \centering
 	\includesvg[width=\textwidth]{chap4/flow_chart}
 	\caption[CL Flow Chart]{Flow chart describing the broad structure of the
 		code as it pertains to constructing an emulator. Dotted lines
 		indicate connections still in development as of 2 October 2023.
 		Double lines indicate references of one file in another.
 		\Verb|utils| contains code which is frequently used throughout
 		the entire pipeline. However, due to its miscellaneous nature, its
 		inclusion in this flow chart would lead to unnecessary clutter.}
 	\label{fig: flow_chart}
\end{figure}

\begin{comment}
\UseVerb{utils}
\end{comment}

By default, each Latin hypercube consists of $N_s = 5000$ entries. For 
justification of this quantity, please refer to
section~\ref{sec: num_samples}.

$N_k = 300$.

\begin{table}[ht!]
\centering
\begin{tabular}{l|c|l}
\hline
Script & Abbreviation & purpose \\ \hline
\Verb|lhs.py| & lhs & Creates unit LHSs. \\
\Verb|generate_emu_data.py| & ged & Fill in hypercube with spectra. \\
\Verb|camb_interface.py| & ci & Computes an individual spectrum. \\
\Verb|train_emu.py| & te & Trains and tests emulator. \\
\Verb|user_interface.py| & ui & Bridges lhs, ged, and te scripts. \\
\Verb|utils.py| & utils & Miscellaneous utility functions. 
\end{tabular}
 \cprotect\caption[Summary of CL Scripts]{Maybe we should axe the last
 	column.}
 \label{tab: script_summary}
\end{table}


\section{LHCs}
\label{sec: lhc_outline}

The \verb|pyDOE2| function that we will use to generate our LHSs
returns a result with entries that range from zero to unity. In continuity
with the language of section~\ref{sec: lhc_theory}, we will refer
to such LHSs as \textit{unit} LHSs.\footnote{We will later denote LHSs with 
the matrix notation $\matr{X}$ and collections of CAMB power spectra as
$\matr{Y}$ to
emphasize the roles of these data in the training and testing of the emulator. 
However, we will continue to use the term ``unit'' as introduced here, and 
the reader should keep in mind that a unit $\matr{X}_u$ therefore does not
indicate an identity matrix.} These unit LHSs are saved as \verb|npy| files
and later rescaled according to the desired parameter ranges. This allows us
to build many different emulators from a single LHS file.

(For example, $\sigma_{12}^2$ sampling--although we did not
get it to work, the infrastructure is still there for whomever comes along in
the future.)

% One reason this goes here: priors show up in almost every script,
% so there isn't a great cass-L section in which to put this.

CL accomplishes this rescaling through the use of prior files.
CL is packaged with three pairs of prior files. The parameter domains are
described in tables~\ref{tab: MEGA_priors},~\ref{tab: CLASSIC_priors},
and~\ref{tab: COMET_priors}. The $\omega_\nu$ prior is always $[0, 0.01]$ and 
the $\sigma_{12}$ prior is always $[0.2, 1]$.

These prior files come in pairs so that the user may choose between a
massless- and a massive-neutrino emulator: the prior name need only be
coupled with the ending \verb|_no_nu| or \verb|_with_nu|, respectively.

To create a new prior file, the user can simply copy one of the
existing files as a template. The format is very simple, but must be
followed carefully for the file to be read correctly.

\begin{table}[ht!]
\centering
\begin{tabular}{l|l|l}
\hline
Parameter & Minimum Value & Maximum Value \\ \hline
$\omega_b$ & 0.005 & 0.28 \\
$\omega_c$ & 0.001 & 0.99 \\
$n_s$ & 0.7 & 1.3 \\
$A_s$\footnotemark & $5.003 \cdot 10^{-10}$ & $1.484 \cdot 10^{-8}$  \\
\end{tabular}
 \cprotect\caption[``MEGA'' priors]{``MEGA'' priors. The range of allowed
 	values is significantly larger than the modern $3\sigma$ intervals
 	for these parameters. It was created from the most extreme values used
 	in table 1 of \citet{Mancini} and table 1 of \citet{Arico}, even including
 	CMB emulators. 
 	\textcolor{red}{Maybe since we didn't end up using this prior file, I
 	should scrap it from the discussion?}}
 \label{tab: MEGA_priors}
\end{table}

\begin{table}[ht!]
\centering
\begin{tabular}{l|l|l}
\hline
Parameter & Minimum Value & Maximum Value \\ \hline
$\omega_b$ & 0.01875 & 0.02625 \\
$\omega_c$ & 0.05 & 0.255 \\
$n_s$ & 0.84 & 1.1 \\
$A_s$\footnotemark & $1.049 \cdot 10^{-9}$ & $4.990 \cdot 10^{-9}$  \\
\end{tabular}
	\cprotect\caption[``CLASSIC'' priors]{``CLASSIC'' priors. Despite
 	significantly smaller ranges than table~\ref{tab: MEGA_priors}, this
 	parameter space is still easily large enough to accommodate modern
 	statistical analyses. \textcolor{green}{citation.}}
 \label{tab: CLASSIC_priors}
\end{table}

\begin{table}[ht!]
\centering
\begin{tabular}{l|l|l}
\hline
Parameter & Minimum Value & Maximum Value \\ \hline
$\omega_b$ & 0.0205 & 0.02415 \\
$\omega_c$ & 0.085 & 0.155 \\
$n_s$ & 0.92 & 1.01 \\
$A_s$\footnotemark & $1.049 \cdot 10^{-9}$ & $4.990 \cdot 10^{-9}$  \\
\end{tabular}
	\cprotect\caption[``COMET'' priors]{``COMET'' priors.
	These values are significantly more restrictive than in
	tables~\ref{tab: MEGA_priors} and~\ref{tab: CLASSIC_priors},
	but the parameter space should still be large enough to accommodate
	most modern statistical analyses. \textcolor{green}{citation.}
	These priors were taken from table 2 of \citet{Eggemeier}.}
 \label{tab: COMET_priors}
\end{table}

All intervals are sampled uniformly, \textcolor{orange}{although some
experimental code exists to interpret the same hypercube as a uniform sampling
in root or square space, for example}. At first, we used the highly ambitious 
(relative to modern uncertainty bars) ``MEGA'' set of priors
(table~\ref{tab: MEGA_priors}),
whose values we decided based on the recent emulator papers from
\citet{Mancini} and \citet{Arico}.
However, during the process of generating training data, 
we found that CAMB was not able to match all of the necessary $\sigma_{12}$ 
values. We refer to such a situation as an unsolvable cell, and will explain
why it happens in section~\ref{sec: generate_emu_data}, which covers the
application of evolution mapping principles within the CL code.

\textcolor{orange}{Talk a little more about the prior ranges, what are their
values in words?}

The default priors that we use correspond to those currently used by COMET. 
\textcolor{orange}{Two justifications for this choice: unsolvable cells, but 
more than that, simplicity and accuracy for the ``demo'' run.}

\section{Creation of Separate Emulators}
\label{sec: 2emu_intro}

One potential weakness of \textcolor{orange}{this} emulator outline emerges 
from the prior range for the physical density in neutrinos. The lower bound 
($\omega_\nu = 0$) of this range for the physical density in neutrinos is 
highly firm; any negative value would be unphysical. However, this 
lower bound is also inclusive in the sense that the massless neutrino case is  
of interest to us. GP predictions are, naturally, at their 
strongest for points surrounded with training samples. Since the massless 
neutrino case represents the end of a parameter range, there will by 
definition be no samples in the $\omega_\nu < 0$ direction with which to
interpolate. Indeed, due to the nature of random sampling (including both 
Latin hypercube and simple random sampling), we do not expect \textit{any}
of our
samples to be located exactly at $\omega_\nu = 0$.
Therefore, we expect the massless-neutrino case to be a slight
\textit{extrapolation} from our training data, which is dangerous for 
accuracy.

%%% Footnote just opens you up to even worse questions
\begin{comment}
\footnote{\textcolor{orange}{What should I say to people who complain: 
``why not just manually add massless-neutrino samples to the original training 
set? Why not simply have one emulator trained over a more diverse training 
set?'' I think it would be better if we simply focus on the $\omega_\nu = 0$
case not being adequately captured by interpolation, since we don't have any
samples on the ``left'' side of $\omega_\nu = 0$.}}
\end{comment}
%%%

To address this unique problem among our parameter ranges, we train two
independent CL emulators. The primary 
emulator is trained over the aforementioned prior range,
$\omega_\nu \in [0, 0.01]$, and $A_s$ depending on the prior file used
(see tables~\ref{tab: MEGA_priors},~\ref{tab: CLASSIC_priors},
and~\ref{tab: COMET_priors}). The secondary emulator is specifically trained
to handle the massless neutrino case. In 
practice, this entails the same prior ranges for the parameters $\omega_b$, 
$\omega_c$, $n_s$, and $\sigma_{12}$. However, here we no longer need $A_s$;
as discussed in chapter~\ref{chap: A_s}, $A_s$ helps to characterize the
structure growth suppression induced by massive neutrinos. Besides this,
$A_s$ behaves as an evolution parameter, and we are already specifying the 
amplitude of the power spectrum through $\sigma_{12}$. Therefore, the
dimension of our LHS decreases from six to four.

To train this massless-neutrino emulator, we use the same pipeline introduced
in section~\ref{sec: flow_chart} and the same code as described in 
chapter~\ref{chap: implementation}. Since the
massless-neutrino emulator is trained over just four parameters, and since we 
nevertheless continue to train over 5000 samples, we expect the
massless-neutrino emulator to be universally more accurate when compared
against an analogous test cube. We also expect our massless-neutrino emulator
to perform better since it will not suffer from errors associated with missing
parameters. Recall from section~\ref{sec: fit_testing} that $A_s$ cannot be
enough to fully characterize the $\el$ values for different cosmologies. This
error will of course only affect the massive-neutrino emulator.
