\chapter{Conceptual Outline of Our Emulator Code and Pipeline}

\textcolor{blue}{The point of this section is to introduce the reader to the
basic structure of the emulator pipeline. Most of the discussion will follow
from a flow chart showing the various conceptual pieces of the emulator. Also
in this section, I will introduce various choices made (e.g. 5000 training
spectra, 300 points for each spectrum) WITHOUT justifying any of them--later,
in chapter~\ref{chap: disc_and_conc}, we'll justify these decisions and
talk about the consequences of alternate settings.}

In order to simplify the process of testing different emulator setups, and to 
make power spectra more accessible to beginners in the field, we have 
developed a Python package, which we call Cassandra-Linear (CL)1. CL 
represents a full emulation pipeline from the generation of training and 
testing data to the plotting of error statistics associated with
the emulation.

% We're gonna need a flow chart for this puppy. That should be priority number
% one, because the writing should constantly refer back to it.

\section{Pipeline and Design Choices}

By default, each Latin hypercube consists of $N_s = 5000$ entries. For 
justification of this quantity, please refer to
section~\ref{sec: num_samples}.

$N_k = 300$.


\section{LHCs}
\label{sec: lhc_outline}

The \verb|pyDOE2| function that we will use to generate our LHSs
returns a result with entries that range from zero to unity. In continuity
with the language of section~\ref{sec: lhc_intro}, we will refer
to such LHSs as \textit{unit} LHSs.\footnote{We will later denote LHSs with 
the matrix notation $\matr{X}$ and collections of CAMB power spectra as
$\matr{Y}$ to
emphasize the roles of these data in the training and testing of the emulator. 
However, we will continue to use the term ``unit'' as introduced here, and 
the reader should keep in mind that a unit $\matr{X}_u$ therefore does not
indicate an identity matrix.} These unit LHSs are saved as \verb|npy| files
and later rescaled according to the desired parameter ranges. This allows us
to build many different emulators from a single LHS file.

(For example, $\sigma_{12}^2$ sampling--although we did not
get it to work, the infrastructure is still there for whomever comes along in
the future.)

% One reason this goes here: priors show up in almost every script,
% so there isn't a great cass-L section in which to put this.

CL accomplishes this rescaling through the use of prior files.
CL is packaged with three pairs of prior files. The parameter domains are
described in tables~\ref{tab: MEGA_priors},~\ref{tab: CLASSIC_priors},
and~\ref{tab: COMET_priors}. The $\omega_\nu$ prior is always $[0, 0.01]$ and 
the $\sigma_{12}$ prior is always $[0.2, 1]$.

These prior files come in pairs so that the user may choose between a
massless- and a massive-neutrino emulator: the prior name need only be
coupled with the ending \verb|_no_nu| or \verb|_with_nu|, respectively.

To create a new prior file, the user can simply copy one of the
existing files as a template. The format is very simple, but must be
followed carefully for the file to be read correctly.

\begin{table}[ht!]
\centering
\begin{tabular}{l|l|l}
\hline
Parameter & Minimum Value & Maximum Value \\ \hline
$\omega_b$ & 0.005 & 0.28 \\
$\omega_c$ & 0.001 & 0.99 \\
$n_s$ & 0.7 & 1.3 \\
$A_s$\footnotemark & $5.003 \cdot 10^{-10}$ & $1.484 \cdot 10^{-8}$  \\
\end{tabular}
 \cprotect\caption[``MEGA'' priors]{``MEGA'' priors. The range of allowed
 	values is significantly larger than the modern $3\sigma$ intervals
 	for these parameters. It was created from the most extreme values used
 	in table 1 of \citet{Mancini} and table 1 of \citet{Arico}, even including
 	CMB emulators. 
 	\textcolor{red}{Maybe since we didn't end up using this prior file, I
 	should scrap it from the discussion?}}
 \label{tab: MEGA_priors}
\end{table}

\begin{table}[ht!]
\centering
\begin{tabular}{l|l|l}
\hline
Parameter & Minimum Value & Maximum Value \\ \hline
$\omega_b$ & 0.01875 & 0.02625 \\
$\omega_c$ & 0.05 & 0.255 \\
$n_s$ & 0.84 & 1.1 \\
$A_s$\footnotemark & $1.049 \cdot 10^{-9}$ & $4.990 \cdot 10^{-9}$  \\
\end{tabular}
	\cprotect\caption[``CLASSIC'' priors]{``CLASSIC'' priors. Despite
 	significantly smaller ranges than table~\ref{tab: MEGA_priors}, this
 	parameter space is still easily large enough to accommodate modern
 	statistical analyses. \textcolor{green}{citation.}}
 \label{tab: CLASSIC_priors}
\end{table}

\begin{table}[ht!]
\centering
\begin{tabular}{l|l|l}
\hline
Parameter & Minimum Value & Maximum Value \\ \hline
$\omega_b$ & 0.0205 & 0.02415 \\
$\omega_c$ & 0.085 & 0.155 \\
$n_s$ & 0.92 & 1.01 \\
$A_s$\footnotemark & $1.049 \cdot 10^{-9}$ & $4.990 \cdot 10^{-9}$  \\
\end{tabular}
	\cprotect\caption[``COMET'' priors]{``COMET'' priors.
	These values are significantly more restrictive than in
	tables~\ref{tab: MEGA_priors} and~\ref{tab: CLASSIC_priors},
	but the parameter space should still be large enough to accommodate
	most modern statistical analyses. \textcolor{green}{citation.}
	These priors were taken from table 2 of \citet{Eggemeier}.}
 \label{tab: COMET_priors}
\end{table}

All intervals are sampled uniformly, \textcolor{orange}{although some
experimental code exists to interpret the same hypercube as a uniform sampling
in root or square space, for example}. At first, we used the highly ambitious 
(relative to modern uncertainty bars) ``MEGA'' set of priors
(table~\ref{tab: priors}),
whose values we decided based on the recent emulator papers from
\textcolor{green}{Spurio Mancini et al 2021. and the other paper, I can't
remember the name.} However, during the process of generating training data, 
we found that CAMB was not able to match all of the necessary $\sigma_{12}$ 
values. We refer to such a situation as an unsolvable cell, and will explain
why it happens in section~\ref{sec: generate_emu_data}, which covers the
application of evolution mapping principles within the CL code.

\textcolor{orange}{Talk a little more about the prior ranges, what are their
values in words?}

The default priors that we use correspond to those currently used by COMET. 
\textcolor{orange}{Two justifications for this choice: unsolvable cells, but 
more than that, simplicity and accuracy for the ``demo'' run.}

\section{Creation of Separate Emulators}
\label{sec: 2emu_intro}

\textcolor{blue}{This is a brief introductory section motivating the creation 
of two emulators:
unlike the other five parameters, the physical density in massive neutrinos 
has a hard lower bound. This creates some minor regression problems (appeal to 
boundary effects / danger of extrapolation).}

One potential weakness of \textcolor{orange}{this} emulator outline emerges 
from the prior range for the physical density in neutrinos. The lower bound 
($\omega_\nu = 0$) of this range for the physical density in neutrinos is 
highly firm; any negative value would be totally unphysical. However, this 
lower bound is also inclusive in the sense that the massless neutrino case is 
still very much of interest to us. GP predictions are, naturally, at their 
strongest in the regions densest with training samples. Since the massless 
neutrino case represents the end of a parameter range, the sampling must be 
less dense there. Indeed, no massless-neutrino cases are even in our training 
sample, XXX being the minimum value of $\omega_\nu$ in our 
LHS.\footnote{\textcolor{orange}{What should I say to people who complain: 
``why not just manually add massless-neutrino samples to the original training 
set? Why not simply have one emulator trained over a more diverse training 
set?'' I think it would be better if we simply focus on the $\omega_\nu = 0$
case not being adequately captured by interpolation, since we don't have any
samples on the ``left'' side of $\omega_\nu = 0$.}}
Therefore, the massless-neutrino case is actually \textcolor{green}{a slight
extrapolation} from our training data, which is dangerous for accuracy.

To 
address this unique problem among our parameter ranges, CL can automatically 
access one of two emulators depending on the user’s input. The primary 
emulator is trained over the aforementioned prior range in $\omega_\nu$
(table~\ref{tab: neutrino_priors}). In addition to this, we’ve trained a 
separate emulator specifically to handle the massless neutrino case. In 
practice, this entails the same prior ranges for the parameters $\omega_b$, 
$\omega_c$, $n_s$, and $\sigma_{12}$. However, here we no longer need $A_s$;
as discussed in chapter~\ref{chap: A_s}, $A_s$ helps to characterize the
structure growth suppression induced by massive neutrinos. Besides this, for 
the purposes of our training and validation data, $A_s$ is redundant since it
is otherwise an evolution parameter, and we are already specifying the 
amplitude of the power spectrum through sigma12. 

To train this massless-neutrino emulator, we use the same code as described in 
in this chapter and chapter~\ref{chap: implementation}. Since the
massless-neutrino emulator is trained over just four parameters, and since we 
nevertheless continue to train over 5000 samples, we expect the
massless-neutrino emulator to be universally more accurate. However, as
discussed and plotted in section~\ref{sec: num_samples}, our decision to train 
over 5000 is justified by the rapidly diminishing returns in additional 
accuracy.

\textcolor{orange}{The following paragraph is bad. I'm not sure yet if I can
salvage anything. The claims are not true and the work is not yet done.}

Therefore, the reason why the massless-neutrino emulator is more 
accurate is because massive neutrinos in particular have a highly complicated 
impact on the power spectrum. Even with the inclusion of the $\omega_\nu$ and 
$A_s$ parameters, our investigations in chapter~\ref{chap: A_s} reveal that
these two parameters may not be sufficient to fully characterize the 
suppression of the power spectrum by massive neutrinos. Of course, we would 
like to reiterate that, though we have applied a symbolic regression approach 
in order to independently justify our analytical expression from
chapter~\ref{chap: A_s}, this by no means guarantees that we have found the 
optimal analytical expression.

To simplify the user experience, this two-emulator solution lives ``under the
hood'' and by default \textcolor{orange}{will be} hidden behind an interface
which automatically queries the correct emulator given some user-input
cosmology.

To justify our decision and to quantify the improvement from this approach, we
have prepared \textcolor{orange}{some} plots in
section~\ref{sec: 2emu_improvement}.

\textcolor{orange}{Conclude this section by saying that this integration of 
multiple emulators into one user-facing script can be further exploited with, 
for example, different emulators for different neutrino mass hierarchies.}

Besides its relevance to the neutrino suppression of the small-scale power
spectrum, $A_s$ appears to be an evolution parameter. Therefore, when the
emulator discussion branches into the massless- and massive-neutrino cases
(refer to section~\ref{sec: 2emu_intro} for the beginning and motivation of
this), the reader should keep in mind that the dimension of the parameter
space decreases in the massless-neutrino case by \textit{two}--$\omega_\nu$ 
obviously must vanish because we here assume $\omega_\nu=0$. However, $A_s$
also vanishes because, without massive neutrinos, $A_s$ reduces to an
evolution parameter and is therefore redundant with $\sigma_{12}$.

% What happens to A_s in the massless-neutrino case? Is it fixed at the
% default for model 0? I'm pretty sure it is!
