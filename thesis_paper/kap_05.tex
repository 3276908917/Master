\chapter{Results and Analysis}
\label{chap: results}

In this chapter, we will discuss the performance of a two-emulator setup
built using the default configuration of CL: both the massless- and
massive-neutrino emulators appraised here use the COMET priors
(table~\ref{tab: COMET_priors}), $N_k = 300$, and $N_s = 3000$ for both the
training and testing data sets.

The final training LHS had an $s^*$ of 0.07972 in the massive case and 
0.02274 in the massless case. The final testing LHS had an $s^*$ of 0.07275 in 
the massive case and 0.02015 in the massless case. All numbers have been 
rounded to four significant figures.

\section{Quantifying the Performance of the Emulator}

To evaluate the performance of the emulator, we use the testing pipeline
explained in chapters~\ref{chap: emu_outline} and~\ref{chap: implementation}.
That is, we create an LHS of test cosmologies, which the training stages have
never seen, and compare the output of the emulator object to that of
CAMB. As in neural network contexts, GPR performance is typically evaluated
using test sets (\citealp{Mancini}, \citealp{Arico}, and
\citealp{Eggemeier}).\footnote{However, these test sets are
more commonly referred to as ``validation sets.'' We use the phrase
``test set'' to call to mind the three conventional data sets in a machine
learning setup: training, validation, and testing. This distinction will
prove useful in section~\ref{sec: future_work}.}

We appreciate that appraisal based on test sets
may not provide hard boundaries on the true
range of errors associated with the emulators,
because the continuity of the parameter hypervolume means that an infinite
number of cosmologies could be tested. Therefore, traditional
goodness-of-fit tests such as $\chi^2$ do not apply here.
However, precisely because
power spectra vary smoothly in this space\footnote{Indeed, without this
property, interpolation would be unproductive.}, we expect the error 
curves to vary similarly. So long as the test LHS represents a reasonable
coverage of the parameter space, we expect the errors calculated therefrom to
be similarly representative of emulator performance. On the other
hand, at the edges of the parameter space, where interpolation begins to
break down, the errors will be at their highest, and the following analyses
may not be representative of these edge cases.

For this chapter as well as chapter~\ref{chap: disc_and_conc}, we will focus
on just two error metrics: percent error and squared error. We include percent
error as it is more common in the literature and because it is immediately
interpretable (\citealp{Mancini}, \citealp{Arico}, and
\citealp{Eggemeier}). By contrast, the squared errors are difficult to
understand unless compared across multiple similar cases.
Nevertheless, we argue
that squared errors represent a more useful metric, at least within a single
paper, because they are unbiased with respect to the magnitude of the emulated
quantity. Consider that $P(k)$ is smallest at the largest $k$; if an emulator
mispredicts $P(k)$ with a constant offset, then the percent error curves will
be largest at the smallest $k$. As a further example, consider that the
overall amplitude of $P(k)$ is smaller for smaller $\tilde{\sigma}_{12}$;
if we again imagine a constant offset, $\tilde{\sigma}_{12}$ will appear as a
problematic parameter if we look only at percent error.


\section{Percent and Square Errors on Random Cosmologies}

* Percent error curves massless

* Squared error curves massless

* Percent error curves massive

* Squared error curves massive

This will be a fairly short section, basically just showing the plot of 5000 
error curves in these two ways.

\textcolor{orange}{Create some plots focusing on the BAO error spikes? i.e.
zoom-in on k-ranges.}

\textcolor{green}{redo the numbers in this next paragraph}

The current state of the massless-neutrino emulator features sub-0.1\% error 
for the vast majority of scales, with the exception of some wild fluctuations  
between k=0.09 and 0.2 / Mpc. Outside of this range, the errors tend to 
sub-0.01\% in the large-scale regime and sub-0.005\% in the small-scale 
regime. Please refer to figures XXX through XXX for illustrations thereof.

As we can see from figures~\ref{fig: massless_default} 
and~\ref{fig: massive_default}, 
both emulators are most accurate at very large and
very small $k$, with significant difficulties around the BAO region. We
consider the small-scale performance a powerful confirmation of our
techniques, as the impact of massive neutrinos is most significant at small
scales.

We highlight that the massive emulator performs significantly worse than its 
massless counterpart. The emulators are difficult to compare because they
were constructed from LHSs of different dimension: four in the massless case,
six in the massive case. Since $N_s = 5000$ is constant, we expect the 
massless LHS to sample the parameter space much more densely than the massive
LHS. In any case, the discrepancy in accuracies will be worsened by the
approximate nature of our fit from section~\ref{sec: proposed_fit}:
the results of section~\ref{sec: fit_testing} indicate
that evolution parameters besides $A_s$ are necessary for complete
characterization of the impact of massive neutrinos. Nevertheless, the
overall percent error of the massive emulator is \textcolor{green}{nearly}
at the level of CAMB itself, so we consider the massless emulator an
encouraging success.

As evolution-mapping emulation for \textit{massive}-neutrino cosmologies
is the novel feature of this work, we will henceforth concentrate exclusively
on the massive-neutrino emulator.

\textcolor{orange}{I plan to spend some time talking 
about \textit{why} parameter x is the current biggest problem for the 
emulator.}

\textcolor{orange}{It would have been nice if you had done like Andrea said,
and produced a plot colored by the ``extremeness'' of the parameters. We
could implement an extremeness index for parameter $x$ simply by taking
subtracting 0.5 and taking the absolute value, assuming that $x$ comes from
the unit LHS. Then, we could take the average over six parameters to get
an extremeness index for the cosmology as a whole. Optionally, for plot
optimization, you could multiply the final value by 2 so that the extremeness
runs from 0 to 1.}


\section{Improvement from Two-emulator Solution}
\label{sec: 2emu_improvement}

We conclude this chapter with a brief demonstration of the advantage of our
two-emulator solution. We will task the massive-neutrino emulator with
predictions over the test set of the massless-neutrino emulator. We consider
this a fair test because we sampled $\omega_\nu$ values for the
massive-neutrino emulator from a uniform distribution [0, 0.01]. Therefore,
in principle, the $\omega_\nu = 0$ case should only be a slight extrapolation.

To understand these results, we remind the reader the overall error of the
massive emulator is larger than that of the massless emulator. As mentioned
in 

To justify our decision and to quantify the improvement from this approach, we
have prepared \textcolor{orange}{some} plots in
section~\ref{sec: 2emu_improvement}.

\textcolor{blue}{This will be an extremely short section with some error
plots of the massive-neutrino cosmology evaluating massless-neutrino
cosmologies.}

\section{Minimum Separation of the Training LHC}
\label{sec: error_from_lhc}

What is the impact of the minimum separation? Surely the minimum separation
should be a proxy for the evenness of the coverage of the space of
cosmologies. Therefore, we expect the error variance to increase much more
dramatically than, say, the average bias.

How would we be able to quantify the error due to this? We could try to 
compare the emulator performance trained on hyper cubes of various minimum 
distances.

\section{Resolution of the k Axis}

This might go better in the CassL section, but I think I ought to motivate the 
decision to use length-300 arrays.

This is a special case where we would argue that percent error is more useful 
than squared error.
That is because percent error can be considered on its own, whereas squared
error relies on comparison for its meaning to emerge. But in this case, the
comparisons are obscure. Consider, for example, comparing an $N_k = 100$
emulator with an $N_k = 500$ emulator: the $N_k = 100$ emulator will have
fewer points from which to compute errors. Simple interpolation is not enough
to restore direct comparability, as the $N_k = 500$ emulator by definition
will suffer less from interpolation than the $N_k = 100$ emulator, for which
each point will have to inform five times as much as the interpolated function
domain.

\section{Number of Training Samples}
\label{sec: num_samples}

\textcolor{blue}{Justify choice of 5000 samples for each: maybe we can make a
trend plot showing diminishing returns in test error?}

% This might go better in the CassL section, but I think I ought to motivate 
% the decision to use 5000 training arrays.

\textcolor{orange}{I'll have to concede that the results of this section are 
not entirely comprehensive; we didn't train any emulators over the 
uncertainties of analogous validation hypercubes. All comparisons here use the
simpler pipeline of just two data sets, training and testing.}
