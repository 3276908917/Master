\chapter{Implementation Details}
\label{chap: implementation}

\textcolor{blue}{This will be a rather long, dry, and technical section with 
subsections based on each of the core scripts making up the Python package \
that I have been developing. It will in some ways paraphrase and summarize the 
documentation, explaining the basic use of the package as well as important 
limitations. Of course, fairly early on in this section, there will be a 
footnote linking to my GitHub repository, which will be made public once I'm 
ready to hand in this thesis.}

\textcolor{blue}{Even if my code doesn't end up in a bigger repository, I 
nevertheless think that it's important to the scientific process that I 
describe in detail the code that I have written. Besides, if any readers want 
to experiment specifically with the ideas discussed in this thesis, they may 
find my code more accessible because it is, in a sense, ``single-purpose''--that 
is to say, written almost exclusively to investigate the topics of this 
paper.}

\textcolor{orange}{It only takes a couple of sentences, but we also need to 
describe how to install the package.}

\section{Building the Latin Hypercube}
\label{sec: build_lhc}

% lhc.py

In this section, we will discuss the building of LHSs, which within CL is 
handled by \texttt|lhs|. As mentioned in section~\ref{sec: lhc_outline},
we first create a unit LHS and then later rescale it according to a set of
priors.

Each entry $\bm{x}$ in our LHS $\matr{X}$ will be either a four- or
six-dimensional vector, 
depending on the emulator's support for massive neutrinos. When building a
massless-neutrino emulator, each $\bm{x}$ will describe the $\omega_b$,
$\omega_c$, $n_s$, and
$\tilde{\sigma}_{12}$ values for a different cosmology in 
this order. When building a massive-neutrino emulator, each $\bm{x}$ will 
additionally specify the $A_s$ and $\omega_\nu$ values for that cosmology, 
again in this order.

To get started generating LHSs, we integrated demonstration code written by 
Daniel Farrow \textcolor{orange}{(and who else?) add these people to the 
Danksagung}. We begin by invoking the \texttt{lhs} function of
\texttt{pyDOE2} a number of times. Each time we get a random new unit LHC, 
whose spacing we analyze with the \texttt{cdist} function from
\texttt{scipy.spatial.distance}.
Recall from section~\ref{sec: lhc_theory} that
our samples more evenly cover the sample space if the minimum separation
$s^*$ increases. Therefore, from among the numerous calls of \texttt{lhs} we 
select the LHS with the largest minimum separation.

In order to maximize the number of random hypercubes generated per unit of
wall time, we have written a multithreaded function called
\verb|multithread_unit_LHC_builder| \textcolor{orange}{Keep your eye on this
one, I'm thinking about condensing lhc.py to just a couple of functions.}
Through the function parameter \verb|num_workers| the user is free to assign
any number of CPU threads to the task; in principle, a CPU can be
wholly dedicated to the optimization of the LHS.
\verb|multithread_unit_LHC_builder| does not contain a return statement but
instead triggers an unending query for LHSs. The user is notified via command
line printout whenever an LHS has been generated whose $s^*$ exceeds
the previous record. Whenever such a superior LHS is
encountered, the function writes the LHS to a file and continues. Therefore,
the user may run this script in the background and terminate whenever. The
function always writes LHSs to the same file, so each new record-setting LHS
overwrites the old one. When the user terminates the function, whatever output
file remains represents the best LHS seen since the function was first called.

%%% It's just too much detail for a master's thesis, no one will care
\begin{comment}
The \verb|cdist| function can be re-used to compare LHSs loaded from 
different
files. However, since there is generally little reason to keep old LHSs
(except, perhaps, to reconstruct specific emulators), it reduces clutter to
simply continue overwriting the same file. Therefore, the function
\verb|multithread_unit_LHC_builder| also includes a parameter
\verb|previous_record|, which is recommended whenever the user would like to
stop the function and then resume it later. In such a case, the parameter
should be set to the \verb|cdist| value of the exis}
\end{comment}
%%%

% I've got a better idea: the function should automatically ask the user if
% he's sure, in the case that we find a file under the name under which we
% intend to write. If the user is sure, we take the cdist of that existing
% file and use it as the previous record!

% we multithreaded the Python script to spawn 12 workers on an 11th gen Intel 
% i7-11700 @ 2.50 GHz. IF YOU ARE USING THE WORK DESKTOP

As of 2 October 2023, CL relies on this brute-force approach to build the
LHSs that are eventually used to train the emulators. To understand the 
computational and wall-time costs associated with this solution, we include 
plots~\ref{fig: function_calls} and~\ref{fig: wall_time}.

\begin{comment} % The following paragraph says this stuff better
we left the system to run for three consecutive days. In this time, the 
largest minimum separation that we generated was approximately 0.08022.  
Recall from section sec_B1 that the theoretical best possible value for this 
setup is approximately 0.24183. It would have been more meaningful if you had 
counted the total number of function calls, but it isn’t too late to set up 
such a run. So, even after assigning a relatively large amount of compute to 
this brute force solution, we fail to obtain an LHC of even a third of the 
best minimum separation.
\end{comment} 

Remember from section~\ref{sec: lhc_theory} equation~\ref{eq: best_lhs_sep},
which tells us the best possible $s^*$
given dimension $d$ and total number of samples $N_s$. Let us consider a 
massless-neutrino emulator (a total of four dimensions per cosmology vector)
trained over five-thousand CAMB spectra. If we plug in $d = 4$ and $N=5000$,
we find $s^*_\text{best} \approx 0.1189$. Clearly, the brute force method
achieves only a comparatively low $s^*$, even over the span of multiple days.

Based on figures~\ref{fig: function_calls} and~\ref{fig: wall_time}, we
claim that our approach is inefficient and not well-suited to the approach of 
maximizing $s^*$. Since suboptimal $s^*$ values imply unevenness in the
coverage of the space of cosmologies, we expect low $s^*$ values to impact
the emulator by increasing variance in the errors, because the worst errors
will be significantly worse, while the best errors will be slightly better in
oversampled regions. We also expect the average error to increase slightly,
as the oversampled regions should benefit less than the undersampled regions
suffer.

% One could argue that the variance in oversampled regions will decrease to
% compensate, right? I need to clarify that the decrease in variance in the
% oversampled regions would not be able to compensate the increased variance
% in the undersampled regions, because the space of power spectra is
% continuous, so closely spaced points reveal less about the true function
% than well spaced points.

We will test these expectations in section~\ref{sec: error_from_lhc} by
varying the $s^*$ of the LHS with which we train the emulator.
\textcolor{orange}{We will explore the question of superior methods in
section~\ref{sec: future_work}}.

\section{Rescaling the LHS}
\label{sec: lhc_rescale}

%s Introduce the build_cosmology script

Now that we have a unit LHS $\matr{X}_u$, we need to scale it 
so that each axis, which currently runs from zero to one, runs along a range 
dictated by one of the priors. This scaling is handled by the \texttt{ged}
function \verb|build_cosmology|. It accepts as parameters the values of
$\omega_b$, $\omega_c$, $n_s$, $\sigma_{12}$, $A_s$, $\omega_\nu$, and, 
optionally, a dictionary of priors. If this
dictionary is not provided, the cosmological parameters are assumed to
already have been scaled. When building a massless-neutrino emulator,
the information $\omega_\nu = 0$ and $A_s = A_s(\text{Aletheia model 0})$ is
automatically provided to \verb|build_cosmology| by
\verb|fill_hypercube|. 
% We really should redo build_cosmology so that it assumes indices match to
% the same parameters every time! This is how the rest of the code works,
% after all.

The precise form of the scaling is given by the formula for transforming a
random variable $x \sim \mathcal{U}(0, 1)$ to a random variable
$x' \sim \mathcal{U}(a, b)$:

\begin{equation}
\label{eq: scaling}
x' = x (b - a) + a
\end{equation}

After scaling the parameters, \verb|build_cosmology| finishes bridging the gap
between the LHS and the CAMB \verb|pars| object (as introduced in
chapter~\ref{chap: CAMB_setup}) by using default values to fill in the
remaining values demanded by CAMB. For example, $H_0$, $w_a$ and $w_0$ are
not parameters over
which we train the emulator, but CAMB requires that they be specified before
a power spectrum can be calculated. In all such cases, we use Aletheia model
0 as default values. \textcolor{orange}{I'm not referring to table 1.2 here
because I would have to expand it with parameters not essential to this work,
so I would have to redo the captions for fig 1.1 and table 1.2...}

So long as the power spectrum's value for $\sigma_{12}$ agrees with the
prior-scaled value from the LHS, it should not matter that 
we use the model 0 values, as these are evolution parameters.
\textcolor{red}{Or should it? If this logic really held, shouldn't we be able 
to modify $w_a$ and $w_0$ in order to get the correct value of
$\tilde{\sigma}_{12}$? But these are evolution parameters, so why don't we?}

\section{Integrating Evolution Mapping}
\label{sec: generate_emu_data}

% generate\_emu\_data.py

%s Now talk about fill_hypercube, a central function of this script

\verb|fill_hypercube| is the central function of \texttt{ged}. It iterates 
through the rows $\bm{x}$ of the unit LHS
$\matr{X}_u$, packages them into a fully-specified cosmology using
\verb|build_cosmology|, and passes the cosmology to one of the 
evaluation functions. \texttt{ged} provides two evaluation functions, which 
combine the CAMB calls from \texttt{ci} with the
principles of evolution mapping: \verb|direct_eval_cell| relies on
\verb|evaluate_cosmology| while \verb|interpolate_cell| relies on
\verb|cosmology_to_Pk_interpolator|. These two options are designed to give
the same results and will differ at a level insignificant to the conclusions
of this work. The emulator pipeline uses the direct evaluation approach by
default.

%s What exactly does it mean here to integrate evolution mapping into the 
%s code?

Recall from chapter~\ref{chap: CAMB_setup} that CAMB does not accept
$\sigma_{12}$ as an input. The importance of \verb|direct_eval_cell| and 
\verb|interpolate_cell| is in circumventing this problem.
After \verb|build_cosmology| returns a complete cosmology dictionary, these
functions computer the MEMNeC. Then, they request CAMB power spectra for the
MEMNeCs at 150\footnote{150 is the maximum number of redshifts that CAMB will
accept in a single call.} linearly-spaced redshifts in the
interval [0, 10]. \textcolor{orange}{log spacing would have been better for
redshift}. This interval suffices to capture the reddest galaxies
in our galaxy redshift surveys. \textcolor{green}{citation}. Next, these
functions request a one-dimensional interpolator from \texttt{scipy} with
the $\tilde{\sigma}_{12}$ values as the $x$ and the $z$ values as the $y$.
By passing the desired $\tilde{\sigma}_{12}$ value to the interpolator, we
can in principle find the redshift at which we need to call CAMB.

%s Bonus section that doesn't really fit anywhere specific: speed-up

Since we are estimating this redshift from an interpolation over 150 power
spectrum samples, the $z_\text{interp}$ returned by our interpolator does not
\textit{exactly} match the theoretical $z_\text{exact}$ at which the
power spectrum would exactly match the desired input value,
$\tilde{\sigma}_{12}(z_\text{exact})$. In order to speed up \texttt{ged},
which is by far the most time-intensive step in the emulator pipeline,
\verb|direct_eval_cell| and \verb|interpolate_cell| stop at one iteration of 
interpolation and return $\tilde{\sigma}_{12}(z_\text{interp})$. Then,
\verb|fill_hypercube| mutates the $\tilde{\sigma}_{12}$ column of the original 
LHS by using the reverse transformation of~\ref{eq: scaling}:

\begin{equation}
x = \frac{x' - a}{b - a}
\end{equation}

to obtain the unit counterpart to $\tilde{\sigma}_{12}(z_\text{interp})$.
This transformed value replaces the original value found in the LHS.
Once \verb|fill_hypercube| finishes computing the spectra, the user
should save the LHS to a new file (marked ``final'' in
figure~\ref{fig: flow_chart}); this LHS is used in the emulator's training.

This shortcut saves a good deal of time but moves the value of
$\tilde{\sigma}_{12}$ negligibly
\textcolor{green}{cite some numbers for this claim!}.
The only downside of moving $\tilde{\sigma}_{12}$ could be that it
decreases the $s^*$ of the LHS. Since the value shifts only weakly, we do not
consider it to produce a relevant decrease in the accuracy of the emulator.

\textcolor{orange}{We should make error plots (comparing the two approaches 
with hyper cube as a control), but I strongly suspect the error will be 
negligible.}

In theory, the above procedure suffices to match any $\tilde{\sigma}_{12}$
value. However, for some cosmologies, $z_\text{exact} < 0$, a case not
currently supported by CAMB. Since all power spectra grow in amplitude with
time, we can quickly establish whether the cosmology is ``solvable'' by
verifying that

\begin{equation}
\label{eq: solvability_cond}
\tilde{\sigma}_{12}(z = 0) \geq \tilde{\sigma}_{12}(z_\text{exact})
.\end{equation}

When this condition is not fulfilled, we can modify the value of $h$ to
compensate. Recall from chapter~\ref{chap: A_s} that changing $h$ while
holding $\omega_b$, $\omega_c$, and $\omega_K$ fixed amounts to varying
$\omega_\text{DE}$, an evolution parameter. Since $h$ is an evolution
parameter in this context, we are free to modify it in order to increase the
range of solvable cosmologies. Although CAMB nominally accepts $h$ values as
low as 0.01, in practice we find that values below around
$h_\text{min} \approx 0.07$ sometimes lead to crashes.

We call this process of tweaking the $h$ and $z$ values `rescaling' to
succinctly distinguish it from the application of priors to scale a unit LHS.
In chapter~\ref{chap: emu_outline}, we noted that \texttt{ged} outputs
files containing rescale parameters. These files contain the $(h, z)$ pairs
at which we evaluated each cosmology in the input LHS. Since we train our
emulator over neither $h$ nor $z$, this information is inconsequential to the
construction and testing of the emulator. Instead, these files are useful for
independently verifying the accuracy of the pipeline.

Unfortunately, even when setting the dimensionless Hubble parameter to its
minimum safe value, some power spectra will still fail
condition~\ref{eq: solvability_cond}.
For the purposes of this treatment, we refer to such cosmologies as
``unsolvable.'' An emulator can still be trained even if only one of the input 
cosmologies is solvable. However, the occurrence of even one
unsolvable cosmology indicates that the actual range of parameters in the
training data is narrower than the nominal input priors. 

The problem of unsolvable cosmologies led us to create the
increasingly restrictive prior sets described in
section~\ref{sec: lhc_outline}.
Of the three provided pairs of priors files, only the COMET priors
(table~\ref{tab: COMET_priors}) are narrow enough to completely evade the 
issue of unsolvable cosmologies. For this reason, the COMET priors are the
defaults used by CL. We defer further discussion of unsolvable cells to
section~\ref{sec: prior_woes}.

%s Don't give up skeleton!


\section{Training the Emulator}
\label{sec: train_emu}

\textcolor{orange}{Don't forget to explain that the training X is actually the
unit LHS, not the rescaled LHS, because the unit LHS is already normalized
and therefore easier to handle in training.}

% train\_emu.py

Once \texttt{ged}'s \verb|fill_hypercube| completes, the user is ready to
begin training the emulator. We recommend that the user store all of the
relevant data files in a subdirectory of \verb|data_sets|, which is a
subdirectory of the \texttt{cassL} code directory. This organization
allows the user to load and repackage all of the essential data with the
\texttt{ui} function \verb|get_data_dict|, which returns the
``data dictionary'' represented in figure~\ref{fig: flow_chart}.
Once the user passes the data dictionary to the \texttt{ui} function
\verb|build_and_test_emulator|, the end of the emulator pipeline has been 
reached, and the remaining work is automatic.

First, \verb|build_and_test_emulator| cleans the input data by dropping
unsolved cosmologies from the training and testing sets. Next, to train a new 
emulator, \verb|build_and_test_emulator| instantiates the
\verb|Emulator_Trainer| class. We will refer to such objects 
as \textit{trainers}.

Each emulator is an instance of
the \texttt{Emulator} class with a \texttt{GPy} GPR object at its core. We 
will refer instances of the \texttt{Emulator} class as \textit{emulator 
objects}.

Besides
accessing this GPR object, emulator objects also convert $\bm{x}$ inputs and
$\bm{y}$ outputs in order to facilitate scientific access. Specifically, 
the GPR itself accepts $\bm{x}$ inputs and predicts $\bm{y}$ outputs 
in a normalized fashion which will not be easily understood unless first 
appropriately transformed.

We separate the \verb|Emulator_Trainer| class from the \verb|Emulator| class
to make saving and loading more efficient. When transferring an emulator to a
different computer, for example, it is not necessary to transfer all of the
data sets used.
% Although these data sets only turn out to be 2\% as large as the GPR object, 
% which is provided by GPy and which performs the predictions. Maybe the
% situation justifies my approach with the large-k data sets? I don't think
% so--I think the GPR object simply gets bigger to keep the proportion the
% same...
However, we still need the \verb|Emulator| class to hold more than just the 
GPR, for example the normalization parameters.



%s Now talk about normalization

\textcolor{orange}{Needs segue.}
The normalizations are necessary for the high performance of the emulators. 
This need goes beyond numerical instabilities. For example, if a parameter's 
prior range falls within the relatively reasonable [0.0001, 0.0006] interval, 
its emulation will still be improved by normalizing it to a [0, 1] interval. 
Nonetheless, the improvement will be more dramatic the further away a prior is 
from this [0, 1] interval. The prior in $A_s$, regardless of the choice
between the three default priors files, covers extremely small values. Consequently, 
without the appropriate normalization, the trained GPR will be nearly 
insensitive to the impact of $A_s$ and \textcolor{green}{almost behave like a 
massless-neutrino emulator}; the lack of neutrino dependence would dominate 
the error in the resultant emulator.
\textcolor{orange}{Great start! But can we add some material in
section~\ref{sec: gpr_intro}: why is GPR vulnerable in this way?}
 
\textcolor{blue}{This section will focus on the particular lines of}
\verb|GPy| \textcolor{blue}{that we used, as well
as the various data-cleaning and normalization statements that we used.
Normalization will be a really important topic. I want to explain some of
the theory behind why the emulator performs poorly with values outside of this
[0, 1] range.}

I don't know how to justify this kernel that I got from Alex via AndreaP...
\textcolor{orange}{That's no problem; Ariel says, just say ``we played around
and found that this works best.''}

\textcolor{blue}{Briefly explain what GPy is doing--don’t treat it like some black box.}


\section{Testing the Emulator}
\label{test_emu}

The majority of the quantification and visualization of the performance of the
emulator are left to the user. However, a couple of functions are provided for 
the sake of simple ``eyeball'' evaluation of the emulator's accuracy.
The \verb|test| function in the \verb|Emulator_Trainer| class (in the
\verb|train_emu| script) takes a set of $\matr{X}$ and $\matr{Y}$ test data 
and computes errors automatically. Specifically, it stores three sets of error 
metrics as class attributes for the trainer object: \verb|deltas| (the simple 
difference between the predicted spectra $\hat{\bm{y}}$ and the test spectra
$\bm{y}$, \verb|rel_errors| (the ratio of \verb|deltas| to the $\bm{y}$), and
\verb|sq_errors| (the squares of \verb|deltas|).

It follows from the definition of these error metrics that all of them have 
the same shape: each is a set of arrays equal in number to the $bm{y}$; and 
the size of each element array is determined by the number of scales at which 
the training power spectra were evaluated. \textcolor{orange}{In the current
version of the code, this number of scales must be equal in the training and
testing sets. But, if we add another layer of interpolation, we could easily
eliminate this limitation}.

The \verb|error_curves| function graphs the performance on the given test set 
using a collection of overplotted error curves, where each curve represents a 
single cosmology. By default, the $y$-axis is relative error and the
$x$-axis is the inverse scale, $k$. To break down the performance based on 
individual cosmological parameters, there is a \verb|param_index| function
parameter which will automatically color each curve according to its value in 
the chosen cosmological parameter. Since colors can sometimes be difficult to 
quickly compare, there is also a \verb|fixed_k| function parameter. When a 
fixed value for the inverse scale is provided, the plot becomes a scatter 
plot, with percent error as the $y$-axis and the chosen cosmological parameter 
as the $x$-axis.

%! Did I make sure to refer to priors correctly when transcribing ^ this 
%! function into train_emu.py? I think so… but it wouldn’t hurt to look one 
%! more time, especially since the X values are now automatically unit!

The \verb|error_statistics| function issues a rudimentary summary of the 
application of some NumPy aggregator\footnote{The user may also pass in
non-NumPy aggregators as long as they can be called in the exact same
way--the \verb|error_statistics| and \verb|error_hist| functions always
perform the call \verb|error_aggregator(errors, axis=1)|.}
\verb|error_aggregator|, which condenses each error array to a single point,
to one of the various error metrics described earlier. The 
most useful aggregators will be \verb|mean|, \verb|median|, and \verb|std|, 
but interesting information can also be gathered from, for example, the
\verb|min|, \verb|max|, and \verb|ptp| aggregators.
\verb|error_statistics| simply prints various statistics associated with the 
aggregation, such as the median of the aggregates.

The \verb|error_hist| function is essentially a visualization of the 
information printed out by the \verb|error_statistics| function: an aggregator
function is applied to the selected error array, then the aggregates are 
binned and plotted as a histogram. The user can specify the number of bins,
otherwise Sturges' rule is used.


\section{Accessing and Using the Emulator}

% user\_interface.py

\textcolor{orange}{Maybe we should rename this section, we're already doing
heavy access in the previous sections.}

\textcolor{blue}{The content of this section is still relatively uncertain. 
Since I am still tweaking some of the emulator's settings, I haven't spent too 
much time on a script dedicated purely to simplifying the interaction between 
the user and the emulator \textit{object} itself. Anyway, the hope is to 
provide some clear and simple 
descriptions of what functions the user should turn to in order to get started 
predicting power spectra using the results of this thesis.}

%s Now talk about the use of prior files

In section~\ref{sec: lhc_outline}, we introduced different priors 
included with CL, as well as their file names. These file names come into play 
when calling the function \verb|prior_file_to_dict|, which reads a prior file
into a Python dictionary. The reader is encouraged to add additional prior
files based on the format of the files provided in the \verb|priors|
subdirectory of the CL code.

%which is a helper function
% to \verb|get_data_dict|, which in turn is a helper function to the 
%\textcolor{orange}{unfinished} build_train_and_test_sets, which is a helper
% function to... ENOUGH!
