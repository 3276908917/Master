\chapter{Results and Analysis}
\label{chap: default_emu}

In this chapter, we will discuss the performance of a two-emulator setup
built using the default configuration of CL: both the massless- and
massive-neutrino emulators appraised here use the COMET priors
(table~\ref{tab: COMET_priors}), $N_k = 300$, and $N_s = 3000$ for both the
training and testing data sets.

The final training LHS had an $s^*$ of 0.07972 in the massive case and 
0.02274 in the massless case. The final testing LHS had an $s^*$ of 0.07275 in 
the massive case and 0.02015 in the massless case. All numbers have been 
rounded to four significant figures.

\section{Quantifying the Performance of the Emulator}

To evaluate the performance of the emulator, we use the testing pipeline
explained in chapters~\ref{chap: emu_outline} and~\ref{chap: implementation}.
That is, we create an LHS of test cosmologies, which the training stages have
never seen, and compare the output of the emulator object to that of
CAMB. As in neural network contexts, GPR performance is typically evaluated
using test sets (\citealp{Mancini}, \citealp{Arico}, and
\citealp{Eggemeier}).\footnote{However, these test sets are
more commonly referred to as ``validation sets.'' We use the phrase
``test set'' to call to mind the three conventional data sets in a machine
learning setup: training, validation, and testing. This distinction will
prove useful in section~\ref{sec: future_work}.}

We appreciate that appraisal based on test sets
may not provide hard boundaries on the true
range of errors associated with the emulators,
because the continuity of the parameter hypervolume means that an infinite
number of cosmologies could be tested. Therefore, traditional
goodness-of-fit tests such as $\chi^2$ do not apply here.
However, precisely because
power spectra vary smoothly in this space\footnote{Indeed, without this
property, interpolation would be unproductive.}, we expect the error 
curves to vary similarly. So long as the test LHS represents a reasonable
coverage of the parameter space, we expect the errors calculated therefrom to
be similarly representative of emulator performance. On the other
hand, at the edges of the parameter space, where interpolation begins to
break down, the errors will be at their highest, and the following analyses
may not be representative of these edge cases.

For this chapter as well as chapter~\ref{chap: disc_and_conc}, we will focus
on just two error metrics: percent error and squared error. We include percent
error as it is more common in the literature and because it is immediately
interpretable (\citealp{Mancini}, \citealp{Arico}, and
\citealp{Eggemeier}). By contrast, the squared errors are difficult to
understand unless compared across multiple similar cases.
Nevertheless, we argue
that squared errors represent a more useful metric, at least within a single
paper, because they are unbiased with respect to the magnitude of the emulated
quantity. Consider that $P(k)$ is smallest at the largest $k$; if an emulator
mispredicts $P(k)$ with a constant offset, then the percent error curves will
be largest at the smallest $k$. As a further example, consider that the
overall amplitude of $P(k)$ is smaller for smaller $\tilde{\sigma}_{12}$;
if we again imagine a constant offset, $\tilde{\sigma}_{12}$ will appear as a
problematic parameter if we look only at percent error.


\section{Percent and Square Errors on Random Cosmologies}

* Percent error curves massless

* Squared error curves massless

* Percent error curves massive

* Squared error curves massive

This will be a fairly short section, basically just showing the plot of 5000 
error curves in these two ways.

\textcolor{orange}{Create some plots focusing on the BAO error spikes? i.e.
zoom-in on k-ranges.}

\textcolor{green}{redo the numbers in this next paragraph}

The current state of the massless-neutrino emulator features sub-0.1\% error 
for the vast majority of scales, with the exception of some wild fluctuations  
between k=0.09 and 0.2 / Mpc. Outside of this range, the errors tend to 
sub-0.01\% in the large-scale regime and sub-0.005\% in the small-scale 
regime. Please refer to figures XXX through XXX for illustrations thereof.

As we can see from figures~\ref{fig: massless_default} 
and~\ref{fig: massive_default}, 
both emulators are most accurate at very large and
very small $k$, with significant difficulties around the BAO region. We
consider the small-scale performance a powerful confirmation of our
techniques, as the impact of massive neutrinos is most significant at small
scales.

We highlight that the massive emulator performs significantly worse than its 
massless counterpart. The emulators are difficult to compare because they
were constructed from LHSs of different dimension: four in the massless case,
six in the massive case. Since $N_s = 5000$ is constant, we expect the 
massless LHS to sample the parameter space much more densely than the massive
LHS. In any case, the discrepancy in accuracies will be worsened by the
approximate nature of our fit from section~\ref{sec: proposed_fit}:
the results of section~\ref{sec: fit_testing} indicate
that evolution parameters besides $A_s$ are necessary for complete
characterization of the impact of massive neutrinos. Nevertheless, the
overall percent error of the massive emulator is \textcolor{green}{nearly}
at the level of CAMB itself, so we consider the massless emulator an
encouraging success.

As evolution-mapping emulation for \textit{massive}-neutrino cosmologies
is the novel feature of this work, we will henceforth concentrate exclusively
on the massive-neutrino emulator.

\section{Performance in Different Parameters}
\label{sec: param_breakdown}

In this section, we will break down the performance of the emulator
by coloring the same error plots as before (figure~\ref{fig: massive_default})
according to different parameters. We use these color plots to argue that our
emulator successfully captures the impact of all of the different parameters
over which it was trained.

\textcolor{orange}{I plan to spend some time talking 
about \textit{why} parameter x is the current biggest problem for the 
emulator.}

\textcolor{orange}{It would have been nice if you had done like Andrea said,
and produced a plot colored by the ``extremeness'' of the parameters. We
could implement an extremeness index for parameter $x$ simply by taking
subtracting 0.5 and taking the absolute value, assuming that $x$ comes from
the unit LHS. Then, we could take the average over six parameters to get
an extremeness index for the cosmology as a whole. Optionally, for plot
optimization, you could multiply the final value by 2 so that the extremeness
runs from 0 to 1.}


\section{Improvement from Two-emulator Solution}
\label{sec: 2emu_improvement}

We conclude this chapter with a brief demonstration of the advantage of our
two-emulator solution. We will task the massive-neutrino emulator with
predictions over the test set of the massless-neutrino emulator. We consider
this a fair test because we sampled $\omega_\nu$ values for the
massive-neutrino emulator from a uniform distribution [0, 0.01]. Therefore,
in principle, the $\omega_\nu = 0$ case should only be a slight extrapolation.

To understand these results, we remind the reader the overall error of the
massive emulator is larger than that of the massless emulator. As mentioned
in 

To justify our decision and to quantify the improvement from this approach, we
have prepared \textcolor{orange}{some} plots in
section~\ref{sec: 2emu_improvement}.

\textcolor{blue}{This will be an extremely short section with some error
plots of the massive-neutrino cosmology evaluating massless-neutrino
cosmologies.}


\section{Tightness of the Priors Used}
\label{sec: prior_woes}

With this section I would like to revisit the specific values for the priors,
that I only briefly mentioned back in section~\ref{sec: build_lhc}.

First of all, from a purely practical consideration, expanding the priors was
not feasible due to the high incidence of unsolvable cells.

But second, this may not be a significant limitation to the utility of the
emulators introduced here, because they are already quite wide compared to
current state-of-the-art parameter inferences. \textcolor{green}{CITATIONS}.

As of 19.06.23, ``COMET'' is the default for the emulator. It is the most 
restrictive of the three options and was implemented in order to totally 
eliminate the problem of unsolvable cells, allowing us to train our 
demonstration emulator over an LHS without any significant gaps.

Our hope was that the narrow parameter ranges would furthermore help the 
demonstration emulator to achieve high accuracy--in principal, success here
means that we can simply ``scale up'' the approach of this work by
simultaneously expanding the priors as well as the total number of training
samples. Unfortunately, it is not clear if we can scale up the emulator past
the point at which unsolvable cells begin to appear. Since Latin hypercube
sampling is designed to evenly sample a space, unsolvable cells certainly
indicate that parts of the parameter space lack representation in the training
data. In these regions, our emulator will be forced to interpolate across
large gaps, or worse, extrapolate (if the unsolvable cells occur at the edges
of the parameter space \textcolor{orange}{This is something that I should have
shown... i.e. with plots}).
% Andrea recommends a plot coloring points by the "extremeness" in our sample
% space.

We predict that increasing the total number of cells will only marginally
reduce the issue of unsolvable cells \textcolor{orange}{This is something that 
I should have shown... i.e. with plots}). We can imagine the subspace of
solvable points as some hypervolume within a hypercube determined by our
priors, and the emulator's training coverage as an approximation of this 
hypervolume with small hypercubes whose size is determined by the separation 
between points in the sample. In the ideal case, the space of solvable points 
is the same as the Latin hypercube. When this is not so, we can at least
reduce the error associated with our approximation of the space of solvable
points by shrinking the hypercubes we use in our approximation (i.e. by
increasing the total number of points in our sample). \textcolor{orange}{
To give a sense of the marginal nature of this error reduction, we can
consider how small our hypercubes already are. For simplicity, let's examine
just one axis of the hypercube. With 5000 samples in the ``MEGA'' priors,
the length of the training coverage MOST STRONGLY DETERMINED BY ONE HYPERCUBE
IS: UNFINISHED THOUGHT}

It seems reasonable to think that unsolvable cells indicate extreme regions of 
the parameter space, rather than isolated holes. Therefore, it would be 
misleading to claim that the final ``MEGA'' emulator corresponds to, for 
example, any prior ranges in table 00A; in truth, the emulator would 
correspond to a potentially (this is a dangerous word and opens you up to hard 
questions) complicated shape inscribed within the six-dimensional rectangular 
hyperprism.

CAMB does not allow for negative redshifts, although it prove be 
interesting to try to resolve this issue with a code that does allow negative 
redshifts, such as CLASS. \textcolor{orange}{Re-iterate the discussion from
section~\ref{sec: boltzmann_intro}: why didn’t we use CLASS for this project?}

\section{Minimum Separation of the Training LHC}
\label{sec: error_from_lhc}

What is the impact of the minimum separation? Surely the minimum separation
should be a proxy for the evenness of the coverage of the space of
cosmologies. Therefore, we expect the error variance to increase much more
dramatically than, say, the average bias.

How would we
be able to quantify the error due to this? We could try to compare the
emulator performance trained on hyper cubes of various minimum distances.

\section{Resolution of the k Axis}

This might go better in the CassL section, but I think I ought to motivate the 
decision to use length-300 arrays.

This is a special case where we would argue that percent error is more useful 
than squared error.
That is because percent error can be considered on its own, whereas squared
error relies on comparison for its meaning to emerge. But in this case, the
comparisons are obscure. Consider, for example, comparing an $N_k = 100$
emulator with an $N_k = 500$ emulator: the $N_k = 100$ emulator will have
fewer points from which to compute errors. Simple interpolation is not enough
to restore direct comparability, as the $N_k = 500$ emulator by definition
will suffer less from interpolation than the $N_k = 100$ emulator, for which
each point will have to inform five times as much as the interpolated function
domain.

\section{Number of Training Samples}
\label{sec: num_samples}

\textcolor{blue}{Justify choice of 5000 samples for each: maybe we can make a
trend plot showing diminishing returns in test error?}

% This might go better in the CassL section, but I think I ought to motivate 
% the decision to use 5000 training arrays.

\textcolor{orange}{I'll have to concede that the results of this section are 
not entirely comprehensive; we didn't train any emulators over the 
uncertainties of analogous validation hypercubes. All comparisons here use the 
simpler pipeline of just two data sets, training and testing.}

\section{Linear Sampling in Different Parameters}

We also tried sampling in $\sigma_{12}^2$ as well as $\sqrt{\sigma_{12}}$.
Unfortunately, we were unable to conclude anything about the effectiveness of
these strategies--there appears to have been some mistake in our code, such
that the errors are much larger than can be explained on account of poor
sampling.

See figures~\ref{fig: sigsquare_sample} and~\ref{fig: sigroot_sample} for
illustrations of the problem. In a future work, it would be helpful to
investigate these problems further. We may find that a different sampling
strategy will more efficiently reduce the deltas that we see in our emulator.