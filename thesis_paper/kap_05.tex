\chapter{Implementation Details}

\textcolor{blue}{This will be a rather long, dry, and technical section with 
subsections based on each of the core scripts making up the Python package \
that I have been developing. It will in some ways paraphrase and summarize the 
documentation, explaining the basic use of the package as well as important 
limitations. Of course, fairly early on in this section, there will be a 
footnote linking to my GitHub repository, which will be made public once I'm 
ready to hand in this thesis.}

\textcolor{blue}{Even if my code doesn't end up in a bigger repository, I 
nevertheless think that it's important to the scientific process that I 
describe in detail the code that I have written. Besides, if any readers want 
to experiment specifically with the ideas discussed in this thesis, they may 
find my code more accessible because it is, in a sense, ``single-purpose''--that 
is to say, written almost exclusively to investigate the topics of this 
paper.}

\textcolor{orange}{It only takes a couple of sentences, but we also need to 
describe how to install the package.}

\section{Building the Latin Hypercube}
\label{sec: build_lhc}

% lhc.py

\textcolor{blue}{This section will briefly recapitulate some of the Latin hypercube sampling
explanation from the introduction. Then we will talk about how this sample is
procured in practice, and what our ``solution'' (inelegant though it may be)
is to obtaining a sample with a decent minimum separation between the points
(remember to briefly repeat why this is important).}

In this section, we will discuss the building of LHSs, which within the
Cassandra-linear is handled by the \verb|lhs.py| script.
By ``LHSs,'' we are here referring exclusively to the
sample of cosmologies for which we want to compute power spectra.
However, as mentioned in section~\ref{sec: lhc_flow_chart}, at this stage we 
are not yet committing to a particular set of priors. Instead, each axis is 
sampled from the uniform distribution $\mathcal{U} (0, 1)$.

% We'll also describe the switch from a rescaled Latin hypercube to a unit
% Latin hypercube that we later interpret differently according to the
% specified priors.

Each entry $\bm{x}$ in our LHS $\matr{X}$ will be either a four- or six-dimensional vector, 
depending on the emulator's support for massive neutrinos. When building a
massless-neutrino emulator, a single LHS entry will describe the $\omega_b$,
$\omega_\text{CDM}$, $n_s$, and $\sigma_{12}$ values for that cosmology in 
this order. When building a massive-neutrino emulator, a single LHS entry will 
additionally describe the $A_s$ and $\omega_\nu$ values for that cosmology, 
again in this order.

To get started generating LHSs, we integrated demonstration code written by 
Daniel Farrow \text{red}{(and who else?) add these people to the Danksagung} 
during the CAKE 2021 workshop. We begin by invoking the \verb|lhs| function of
\verb|pyDOE2| a number of times. Each time we get a random new unit LHC, whose
spacing we analyze with the \verb|cdist| function from
\verb|scipy.spatial.distance|.  Recall from section~\ref{sec: lhc_theory} that
our sample more evenly covers the sample space if the minimum separation 
between the sample points increases. \textcolor{green}{Uneven separations will
result in excess error depending on the location of the cosmology in parameter
space}. Therefore, from among the numerous calls of \verb|lhs| we select the
hypercube sample with the largest minimum separation.

In order to maximize the number of random hypercubes generated per unit of
wall time, we have written a multithreaded function called
\verb|multithread_unit_LHC_builder| \textcolor{orange}{Keep your eye on this
one, I'm thinking about condensing lhc.py to just a couple of functions.}
Through the function parameter \verb|num_workers| the user is free to assign
any number of CPU threads to the task, so in principle the computer can be
wholly dedicated to the optimization of the LHS. This function represents an
unending query for LHSs. The user will be notified via command
line printout whenever an LHS has been generated whose minimum separation is
higher than the previous record value. Whenever such a superior LHS is
encountered, the function writes the LHS to a file and continues. Therefore,
the user may run this script in the background and terminate whenever. The
function always writes LHSs to the same file, so each new record-setting LHS
overwrites the old one. When the user terminates the function, whatever output
file remains represents the best LHS seen since the function was first called.

\begin{comment}
The \verb|cdist| function can be re-used to compare LHSs loaded from 
different
files. However, since there is generally little reason to keep old LHSs
(except, perhaps, to reconstruct specific emulators), it reduces clutter to
simply continue overwriting the same file. Therefore, the function
\verb|multithread_unit_LHC_builder| also includes a parameter
\verb|previous_record|, which is recommended whenever the user would like to
stop the function and then resume it later. In such a case, the parameter
should be set to the \verb|cdist| value of the exis}
\end{comment}

% I've got a better idea: the function should automatically ask the user if
% he's sure, in the case that we find a file under the name under which we
% intend to write. If the user is sure, we take the cdist of that existing
% file and use it as the previous record!

% we multithreaded the Python script to spawn 12 workers on an 11th gen Intel i7-11700 @ 2.50 GHz. IF YOU ARE USING THE WORK DESKTOP

As of the current (as of \textcolor{orange}{24.08.2023}) version of the
Cassandra-Linear code, this brute-force approach is the method used to build
the Latin hypercube samples eventually used to train the emulators. To 
understand the computational and wall-time costs associated with our
brute-force solution, we include plots XXY and XXY.

\begin{comment} % The following paragraph says this stuff better
we left the system to run for three consecutive days. In this time, the 
largest minimum separation that we generated was approximately 0.08022.  
Recall from section sec_B1 that the theoretical best possible value for this 
setup is approximately 0.24183. It would have been more meaningful if you had 
counted the total number of function calls, but it isnâ€™t too late to set up 
such a run. So, even after assigning a relatively large amount of compute to 
this brute force solution, we fail to obtain an LHC of even a third of the 
best minimum separation.
\end{comment} 

Remember from section~\ref{sec: lhc_theory} that we have an equation for the
best possible minimum separation that we can achieve with a Latin hypercube
of some given dimension and total number of samples. Let us consider a massless-neutrino emulator (a total of four dimensions per cosmology vector)
trained over five-thousand CAMB spectra. If we plug in $d = 4$ and $N=5000$,
we get XXY. Similarly, for our massive-neutrino emulator (a total of six 
dimensions per cosmology) trained over the same number of spectra, we find a 
theoretical best minimum-separation of XXY. 

It is based on figures XXY and XXY that we may claim that our approach is
inefficient and not well-suited to the approach of finding LHCs with large
minimum separations. As mentioned in section~\ref{sec: lhc_theory},
we value a high minimum separation because it means we are sampling the
space of cosmologies evenly rather than oversampling particular regions.
Therefore, we expect the consequence of a low minimum separation in the LHC to
be a higher error variance as well as a slightly higher average bias.

% One could argue that the variance in oversampled regions will decrease to
% compensate, right? I need to clarify that the decrease in variance in the
% oversampled regions would not be able to compensate the increased variance
% in the undersampled regions, because the space of power spectra is
% continuous, so closely spaced points reveal less about the true function
% than well spaced points.

Later, we will test our expectations by comparing emulators with different
minimum separations in their LHSs in section~\ref{sec: error_from_lhc}, to
which we defer remaining concerns about our approach to LHC generation.
Additionally, we will explore the question of superior methods in
section~\ref{sec: future_work}.

\section{Integrating Evolution Mapping}
\label{sec: generate_emu_data}

% generate\_emu\_data.py

\textcolor{blue}{This section will describe the process going from a Latin 
hypercube to a set of CAMB power spectra. We will describe how an array of six 
parameter values is fleshed out into a params object understood by CAMB. We 
will describe the procedure of modifying $h$ and $z$ until we arrive roughly 
at the $\sigma_{12}$ value that we desire, as well as the process of writing 
the \textit{actual} $\sigma_{12}$ value that we obtained back to the original 
hypercube. Why is this important? Because the Latin hypercube is used again 
later in the user\_interface.py script for the purpose of training the 
Gaussian emulator object.}

Now that we have a ``good enough'' unit LHC $\matr{X}$, we need to rescale it 
so that each axis, which currently runs from zero to one, runs along a range 
dictated by one of the cosmological parameters over which we are training the 
emulator. This rescaling is handled by the function \verb|build_cosmology| in
the \verb|generate_emu_data| script. It accepts as parameters the values of
$\omega_b$, $\omega_c$, $n_s$, $\sigma_{12}$, $A_s$, $\omega_nu$, and, 
optionally, a dictionary of parameter ranges. If the
parameter ranges are not provided, the cosmological parameters are assumed to
already have been rescaled. 
% We really should redo build_cosmology so that it assumes indices match to
% the same parameters every time! This is how the rest of the code works,
% after all.

\textcolor{orange}{And yet build cosmology is doing so much more than just
that! Talk about using Aletheia model 0 as the default for all other stuffs!}

\textcolor{orange}{What are the equations for rescaling that build cosmology
uses?}

%s Now talk about fill_hypercube, a central function of this script

\verb|fill_hypercube| iterates through the rows $\bm{x}$ of the unit LHC
$\matr{X}$, packages them into a fully-specified cosmology using the function
\verb|build_cosmology|, and passes them to one of the evaluation functions,
which combines the CAMB calls from the \verb|camb_interface| script with the
principles of evolution mapping. Currently, the default evaluation function is
\verb|direct_eval_cell|, which relies on the \verb|evaluate_cosmology|
function from the \verb|camb_interface| script.

 The code has gone through several crucial bug fixes since switching to a different set of priors, so we need to test this prior suite again and re-assess the rate of unsolvable cells. The ``classic'' priors represent a middle ground between those of ``COMET'' and ``MEGA''. We need to test this prior suite again to see if it still suffers from the large number of unsolvable cells which prompted us to switch to the set of priors called ``COMET.'' As of 19.06.23, ``COMET'' is the default for the emulator. It is the most restrictive of the three options and is intended to totally eliminate the problem of unsolvable cells, so that a complete LHC can be used to train a demonstration emulator. The hope is for the demonstration emulator to be extremely accurate but confined to a very narrow range in each parameter. That way, with a relatively limited sample size, we can still achieve encouraging levels of error. In principal, if the demonstration emulator performs well, we can ``scale up'' the approach by expanding the priors as well as the total number of training samples. However, it is not clear if we can scale up the emulator past the point at which unsolvable cells begin to appear. It seems reasonable to think that unsolvable cells indicate extreme regions of the parameter space, rather than isolated holes. Therefore, it would be misleading to claim that the final emulator corresponded to, for example, any prior ranges in table 00A; in truth, the emulator would correspond to a potentially (this is a dangerous word and opens you up to hard questions) complicated shape inscribed within the six-dimensional rectangular hyperprism.

%s What exactly does it mean here to integrate evolution mapping into the 
%s code?

We will now describe the process of rescaling a cosmology with the evolution 
parameters z and h in order to match the desired $\sigma_{12}$ value.

For the purposes of this treatment, we refer to any cell, for which this 
approach does not work, as an unsolvable cell. In practice, this means that 
even at the lowest h values supported by CAMB (\textcolor{orange}{what are 
these, and why is it unsafe to just use 0.01, even though that is sometimes 
allowed?}), we cannot match the desired sigma 12 value with a nonnegative 
redshift. CAMB does not allow for negative redshifts, although it prove be 
interesting to try to resolve this issue with a code that does allow negative 
redshifts, such as CLASS. \textcolor{orange}{Re-iterate the discussion from
section~\ref{sec: boltzmann_intro}: why didnâ€™t we use CLASS for this project?}

%s Bonus section that doesn't really fit anywhere specific: speed-up

Arielâ€™s idea for speeding up the program: use a best-guess z which only approximates the desired sigma12. Then, when we calculate the power spectrum using this redshift, get the actual sigma12 value from CAMB and replace the LHC entry with the true value.
Weâ€™ve implemented this now. We should definitely talk about how little of a difference that this makes. We should make error plots (comparing the two approaches with hyper cube as a control), but I strongly suspect the error will be negligible.


\section{Training the Emulator}
\label{sec: train_emu}

% train\_emu.py

This section will focus on the particular lines of GPy that we used, as well
as the various data-cleaning and normalization statements that we used.
Normalization will be a really important topic. I want to explain some of
the theory behind why the emulator performs poorly with values outside of this
[0, 1] range.

I don't know how to justify this kernel that I got from Alex via AndreaP...

\section{Accessing and Using the Emulator}

% user\_interface.py

The content of this section is still relatively uncertain. Since I am still tweaking some of the emulator's settings, I haven't spent too much time on a script dedicated purely to simplifying the interaction between the user and the emulator \textit{object} itself.

Anyway, the hope is to provide some clear and simple descriptions of what functions the user should turn to in order to get started predicting power spectra using the results of this thesis. The functions can either create a new emulator object and train it or load an existing one with easy-to-understand functions for interacting with the object.

Here, we will talk about the details of the training as well as the importance of normalization. Since this is still technical, it may not be appropriate for a UI script. So, I am considering breaking out a new script (and therefore a new section), maybe called ``train\_emu.py.''

% Now talk about the use of prior files

In section~\ref{sec: default_priors}, we introduced different priors 
included with CL, as well as their file names. These file names come into play 
when calling the function \verb|prior_file_to_dict|, which reads a prior file
into a Python dictionary. The reader is encouraged to add additional prior
files based on the format of the files provided in the \verb|priors|
subdirectory of the CL code.

%which is a helper function
% to \verb|get_data_dict|, which in turn is a helper function to the 
%\textcolor{orange}{unfinished} build_train_and_test_sets, which is a helper
% function to... ENOUGH!

\verb|build_train_and_test_sets|. in the
\verb|generate_emu_data| script.