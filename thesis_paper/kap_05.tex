\chapter{Implementation Details}
\label{chap: implementation}

\textcolor{blue}{This will be a rather long, dry, and technical section with 
subsections based on each of the core scripts making up the Python package \
that I have been developing. It will in some ways paraphrase and summarize the 
documentation, explaining the basic use of the package as well as important 
limitations. Of course, fairly early on in this section, there will be a 
footnote linking to my GitHub repository, which will be made public once I'm 
ready to hand in this thesis.}

\textcolor{blue}{Even if my code doesn't end up in a bigger repository, I 
nevertheless think that it's important to the scientific process that I 
describe in detail the code that I have written. Besides, if any readers want 
to experiment specifically with the ideas discussed in this thesis, they may 
find my code more accessible because it is, in a sense, ``single-purpose''--that 
is to say, written almost exclusively to investigate the topics of this 
paper.}

\textcolor{orange}{It only takes a couple of sentences, but we also need to 
describe how to install the package.}

\section{Building the Latin Hypercube}
\label{sec: build_lhc}

% lhc.py

In this section, we will discuss the building of LHSs, which within CL is 
handled by \texttt|lhs|. As mentioned in section~\ref{sec: lhc_outline},
we first create a unit LHS and then later rescale it according to a set of
priors.

Each entry $\bm{x}$ in our LHS $\matr{X}$ will be either a four- or
six-dimensional vector, 
depending on the emulator's support for massive neutrinos. When building a
massless-neutrino emulator, each $\bm{x}$ will describe the $\omega_b$,
$\omega_c$, $n_s$, and
$\tilde{\sigma}_{12}$ values for a different cosmology in 
this order. When building a massive-neutrino emulator, each $\bm{x}$ will 
additionally specify the $A_s$ and $\omega_\nu$ values for that cosmology, 
again in this order.

To get started generating LHSs, we integrated demonstration code written by 
Daniel Farrow \textcolor{orange}{(and who else?) add these people to the 
Danksagung}. We begin by invoking the \texttt{lhs} function of
\texttt{pyDOE2} a number of times. Each time we get a random new unit LHC, 
whose spacing we analyze with the \texttt{cdist} function from
\texttt{scipy.spatial.distance}.
Recall from section~\ref{sec: lhc_theory} that
our samples more evenly cover the sample space if the minimum separation
$s^*$ increases. Therefore, from among the numerous calls of \texttt{lhs} we 
select the LHS with the largest minimum separation.

In order to maximize the number of random hypercubes generated per unit of
wall time, we have written a multithreaded function called
\verb|multithread_unit_LHC_builder| \textcolor{orange}{Keep your eye on this
one, I'm thinking about condensing lhc.py to just a couple of functions.}
Through the function parameter \verb|num_workers| the user is free to assign
any number of CPU threads to the task; in principle, a CPU can be
wholly dedicated to the optimization of the LHS.
\verb|multithread_unit_LHC_builder| does not contain a return statement but
instead triggers an unending query for LHSs. The user is notified via command
line printout whenever an LHS has been generated whose $s^*$ exceeds
the previous record. Whenever such a superior LHS is
encountered, the function writes the LHS to a file and continues. Therefore,
the user may run this script in the background and terminate whenever. The
function always writes LHSs to the same file, so each new record-setting LHS
overwrites the old one. When the user terminates the function, whatever output
file remains represents the best LHS seen since the function was first called.

%%% It's just too much detail for a master's thesis, no one will care
\begin{comment}
The \verb|cdist| function can be re-used to compare LHSs loaded from 
different
files. However, since there is generally little reason to keep old LHSs
(except, perhaps, to reconstruct specific emulators), it reduces clutter to
simply continue overwriting the same file. Therefore, the function
\verb|multithread_unit_LHC_builder| also includes a parameter
\verb|previous_record|, which is recommended whenever the user would like to
stop the function and then resume it later. In such a case, the parameter
should be set to the \verb|cdist| value of the exis}
\end{comment}
%%%

% I've got a better idea: the function should automatically ask the user if
% he's sure, in the case that we find a file under the name under which we
% intend to write. If the user is sure, we take the cdist of that existing
% file and use it as the previous record!

% we multithreaded the Python script to spawn 12 workers on an 11th gen Intel 
% i7-11700 @ 2.50 GHz. IF YOU ARE USING THE WORK DESKTOP

As of 2 October 2023, CL relies on this brute-force approach to build the
LHSs that are eventually used to train the emulators. To understand the 
computational and wall-time costs associated with this solution, we include 
plots~\ref{fig: function_calls} and~\ref{fig: wall_time}.

\begin{comment} % The following paragraph says this stuff better
we left the system to run for three consecutive days. In this time, the 
largest minimum separation that we generated was approximately 0.08022.  
Recall from section sec_B1 that the theoretical best possible value for this 
setup is approximately 0.24183. It would have been more meaningful if you had 
counted the total number of function calls, but it isnâ€™t too late to set up 
such a run. So, even after assigning a relatively large amount of compute to 
this brute force solution, we fail to obtain an LHC of even a third of the 
best minimum separation.
\end{comment} 

Remember from section~\ref{sec: lhc_theory} equation~\ref{eq: best_lhs_sep},
which tells us the best possible $s^*$
given dimension $d$ and total number of samples $N_s$. Let us consider a 
massless-neutrino emulator (a total of four dimensions per cosmology vector)
trained over five-thousand CAMB spectra. If we plug in $d = 4$ and $N=5000$,
we find $s^*_\text{best} \approx 0.1189$. Clearly, the brute force method
achieves only a comparatively low $s^*$, even over the span of multiple days.

Based on figures~\ref{fig: function_calls} and~\ref{fig: wall_time}, we
claim that our approach is inefficient and not well-suited to the approach of 
maximizing $s^*$. Since suboptimal $s^*$ values imply unevenness in the
coverage of the space of cosmologies, we expect low $s^*$ values to impact
the emulator by increasing variance in the errors, because the worst errors
will be significantly worse, while the best errors will be slightly better in
oversampled regions. We also expect the average error to increase slightly,
as the oversampled regions should benefit less than the undersampled regions
suffer.

% One could argue that the variance in oversampled regions will decrease to
% compensate, right? I need to clarify that the decrease in variance in the
% oversampled regions would not be able to compensate the increased variance
% in the undersampled regions, because the space of power spectra is
% continuous, so closely spaced points reveal less about the true function
% than well spaced points.

We will test these expectations in section~\ref{sec: error_from_lhc} by
varying the $s^*$ of the LHS with which we train the emulator.
\textcolor{orange}{We will explore the question of superior methods in
section~\ref{sec: future_work}}.

\section{Rescaling the LHS}
\label{sec: lhc_rescale}

\textcolor{blue}{This section will describe the process going from a Latin 
hypercube to a set of CAMB power spectra. We will describe how an array of six 
parameter values is fleshed out into a params object understood by CAMB.}

\textcolor{blue}{What happens to $A_s$ in the massless-neutrino case? Is it 
fixed at the default for model 0? I'm pretty sure it is!}

%s Preface the script as a whole. Where do I put this?
\textcolor{gray}{\texttt{ged} includes more complicated functions that apply the 
framework established in \texttt{ci} to the filling in of the 
hypercube object that we obtained from \texttt{lhc} via
\texttt{ui}.}

%s Introduce the build_cosmology script

Now that we have a unit LHS $\matr{X}_u$, we need to scale it 
so that each axis, which currently runs from zero to one, runs along a range 
dictated by one of the priors. This scaling is handled by the \texttt{ged}
function \verb|build_cosmology|. It accepts as parameters the values of
$\omega_b$, $\omega_c$, $n_s$, $\sigma_{12}$, $A_s$, $\omega_\nu$, and, 
optionally, a dictionary of priors. If this
dictionary is not provided, the cosmological parameters are assumed to
already have been scaled. When building a massless-neutrino emulator,
the information $\omega_\nu = 0$ and $A_s = A_s(\text{Aletheia model 0})$ is
automatically provided to \verb|build_cosmology| by
\verb|fill_hypercube|. 
% We really should redo build_cosmology so that it assumes indices match to
% the same parameters every time! This is how the rest of the code works,
% after all.

The precise form of the scaling is given by the formula for transforming a
random variable $x \sim \mathcal{U}(0, 1)$ to a random variable
$x' \sim \mathcal{U}(a, b)$:

\begin{equation}
x' = x (b - a) + a
\end{equation}

After scaling the parameters, \verb|build_cosmology| finishes bridging the gap
between the LHS and the CAMB \verb|pars| object (as introduced in
chapter~\label{chap: CAMB_setup}) by using default values to fill in the
remaining values demanded by CAMB. For example, $H_0$, $w_a$ and $w_0$ are
not parameters over
which we train the emulator, but CAMB requires that they be specified before
a power spectrum can be calculated. In all such cases, we use Aletheia model
0 as default values. \textcolor{orange}{I'm not referring to table 1.2 here
because I would have to expand it with parameters not essential to this work,
so I would have to redo the captions for fig 1.1 and table 1.2...}

\section{Integrating Evolution Mapping}
\label{sec: generate_emu_data}

Another paragraph I want to have in this section: stress the part of the 
evolution mapping introduction, that the $\sigma_{12}$ value we're using to 
describe the model is actually the $\sigma_{12}$ value of the model's MEMNeC! 
This is so important and confusing that maybe I'll even recapitulate again 
later in the section on the \verb|generate_emu_data| script.

% generate\_emu\_data.py

\textcolor{blue}{We 
will describe the procedure of modifying $h$ and $z$ until we arrive roughly 
at the $\tilde{\sigma}_{12}$ value that we desire, as well as the process of 
writing the \textit{actual} $\tilde{\sigma}_{12}$ value that we obtained back 
to the original hypercube. Why is this important? Because the Latin hypercube 
is used again later in \texttt{ui} for the purpose of training the 
GPR object.}

In practice, we split these parameters into two categories: we modify the
values of $h$ and $z$ until we obtain the desired value of $\sigma_{12}$
(this will be explained later in this section), and the rest are parameters
that we set to set to ``default'' values according to Aletheia model 0.
So long as the power spectrum's value for $\sigma_{12}$ agrees with the
prior-scaled value from the LHS, \textcolor{green}{it should not matter that 
we use the model 0 values, as these are evolution parameters}.
\textit{red}{Or should it? If this logic really held, shouldn't we be able to 
modify $w_a$ and $w_0$ in order to get the correct value of $\sigma_{12}$? But
I'm almost certain these are evolution parameters! Also: $\omega_\text{DE}$
is only an evolution parameter when everything else (all other omegas) is 
fixed...}


%s Now talk about fill_hypercube, a central function of this script

\verb|fill_hypercube| is the central function of \texttt{ged}. It iterates 
through the rows $\bm{x}$ of the unit LHS
$\matr{X}_u$, packages them into a fully-specified cosmology using
\verb|build_cosmology|, and passes the cosmology to one of the 
evaluation functions. \texttt{ged} provides two evaluation functions, which 
combine the CAMB calls from \texttt{ci} with the
principles of evolution mapping: \verb|direct_eval_cell| relies on
\verb|evaluate_cosmology| while \verb|interpolate_cell| relies on
\verb|cosmology_to_Pk_interpolator|. These two options are designed to give
the same results and will differ at a level insignificant to the conclusions
of this work. The emulator pipeline uses the direct evaluation approach by
default.

%s What exactly does it mean here to integrate evolution mapping into the 
%s code?

Recall from chapter~\ref{chap: CAMB_setup} that CAMB does not accept
$\sigma_{12}$ as an input. The importance of \verb|direct_eval_cell| and 
\verb|interpolate_cell| is in circumventing this problem.
After \verb|build_cosmology| returns a complete cosmology dictionary, these
functions computer the MEMNeC. Then, they request CAMB power spectra for the
MEMNeCs at 150 linearly-spaced redshifts in the
interval [0, 10]. \textcolor{orange}{log spacing would have been better for
redshift}. This interval suffices to capture the reddest galaxies
in our galaxy redshift surveys. \textcolor{green}{citation}. Next, these
functions request a one-dimensional interpolator from \texttt{scipy} with
the $\tilde{\sigma}_{12}$ values as the $x$ and the $z$ values as the $y$.
By passing the desired $\tilde{\sigma}_{12}$ value to the interpolator, we
can in principle find the redshift at which we need to call CAMB.

In practice, some cosmologies cannot be analyzed in this way. CAMB does not
support negative redshifts. Since all power spectra grow in amplitude with
time, we can quickly establish whether the cosmology is ``solvable'' by
verifying that

\begin{equation}
\tilde{\sigma}_{12}(z = 0) \geq \tilde{\sigma}_{12}(z_\text{target})
.\end{equation}

power spectrum today ($z = 0$) should always yield the maximum 

We will now describe the process of rescaling a cosmology with the evolution 
parameters z and h in order to match the desired $\sigma_{12}$ value.

For the purposes of this treatment, we refer to any cell, for which this 
approach does not work, as an unsolvable cell. In practice, this means that 
even at the lowest h values supported by CAMB (\textcolor{orange}{what are 
these, and why is it unsafe to just use 0.01, even though that is sometimes 
allowed?}), we cannot match the desired sigma 12 value with a nonnegative 
redshift. CAMB does not allow for negative redshifts, although it prove be 
interesting to try to resolve this issue with a code that does allow negative 
redshifts, such as CLASS. \textcolor{orange}{Re-iterate the discussion from
section~\ref{sec: boltzmann_intro}: why didnâ€™t we use CLASS for this project?}

%s Don't give up skeleton!

The consequences of unsolvable cells will be discussed more in
section~\ref{sec: prior_woes}. Despite the discouraging situation described 
there,
unsolvable cells do not immediately sink the entire data set. The remaining
still be used to train an emulator as long as even one cell has been solved.
\textcolor{orange}{This will be explained in section~\ref{sec: train_emu}}.

%s Bonus section that doesn't really fit anywhere specific: speed-up

Arielâ€™s idea for speeding up the program: use a best-guess z which only approximates the desired sigma12. Then, when we calculate the power spectrum using this redshift, get the actual sigma12 value from CAMB and replace the LHC entry with the true value.
Weâ€™ve implemented this now. We should definitely talk about how little of a difference that this makes. We should make error plots (comparing the two approaches with hyper cube as a control), but I strongly suspect the error will be negligible.


\section{Training the Emulator}
\label{sec: train_emu}

\textcolor{orange}{Don't forget to explain that the training X is actually the
unit LHS, not the rescaled LHS, because the unit LHS is already normalized
and therefore easier to handle in training.}

% train\_emu.py

\textcolor{blue}{This section will focus on the particular lines of}
\verb|GPy| \textcolor{blue}{that we used, as well
as the various data-cleaning and normalization statements that we used.
Normalization will be a really important topic. I want to explain some of
the theory behind why the emulator performs poorly with values outside of this
[0, 1] range.}

I don't know how to justify this kernel that I got from Alex via AndreaP...
\textcolor{orange}{That's no problem; Ariel says, just say ``we played around
and found that this works best.''}

\textcolor{blue}{Briefly explain what GPy is doingâ€”donâ€™t treat it like some black box.}

This script contains the functions used to build an emulator from the 
generated data sets. To train a new emulator, one instantiates the
\verb|Emulator_Trainer| class. We will henceforth refer to objects therefrom 
as trainers.

We separate the \verb|Emulator_Trainer| class from the \verb|Emulator| class
to make saving and loading more efficient. When transferring an emulator to a
different computer, for example, it is not necessary to transfer all of the
data sets used.
% Although these data sets only turn out to be 2\% as large as the GPR object, 
% which is provided by GPy and which performs the predictions. Maybe the
% situation justifies my approach with the large-k data sets? I don't think
% so--I think the GPR object simply gets bigger to keep the proportion the
% same...
However, we still need the \verb|Emulator| class to hold more than just the 
GPR, for example the normalization parameters.

Each emulator is an instance of
the \verb|Emulator| class with a \verb|GPy| GPR object at its core. We will 
henceforth refer to such instances as emulator \textit{objects}. Besides the 
GPR object at the core, emulator objects also convert $\bm{x}$ inputs and
$\bm{y}$ outputs in order to facilitate scientific access. Specifically, 
the GPR itself accepts $\bm{x}$ inputs and predicts $\bm{y}$ outputs 
in a normalized fashion which will not be easily understood unless first 
appropriately transformed.

%s Now talk about normalization

\textcolor{orange}{Needs segue.}
The normalizations are necessary for the high performance of the emulators. 
This need goes beyond numerical instabilities. For example, if a parameter's 
prior range falls within the relatively reasonable [0.0001, 0.0006] interval, 
its emulation will still be improved by normalizing it to a [0, 1] interval. 
Nonetheless, the improvement will be more dramatic the further away a prior is 
from this [0, 1] interval. The prior in $A_s$, regardless of the choice
between the three default priors files, covers extremely small values. Consequently, 
without the appropriate normalization, the trained GPR will be nearly 
insensitive to the impact of $A_s$ and \textcolor{green}{almost behave like a 
massless-neutrino emulator}; the lack of neutrino dependence would dominate 
the error in the resultant emulator.
\textcolor{orange}{Great start! But can we add some material in
section~\ref{sec: gpr_intro}: why is GPR vulnerable in this way?}


\section{Testing the Emulator}
\label{test_emu}

The majority of the quantification and visualization of the performance of the
emulator are left to the user. However, a couple of functions are provided for 
the sake of simple ``eyeball'' evaluation of the emulator's accuracy.
The \verb|test| function in the \verb|Emulator_Trainer| class (in the
\verb|train_emu| script) takes a set of $\matr{X}$ and $\matr{Y}$ test data 
and computes errors automatically. Specifically, it stores three sets of error 
metrics as class attributes for the trainer object: \verb|deltas| (the simple 
difference between the predicted spectra $\hat{\bm{y}}$ and the test spectra
$\bm{y}$, \verb|rel_errors| (the ratio of \verb|deltas| to the $\bm{y}$), and
\verb|sq_errors| (the squares of \verb|deltas|).

It follows from the definition of these error metrics that all of them have 
the same shape: each is a set of arrays equal in number to the $bm{y}$; and 
the size of each element array is determined by the number of scales at which 
the training power spectra were evaluated. \textcolor{orange}{In the current
version of the code, this number of scales must be equal in the training and
testing sets. But, if we add another layer of interpolation, we could easily
eliminate this limitation}.

The \verb|error_curves| function graphs the performance on the given test set 
using a collection of overplotted error curves, where each curve represents a 
single cosmology. By default, the $y$-axis is relative error and the
$x$-axis is the inverse scale, $k$. To break down the performance based on 
individual cosmological parameters, there is a \verb|param_index| function
parameter which will automatically color each curve according to its value in 
the chosen cosmological parameter. Since colors can sometimes be difficult to 
quickly compare, there is also a \verb|fixed_k| function parameter. When a 
fixed value for the inverse scale is provided, the plot becomes a scatter 
plot, with percent error as the $y$-axis and the chosen cosmological parameter 
as the $x$-axis.

%! Did I make sure to refer to priors correctly when transcribing ^ this 
%! function into train_emu.py? I think soâ€¦ but it wouldnâ€™t hurt to look one 
%! more time, especially since the X values are now automatically unit!

The \verb|error_statistics| function issues a rudimentary summary of the 
application of some NumPy aggregator\footnote{The user may also pass in
non-NumPy aggregators as long as they can be called in the exact same
way--the \verb|error_statistics| and \verb|error_hist| functions always
perform the call \verb|error_aggregator(errors, axis=1)|.}
\verb|error_aggregator|, which condenses each error array to a single point,
to one of the various error metrics described earlier. The 
most useful aggregators will be \verb|mean|, \verb|median|, and \verb|std|, 
but interesting information can also be gathered from, for example, the
\verb|min|, \verb|max|, and \verb|ptp| aggregators.
\verb|error_statistics| simply prints various statistics associated with the 
aggregation, such as the median of the aggregates.

The \verb|error_hist| function is essentially a visualization of the 
information printed out by the \verb|error_statistics| function: an aggregator
function is applied to the selected error array, then the aggregates are 
binned and plotted as a histogram. The user can specify the number of bins,
otherwise Sturges' rule is used.


\section{Accessing and Using the Emulator}

% user\_interface.py

\textcolor{orange}{Maybe we should rename this section, we're already doing
heavy access in the previous sections.}

\textcolor{blue}{The content of this section is still relatively uncertain. 
Since I am still tweaking some of the emulator's settings, I haven't spent too 
much time on a script dedicated purely to simplifying the interaction between 
the user and the emulator \textit{object} itself. Anyway, the hope is to 
provide some clear and simple 
descriptions of what functions the user should turn to in order to get started 
predicting power spectra using the results of this thesis.}

%s Now talk about the use of prior files

In section~\ref{sec: lhc_outline}, we introduced different priors 
included with CL, as well as their file names. These file names come into play 
when calling the function \verb|prior_file_to_dict|, which reads a prior file
into a Python dictionary. The reader is encouraged to add additional prior
files based on the format of the files provided in the \verb|priors|
subdirectory of the CL code.

%which is a helper function
% to \verb|get_data_dict|, which in turn is a helper function to the 
%\textcolor{orange}{unfinished} build_train_and_test_sets, which is a helper
% function to... ENOUGH!
