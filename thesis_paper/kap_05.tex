\chapter{Implementation Details}
\label{chap: implementation}

\textcolor{blue}{This will be a rather long, dry, and technical section with 
subsections based on each of the core scripts making up the Python package \
that I have been developing. It will in some ways paraphrase and summarize the 
documentation, explaining the basic use of the package as well as important 
limitations. Of course, fairly early on in this section, there will be a 
footnote linking to my GitHub repository, which will be made public once I'm 
ready to hand in this thesis.}

\textcolor{blue}{Even if my code doesn't end up in a bigger repository, I 
nevertheless think that it's important to the scientific process that I 
describe in detail the code that I have written. Besides, if any readers want 
to experiment specifically with the ideas discussed in this thesis, they may 
find my code more accessible because it is, in a sense, ``single-purpose''--that 
is to say, written almost exclusively to investigate the topics of this 
paper.}

\textcolor{orange}{It only takes a couple of sentences, but we also need to 
describe how to install the package.}

\section{Building the Latin Hypercube}
\label{sec: build_lhc}

% lhc.py

\textcolor{blue}{This section will briefly recapitulate some of the Latin hypercube sampling
explanation from the introduction. Then we will talk about how this sample is
procured in practice, and what our ``solution'' (inelegant though it may be)
is to obtaining a sample with a decent minimum separation between the points
(remember to briefly repeat why this is important).}

The \verb|pyDOE2| function that we will use to generate our LHSs
returns a result with entries that range from zero to unity. 

In this section, we will discuss the building of LHSs, which within the
Cassandra-linear is handled by the \verb|lhs.py| script.
By ``LHSs,'' we are here referring exclusively to the
sample of cosmologies for which we want to compute power spectra.
However, as mentioned in section~\ref{sec: lhc_outline}, at this stage we 
are not yet committing to a particular set of priors. Instead, each axis is 
sampled from the uniform distribution $\mathcal{U} (0, 1)$.

% We'll also describe the switch from a rescaled Latin hypercube to a unit
% Latin hypercube that we later interpret differently according to the
% specified priors.

Each entry $\bm{x}$ in our LHS $\matr{X}$ will be either a four- or six-dimensional vector, 
depending on the emulator's support for massive neutrinos. When building a
massless-neutrino emulator, a single LHS entry will describe the $\omega_b$,
$\omega_\text{CDM}$, $n_s$, and $\sigma_{12}$ values for that cosmology in 
this order. When building a massive-neutrino emulator, a single LHS entry will 
additionally describe the $A_s$ and $\omega_\nu$ values for that cosmology, 
again in this order.

To get started generating LHSs, we integrated demonstration code written by 
Daniel Farrow \text{red}{(and who else?) add these people to the Danksagung} 
during the CAKE 2021 workshop. We begin by invoking the \verb|lhs| function of
\verb|pyDOE2| a number of times. Each time we get a random new unit LHC, whose
spacing we analyze with the \verb|cdist| function from
\verb|scipy.spatial.distance|.  Recall from section~\ref{sec: lhc_theory} that
our sample more evenly covers the sample space if the minimum separation 
between the sample points increases. \textcolor{green}{Uneven separations will
result in excess error depending on the location of the cosmology in parameter
space}. Therefore, from among the numerous calls of \verb|lhs| we select the
hypercube sample with the largest minimum separation.

In order to maximize the number of random hypercubes generated per unit of
wall time, we have written a multithreaded function called
\verb|multithread_unit_LHC_builder| \textcolor{orange}{Keep your eye on this
one, I'm thinking about condensing lhc.py to just a couple of functions.}
Through the function parameter \verb|num_workers| the user is free to assign
any number of CPU threads to the task, so in principle the computer can be
wholly dedicated to the optimization of the LHS. This function represents an
unending query for LHSs. The user will be notified via command
line printout whenever an LHS has been generated whose minimum separation is
higher than the previous record value. Whenever such a superior LHS is
encountered, the function writes the LHS to a file and continues. Therefore,
the user may run this script in the background and terminate whenever. The
function always writes LHSs to the same file, so each new record-setting LHS
overwrites the old one. When the user terminates the function, whatever output
file remains represents the best LHS seen since the function was first called.

\begin{comment}
The \verb|cdist| function can be re-used to compare LHSs loaded from 
different
files. However, since there is generally little reason to keep old LHSs
(except, perhaps, to reconstruct specific emulators), it reduces clutter to
simply continue overwriting the same file. Therefore, the function
\verb|multithread_unit_LHC_builder| also includes a parameter
\verb|previous_record|, which is recommended whenever the user would like to
stop the function and then resume it later. In such a case, the parameter
should be set to the \verb|cdist| value of the exis}
\end{comment}

% I've got a better idea: the function should automatically ask the user if
% he's sure, in the case that we find a file under the name under which we
% intend to write. If the user is sure, we take the cdist of that existing
% file and use it as the previous record!

% we multithreaded the Python script to spawn 12 workers on an 11th gen Intel i7-11700 @ 2.50 GHz. IF YOU ARE USING THE WORK DESKTOP

As of the current (as of \textcolor{orange}{24.08.2023}) version of the
Cassandra-Linear code, this brute-force approach is the method used to build
the Latin hypercube samples eventually used to train the emulators. To 
understand the computational and wall-time costs associated with our
brute-force solution, we include plots XXY and XXY.

\begin{comment} % The following paragraph says this stuff better
we left the system to run for three consecutive days. In this time, the 
largest minimum separation that we generated was approximately 0.08022.  
Recall from section sec_B1 that the theoretical best possible value for this 
setup is approximately 0.24183. It would have been more meaningful if you had 
counted the total number of function calls, but it isn’t too late to set up 
such a run. So, even after assigning a relatively large amount of compute to 
this brute force solution, we fail to obtain an LHC of even a third of the 
best minimum separation.
\end{comment} 

Remember from section~\ref{sec: lhc_theory} that we have an equation for the
best possible minimum separation that we can achieve with a Latin hypercube
of some given dimension and total number of samples. Let us consider a massless-neutrino emulator (a total of four dimensions per cosmology vector)
trained over five-thousand CAMB spectra. If we plug in $d = 4$ and $N=5000$,
we get XXY. Similarly, for our massive-neutrino emulator (a total of six 
dimensions per cosmology) trained over the same number of spectra, we find a 
theoretical best minimum-separation of XXY. 

It is based on figures XXY and XXY that we may claim that our approach is
inefficient and not well-suited to the approach of finding LHCs with large
minimum separations. As mentioned in section~\ref{sec: lhc_theory},
we value a high minimum separation because it means we are sampling the
space of cosmologies evenly rather than oversampling particular regions.
Therefore, we expect the consequence of a low minimum separation in the LHC to
be a higher error variance as well as a slightly higher average bias.

% One could argue that the variance in oversampled regions will decrease to
% compensate, right? I need to clarify that the decrease in variance in the
% oversampled regions would not be able to compensate the increased variance
% in the undersampled regions, because the space of power spectra is
% continuous, so closely spaced points reveal less about the true function
% than well spaced points.

Later, we will test our expectations by comparing emulators with different
minimum separations in their LHSs in section~\ref{sec: error_from_lhc}, to
which we defer remaining concerns about our approach to LHC generation.
Additionally, we will explore the question of superior methods in
section~\ref{sec: future_work}.

\section{Integrating Evolution Mapping}
\label{sec: generate_emu_data}

\textcolor{blue}{What happens to $A_s$ in the massless-neutrino case? Is it 
fixed at the default for model 0? I'm pretty sure it is!}

Another paragraph I want to have in this section: stress the part of the 
evolution mapping introduction, that the $\sigma_{12}$ value we're using to 
describe the model is actually the $\sigma_{12}$ value of the model's MEMNeC! 
This is so important and confusing that maybe I'll even recapitulate again 
later in the section on the \verb|generate_emu_data| script.

% generate\_emu\_data.py

\textcolor{blue}{This section will describe the process going from a Latin 
hypercube to a set of CAMB power spectra. We will describe how an array of six 
parameter values is fleshed out into a params object understood by CAMB. We 
will describe the procedure of modifying $h$ and $z$ until we arrive roughly 
at the $\sigma_{12}$ value that we desire, as well as the process of writing 
the \textit{actual} $\sigma_{12}$ value that we obtained back to the original 
hypercube. Why is this important? Because the Latin hypercube is used again 
later in the user\_interface.py script for the purpose of training the 
Gaussian emulator object.}

%s Preface the script as a whole. Where do I put this?
\verb|generate_emu_data.py| includes more complicated functions that apply the 
framework established in \verb|camb_interface.py| to the filling in of the 
hypercube object that we obtained from \verb|lhc.py| over
\verb|user_interface.py|.

%s Introduce the build_cosmology script

Now that we have a ``good enough'' unit LHC $\matr{X}$, we need to rescale it 
so that each axis, which currently runs from zero to one, runs along a range 
dictated by one of the cosmological parameters over which we are training the 
emulator. This rescaling is handled by the function \verb|build_cosmology| in
the \verb|generate_emu_data| script. It accepts as parameters the values of
$\omega_b$, $\omega_c$, $n_s$, $\sigma_{12}$, $A_s$, $\omega_nu$, and, 
optionally, a dictionary of parameter ranges. If the
parameter ranges are not provided, the cosmological parameters are assumed to
already have been rescaled. 
% We really should redo build_cosmology so that it assumes indices match to
% the same parameters every time! This is how the rest of the code works,
% after all.

\textcolor{orange}{What are the equations for rescaling that build cosmology
uses?}

The \verb|build_cosmology| function bridges the gap between the LHC format and 
the format expected by CAMB. Remember as described in
section~\ref{sec: 2emu_intro} that our LHC has either four or six dimensions 
depending on whether the neutrinos have mass. Separate from this, when 
computing a CAMB power spectrum, we need to
specify\footnote{By ``specify,'' we also mean the parameters for which CAMB
provides default values.}
several more parameters. These include, among others, the Hubble constant 
$H_0$ as well as the DE EOS parameters $w_a$ and $w_0$.

In practice, we split these parameters into two categories: we modify the
values of $h$ and $z$ until we obtain the desired value of $\sigma_{12}$
(this will be explained later in this section), and the rest are parameters
that we set to set to ``default'' values according to Aletheia model 0.
So long as the power spectrum's value for $\sigma_{12}$ agrees with the
prior-scaled value from the LHS, \textcolor{green}{it should not matter that 
we use the model 0 values, as these are evolution parameters}.
\textit{red}{Or should it? If this logic really held, shouldn't we be able to 
modify $w_a$ and $w_0$ in order to get the correct value of $\sigma_{12}$? But
I'm almost certain these are evolution parameters! Also: $\omega_\text{DE}$
is only an evolution parameter when everything else (all other omegas) is 
fixed...}

So, \verb|build_cosmology| makes a copy of the full Aletheia model 0 cosmology 
and replaces four or six entries depending on the dimensionality of the LHC. 

%s What kinds of assumptions does build_cosmology make

Remember that Aletheia model 0 corresponds to the Planck best fit parameters
this entails a flat Universe, $\omega_K = 0$.
\textcolor{red}{According to Andrea P., extreme values of $\omega_K$
affect the shape of $P(k)$. Doesn't that mean that $\omega_K$ is truly a
shape parameter?} We do not emulate over $\omega_K$ but assume take
it to be zero for all cells. However, as we explain in
section~\ref{sec: ev_mapping_intro}, this does not limit the applicability of
our emulators. The use of nonzero $\omega_K$ values will simply entail a
relabeling of the emulated power spectra. \textcolor{orange}{Then again, I'm
holding $\sigma_{12}$ fixed, so does it change anything at all?}

%s Now talk about fill_hypercube, a central function of this script

\verb|fill_hypercube| iterates through the rows $\bm{x}$ of the unit LHC
$\matr{X}$, packages them into a fully-specified cosmology using the function
\verb|build_cosmology|, and passes them to one of the evaluation functions,
which combines the CAMB calls from the \verb|camb_interface| script with the
principles of evolution mapping. Currently, the default evaluation function is
\verb|direct_eval_cell|, which relies on the \verb|evaluate_cosmology|
function from the \verb|camb_interface| script.

% 

%s What exactly does it mean here to integrate evolution mapping into the 
%s code?

We will now describe the process of rescaling a cosmology with the evolution 
parameters z and h in order to match the desired $\sigma_{12}$ value.

For the purposes of this treatment, we refer to any cell, for which this 
approach does not work, as an unsolvable cell. In practice, this means that 
even at the lowest h values supported by CAMB (\textcolor{orange}{what are 
these, and why is it unsafe to just use 0.01, even though that is sometimes 
allowed?}), we cannot match the desired sigma 12 value with a nonnegative 
redshift. CAMB does not allow for negative redshifts, although it prove be 
interesting to try to resolve this issue with a code that does allow negative 
redshifts, such as CLASS. \textcolor{orange}{Re-iterate the discussion from
section~\ref{sec: boltzmann_intro}: why didn’t we use CLASS for this project?}

%s Don't give up skeleton!

The consequences of unsolvable cells will be discussed more in
section~\ref{sec: prior_woes}. Despite the discouraging situation described 
there,
unsolvable cells do not immediately sink the entire data set. The remaining
still be used to train an emulator as long as even one cell has been solved.
\textcolor{orange}{This will be explained in section~\ref{sec: train_emu}}.

%s Bonus section that doesn't really fit anywhere specific: speed-up

Ariel’s idea for speeding up the program: use a best-guess z which only approximates the desired sigma12. Then, when we calculate the power spectrum using this redshift, get the actual sigma12 value from CAMB and replace the LHC entry with the true value.
We’ve implemented this now. We should definitely talk about how little of a difference that this makes. We should make error plots (comparing the two approaches with hyper cube as a control), but I strongly suspect the error will be negligible.


\section{Training the Emulator}
\label{sec: train_emu}

\textcolor{orange}{Don't forget to explain that the training X is actually the
unit LHS, not the rescaled LHS, because the unit LHS is already normalized
and therefore easier to handle in training.}

% train\_emu.py

\textcolor{blue}{This section will focus on the particular lines of}
\verb|GPy| \textcolor{blue}{that we used, as well
as the various data-cleaning and normalization statements that we used.
Normalization will be a really important topic. I want to explain some of
the theory behind why the emulator performs poorly with values outside of this
[0, 1] range.}

I don't know how to justify this kernel that I got from Alex via AndreaP...
\textcolor{orange}{That's no problem; Ariel says, just say ``we played around
and found that this works best.''}

\textcolor{blue}{Briefly explain what GPy is doing—don’t treat it like some black box.}

This script contains the functions used to build an emulator from the 
generated data sets. To train a new emulator, one instantiates the
\verb|Emulator_Trainer| class. We will henceforth refer to objects therefrom 
as trainers.

We separate the \verb|Emulator_Trainer| class from the \verb|Emulator| class
to make saving and loading more efficient. When transferring an emulator to a
different computer, for example, it is not necessary to transfer all of the
data sets used.
% Although these data sets only turn out to be 2\% as large as the GPR object, 
% which is provided by GPy and which performs the predictions. Maybe the
% situation justifies my approach with the large-k data sets? I don't think
% so--I think the GPR object simply gets bigger to keep the proportion the
% same...
However, we still need the \verb|Emulator| class to hold more than just the 
GPR, for example the normalization parameters.

Each emulator is an instance of
the \verb|Emulator| class with a \verb|GPy| GPR object at its core. We will 
henceforth refer to such instances as emulator \textit{objects}. Besides the 
GPR object at the core, emulator objects also convert $\bm{x}$ inputs and
$\bm{y}$ outputs in order to facilitate scientific access. Specifically, 
the GPR itself accepts $\bm{x}$ inputs and predicts $\bm{y}$ outputs 
in a normalized fashion which will not be easily understood unless first 
appropriately transformed.

%s Now talk about normalization

\textcolor{orange}{Needs segue.}
The normalizations are necessary for the high performance of the emulators. 
This need goes beyond numerical instabilities. For example, if a parameter's 
prior range falls within the relatively reasonable [0.0001, 0.0006] interval, 
its emulation will still be improved by normalizing it to a [0, 1] interval. 
Nonetheless, the improvement will be more dramatic the further away a prior is 
from this [0, 1] interval. The prior in $A_s$, regardless of the choice
between the three default priors files, covers extremely small values. Consequently, 
without the appropriate normalization, the trained GPR will be nearly 
insensitive to the impact of $A_s$ and \textcolor{green}{almost behave like a 
massless-neutrino emulator}; the lack of neutrino dependence would dominate 
the error in the resultant emulator.
\textcolor{orange}{Great start! But can we add some material in
section~\ref{sec: gpr_intro}: why is GPR vulnerable in this way?}


\section{Testing the Emulator}
\label{test_emu}

The majority of the quantification and visualization of the performance of the
emulator are left to the user. However, a couple of functions are provided for 
the sake of simple ``eyeball'' evaluation of the emulator's accuracy.
The \verb|test| function in the \verb|Emulator_Trainer| class (in the
\verb|train_emu| script) takes a set of $\matr{X}$ and $\matr{Y}$ test data 
and computes errors automatically. Specifically, it stores three sets of error 
metrics as class attributes for the trainer object: \verb|deltas| (the simple 
difference between the predicted spectra $\hat{\bm{y}}$ and the test spectra
$\bm{y}$, \verb|rel_errors| (the ratio of \verb|deltas| to the $\bm{y}$), and
\verb|sq_errors| (the squares of \verb|deltas|).

It follows from the definition of these error metrics that all of them have 
the same shape: each is a set of arrays equal in number to the $bm{y}$; and 
the size of each element array is determined by the number of scales at which 
the training power spectra were evaluated. \textcolor{orange}{In the current
version of the code, this number of scales must be equal in the training and
testing sets. But, if we add another layer of interpolation, we could easily
eliminate this limitation}.

The \verb|error_curves| function graphs the performance on the given test set 
using a collection of overplotted error curves, where each curve represents a 
single cosmology. By default, the $y$-axis is relative error and the
$x$-axis is the inverse scale, $k$. To break down the performance based on 
individual cosmological parameters, there is a \verb|param_index| function
parameter which will automatically color each curve according to its value in 
the chosen cosmological parameter. Since colors can sometimes be difficult to 
quickly compare, there is also a \verb|fixed_k| function parameter. When a 
fixed value for the inverse scale is provided, the plot becomes a scatter 
plot, with percent error as the $y$-axis and the chosen cosmological parameter 
as the $x$-axis.

%! Did I make sure to refer to priors correctly when transcribing ^ this 
%! function into train_emu.py? I think so… but it wouldn’t hurt to look one 
%! more time, especially since the X values are now automatically unit!

The \verb|error_statistics| function issues a rudimentary summary of the 
application of some NumPy aggregator\footnote{The user may also pass in
non-NumPy aggregators as long as they can be called in the exact same
way--the \verb|error_statistics| and \verb|error_hist| functions always
perform the call \verb|error_aggregator(errors, axis=1)|.}
\verb|error_aggregator|, which condenses each error array to a single point,
to one of the various error metrics described earlier. The 
most useful aggregators will be \verb|mean|, \verb|median|, and \verb|std|, 
but interesting information can also be gathered from, for example, the
\verb|min|, \verb|max|, and \verb|ptp| aggregators.
\verb|error_statistics| simply prints various statistics associated with the 
aggregation, such as the median of the aggregates.

The \verb|error_hist| function is essentially a visualization of the 
information printed out by the \verb|error_statistics| function: an aggregator
function is applied to the selected error array, then the aggregates are 
binned and plotted as a histogram. The user can specify the number of bins,
otherwise Sturges' rule is used.


\section{Accessing and Using the Emulator}

% user\_interface.py

\textcolor{orange}{Maybe we should rename this section, we're already doing
heavy access in the previous sections.}

\textcolor{blue}{The content of this section is still relatively uncertain. 
Since I am still tweaking some of the emulator's settings, I haven't spent too 
much time on a script dedicated purely to simplifying the interaction between 
the user and the emulator \textit{object} itself. Anyway, the hope is to 
provide some clear and simple 
descriptions of what functions the user should turn to in order to get started 
predicting power spectra using the results of this thesis.}

%s Now talk about the use of prior files

In section~\ref{sec: lhc_outline}, we introduced different priors 
included with CL, as well as their file names. These file names come into play 
when calling the function \verb|prior_file_to_dict|, which reads a prior file
into a Python dictionary. The reader is encouraged to add additional prior
files based on the format of the files provided in the \verb|priors|
subdirectory of the CL code.

%which is a helper function
% to \verb|get_data_dict|, which in turn is a helper function to the 
%\textcolor{orange}{unfinished} build_train_and_test_sets, which is a helper
% function to... ENOUGH!
