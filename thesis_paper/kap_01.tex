\chapter{Introduction, Theory, and Background}

\textcolor{blue}{I have a lot of different important concepts that I need to 
get through, so I can easily imagine this becoming a relatively long 
introduction compared to other master's theses.}

A primary goal of cosmology is to specify, as narrowly as possible, the 
parameters which define our Universe. These include, for example, the overall 
curvature of the Universe as well as its cold dark matter (CDM) content. These
parameters determine the full evolution of the Universe after the inflationary
period (whose beginnings were non-deterministic). \textcolor{green}{citation.
Probably we can get something from a Komatsu paper.} For example, depending on
the makeup of the Universe--how much of its total energy budget exists in the
form of each `ingredient' (cold dark matter, radiation, etc.)--the Universe
can have a finite lifetime. When the proportion of matter is high enough,
gravity will cause the Universe to collapse again on itself. By contrast, if
the proportion of dark energy is high enough, the Universe will continue to
accelerate in its expansion forever.

Cosmology has rapidly evolved into a high-precision science. For example, with
COBE (1989-1993) followed by WMAP (2001-2010) followed by Planck (2009-2013),
the uncertainties on several cosmological parameters have been tightened
significantly.
\textcolor{red}{Should I try to offer concrete examples of how these missions
increased the precision on our parameter values? I feel like that might not be
a good use of space in the intro.}

Ultimately, the goal of this work is to speed up the kinds of statistical
analyses which are necessary to tighten these uncertainties.
These analyses compare our cosmological observations to what we
would expect to see if we solved the equations of cosmological evolution with
different values for parameters.

\section{Brief Glossary of Our Cosmological Parameters}
\label{sec: param_glossary}

\textcolor{blue}{We need to explain what the different parameters mean! The
big omega terms are likely to be more familiar to readers, we can start with 
those.}

We will get more specific about terms like ``cosmological observations''
and ``what we would expect to see'' in the next section (\ref{sec: Pk_intro}, 
on the matter power spectrum). First, we should briefly introduce concrete
examples of cosmological parameters in which we are interested.

In this work, we will concentrate on different parameters to different 
extents.

The most important parameters for this work are $\omega_b$, $\omega_\text{CDM}$, $\omega_\nu$, and $n_s$.

$h$, $z$, and $A_s$ are means to ends

$\omega_k$, $\omega_\text{DE}$, $w_0$, $w_a$ make only minor appearances.

{segue from power spectrum discussion?} There is another facet to densities that will be conceptually important to this work: densities $\rho_i$ of different energy species. Similarly to before, it is common to define a related quantity $\Omega_i$ that is normalized, although here the normalization is enforces

\begin{equation}
\sum_i \Omega_i = 1
\end{equation}

IT’s MORE COMPLICATED THAN THAT—TALK ABOUT FLATNESS OF UNIVERSE and critical densities. These $\Omega_i$ parameters are referred to as “fractional energy densities.” The constant of normalization is simply $h^2$. $h$ is merely a different 

\begin{equation}
h = H_0 \, \frac{1}{100 }
\end{equation}

For example, the Planck best-fit value for H_0,  $67 \, \frac{\mathrm{km} / \mathrm{s}}{\mathrm{Mpc}}$, corresponds to the $h$ value $0.67$. 

Unfortunately, this normalization has its disadvantages. WHAT ARE THEY. In particular, in the context of parameter inference, it is inconvenient to have a parameter composed of other 


\section{The Matter Power Spectrum}
\label{sec: Pk_intro}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Einbinden einer Grafik  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[htb]
  \centering
  \includegraphics[scale=0.5]{siegel}
  \caption[Kurzform f"ur das Abbildungsverzeichnis]{Dies ist die Erkl"arung zum Bild.}
\end{figure}

How can we describe the Universe in such 
a way as to allow quantitative definitions of these phrases? As a starting 
point, we can imagine quantifying the energy density $\rho(\bm{x})$ of the
Universe at any one point $\bm{x}$ in three-dimensional space.

Let us return to the idea that cosmological parameters determine the the 
evolution of the Universe.
% In what ways can we quantify the Universe, in order to be able to describe
% its evolution? We can simply consider the time dependence of the density.

We can define a similar quantity, a relative density:

\begin{equation}
\delta(\bm{x}) = \frac{\rho(\bm{x}) - \bar{\rho}}{\bar{\rho}}
\end{equation}

which we refer to as the ``matter density contrast field.'' $\bar{\rho}$
represents the average energy density of the entire Universe.

$\delta(\bm{x})$ easier than $\rho(\bm{x})$ to work with since it
is unitless and especially because it is normalized--the integral of the
matter density contrast field, taken over the entire Universe, is unity. One particularly popular metric is the cosmic matter density 
contrast field. evolution of the Universe? The various constituents of the Universe 

Next, I want to talk about one way of describing the matter density contrast 
field: the power spectrum. The power spectrum can be probed in many different 
ways, and its precise shape and amplitude can tell us about several of these 
cosmological parameters.

Actually, the power spectra we are discussing in this thesis are linear-theory
power spectra of non-neutrino matter. \textcolor{blue}{But anyway, here I will cover some of 
the tried-and-true basic explanations of what the power spectrum is and why it 
is interesting for the question of parameter inference. I also want to 
discuss: why do we care about the linear-theory power spectrum? Why not jump 
straight to the nonlinear case?}

\textcolor{red}{The argument on the exponential should be a dot product, 
right?}

\begin{equation}
\tilde{\delta} (\bm{k}) = \int d^3 x \delta(\bm{x}) \exp(-i \bm{k} \bm{x})
\end{equation}

\begin{equation}
\langle \tilde{\delta} (\bm{k}_1) \tilde{\delta} (\bm{k}_2)^* \rangle
=
(2 \pi)^3 \delta_D^{(3)} (\bm{k}_2 \ \bm{k}_1) P(\bm{k})
\end{equation}


\section{Boltzmann Solvers and CAMB}

I want to talk about what a Boltzmann solver is and what kinds of equations it is solving.

To hint at what's to come, I start off this section by noting that several cosmological parameters have a fairly unique impact on the shape of the power spectrum, while others have a degenerate impact. Wouldn't it be great if we could know what the power spectrum would look like if we increased parameter $x$? Boltzmann solvers can help us with that.

This will mostly just be a theoretical discussion of these solvers. The hands-on stuff comes in the non-introductory section on CAMB.

I will also mention a couple of specific Boltzmann solvers, like CLASS and CAMB. I will briefly justify our use of CAMB over CLASS.

%%%

In essence, Boltzmann codes solve XXX in order to give us the power spectrum
of any universe characterized by some set of cosmological parameters. For
example, figure~\ref{fig: vary_omega_b} shows the impact of varying the
physical density in baryons, $\omega_b$. 

Several parameters have fairly unique impacts on the power spectrum.
Therefore, we can imagine building a collection of power spectra labeled by
their parameter configurations and comparing our real-world observations to
them. This should allow us to perform parameter inference.

\section{Monte Carlo Markov Chains}

This can be a very brief section, but I want to discuss a little bit of how most modern parameter inference works because it motivates the need for extremely fast power spectrum computation. It provides a sort of conceptual bridge between our ``pure'' goal (quantifying the cosmos) and the nitty-gritty bulk of the paper (optimizing emulator performance).

Metropolis-Hastings algorithm.

We don't know what the true probability distribution of power spectra is. In order to build this distribution with simulation results, we simply draw from the distribution. \textcolor{orange}{Refer to ``Data to Insights'' lecture notes in order to tighten this description.}

\section{Emulation: Basic Principles}
\label{sec: emulation_intro}

To conduct these MCMC analyses, we need several thousands of power spectra. However, if our Boltzmann solvers take on the order of three seconds to run, then these solvers will become the bottleneck of our analysis. \textcolor{orange}{Give some specific numbers for this.}

This motivates the introduction of emulation, basically multi-dimensional interpolation, in order to predict the power spectra. These predictions are orders of magnitude less time-expensive. 

Emulators interpolate across a high-dimensional parameter space. The primary
limitation is that the emulator has to be built with every possible parameter
in mind that an end-user could wish to vary. Yet there is a large number of
different cosmological parameters discussed in the modern literature.
``Currently available emulators only sample a few cosmological parameters,
often with restrictive ranges, and are not applicable to more general
parameter
spaces'' (\cbib{San21}). ``Due to the high computational cost of the required
simulations, [...] current emulators leave out parameters such as the
curvature
of the Universe or dynamic energy models beyond the standard CPL
parametrization'' (\cbib{San21}).

I'll talk a little about different emulators currently available, such as COMET. Some emulate non-linear power spectra, for example, and several even include massive neutrinos. But this thesis will demonstrate that massive neutrinos can be included into our evolution mapping approach, which will be introduced in section~\ref{sec: ev_mapping}.

% (This is good news because the evolution mapping approach greatly simplifies the parameter space, and enhances the accuracy), which is the subject of the next section.

\section{Gaussian Process Regression}

% What is a Gaussian Process?

Most emulators are based on a Gaussian Process (GP). A GP is a Gaussian
distribution over functions\footnote
{A GP is the limit of a one-hidden-layer neural network as the number of
neurons approaches infinity.}, which can be interpreted
as the infinite-dimensional generalization of the multivariate normal
distribution. The inference of continuous values with a GP prior
is known as Gaussian process regression, or Kriging. GP regression is a
powerful non-linear multivariate interpolation tool. The computational
complexity of inference and likelihood evaluation within GP regression is cubic
in the number of points. This makes GP regression an excellent companion to
Latin hypercube sampling (LHS), which makes highly-efficient use of a limited 
number of samples and will be explained in greater depth in section~\ref{sec:
lhc_intro}.

Neural networks (NNs) generally need much larger sample sizes to reach
comparable levels of
accuracy. Due to various alterations in the Cassandra-Linear code over its
development, several regenerations of the various emulator data sets were
necessary. This practical constraint motivated the use of a GP for our
emulator. Furthermore, NNs invariably require much more complicated setup and
tuning--for example, in the precise architecture of the network (e.g. nodes
per layer, layer types) as well as the hyperparameters (e.g. learning rate).
By contrast, as we explain in section~\ref{sec: emu_training}, a Gaussian
process regression is highly straightforward to set up and modify. Therefore,
for a demonstration project such as Cassandra-Linear, we elected to base our
emulator on a GP. Please refer to the section~\ref{sec: future_work} for a
continuation of this discussion.

Are there other prediction approaches besides GPs and NNs? IF so, I need to
further justify WHY we’re using GPs.
GPs work best when there are few samples and a lot of parameters, right?
But why is that so? What is the math behind that?


\section{Sampling Approach: Latin Hypercube}
\label{sec: lhc_theory}

I imagine this is going to be an extremely short section. We should motivate why we're using this style of sampling.

What is the theoretical best LHC that we could make?

Besides, can we explain this equation?

\section{Evolution Mapping}

This section will also include an extremely brief summary of Ariel's paper motivating the use of $\sigma_{12}$ instead of $\sigma_8$.

    I have two primary objectives for this section: explain the unit system
    we are using (ditch $h$ factor because it messes up everything--but
    only briefly summarize the main arguments of Sanchez 2020), and briefly
    summarize why we can funnel all of the evolution parameters through
    $\sigma_{12}$ in this way. Unfortunately, this second objective will
    almost certainly require you to bust out a few equations, and even to
    manipulate them a little to tease out relations essential to this paper.

Conventional emulator calibration entails the historical units of Mpc / $h$,
but if we use instead units of Mpc, then we can distill all of the evolution
parameters into one parameter, $\sigma_{12}$. Since $h$ is already its own parameter, the conventional $\sigma_8$ parameter is truly a mixture
of two parameters. This presents a host of misleading results  and statistical
ambiguities (\cbib{San20}) which are outside of the scope of this work but
which prompt us to abandon $\sigma_8$.
Similarly, throughout this paper we will refrain from using the conventional
fractional density parameters $\Omega_i$ in favor of the physical density
parameters $\omega_i = \Omega_i h^2$ which similarly eliminate the
dependence on $h$.

(\cbib{San21}) proposes to divide up the full set of cosmological
parameters into two categories: \textit{evolution} parameters $\mathcal{O}_E$
(such as $\omega_b$, $\omega_c$, and $\eta_s$)
affect the amplitude of the power spectrum at a particular redshift, while
\textit{shape} parameters $\mathcal{O}_S$
(such as $\omega_K$, $\omega_\text{DE}$, w(a))
affect the shape of the power
spectrum.

We take, as the evolution mapping relation for the power spectrum, equation 13
from \cbib{San21}:

\begin{equation}
\label{eq: evMapping_pSpectrum}
    \Delta^2_L (k | z, \Theta_s, \Theta_e)
    =
    \Delta_L^2 (k | \Theta_s, \sigma_{12} \left( z, \Theta_s, \Theta_e \right))
\end{equation}

Why is this scheme important? Evolution mapping greatly simplifies the emulator
implementation. Because we can
funnel all of the evolution parameters through $\sigma_{12}$, we've effectively
collapsed an entire category of parameters to just one parameter. Fewer
parameters means that we get a more accurate emulator.

``At the linear level, all models characterized by identical shape parameters
and the same values of the parameter combinations $b \sigma_{12}(z)$ and
$f \sigma_{12}(z)$ will be identical'' (\cbib{San21}).

Now, for the hiccup, which segues into the next section: this scheme is broken by one parameter, the Universe's
density in neutrinos. (In the next section: why this is so and what we can do
about it.)

%%%

Three shape parameters of core interest to this paper are the physical density in baryons $\omega_b$ (whose impact on the power spectrum is shown in figure~\ref{fig: omega_b_dependence}), the physical density in cold dark matter $\omega_c$ (figure~\ref{fig: omega_c_dependence}), and the spectral index $n_s$, (figure~\ref{fig: ns_dependence}). The remaining parameters $\sigma_{12}$ and $A_s$, as well as the quantities $z$ and $h$, all shift only the amplitude of the power spectrum, as illustrated in figure~\ref{eq: sig12_dependence}. 

For the sake of completion, we also show the impact of the evolution parameter $\sigma_{12}$ in figure~\ref{fig: sig12_dependence} and stress that the other quantities 


\section{Neutrinos and Their Cosmological Impact}

(\cbib{Kiakotou}): ``Neutrinos with masses on the eV scale or below will be a
hot component of the dark matter and will free-stream out of overdensities and
thus wipe out small-scale structures.''

``In general, a larger density of relativistic species leads to a smaller
growth of matter fluctuations'' (\cbib{Zennaro}).

The point of this section is: why is $\omega_\nu$ bad for the
evolution mapping scheme? Because neutrinos exhibit redshift-dependent
damping of the power-spectrum, and therefore affect both the shape and the
amplitude of the power spectrum. Whenever massive neutrinos are present,
the growth factor becomes scale-dependent, which disrupts the
evolution-mapping scheme.

Why do they behave in this way? All neutrinos start off as
relativistic particles in the early Universe, acting as a type of radiation.
But as the Universe continues to expand and cool, the neutrinos behave
increasingly like dark matter.
In this way, the physical density in neutrinos impacts both the shape and the
evolution.

``The popular heuristic formula for the linear theory suppression of the matter
fluctuations by free-streaming $\nu$, $\Delta P(k) / P(k) \approx -8 f_\nu$, is
valid only on very small scales $k > 0.8 h$ / Mpc, However, it is not of
practical use as this is in the strongly nonlinear regime of matter
clustering'' (\cbib{Kiakotou}).

One proposed solution is to treat the neutrinos as a small correction factor
to the results from an anologous cosmology with the same $\omega_m$ but with
$\omega_\nu = 0$. This of course limits the applicability of our emulator to
cosmologies with very small $\omega_\nu$, but this constraint agrees with
current observations (\textcolor{orange}{which?}).

I want to end this section with a vague plan of action: we want to play around with CAMB power spectra to see if there are any simple ways around this limitation in our approach.

%%% New stuff

We already have an approximation for the power spectrum of a massive-neutrino cosmology within the evolution mapping scheme. The $\sigma_{12}$ value that we described earlier is actually the $\sigma_{12}$ value of the model's MEMNeC. can already be approximated within evolution-mapping by slightly altering scheme. \textcolor{red}{Is it fair to say we are adjusting, or was this actually the same scheme as it always was?} We take a MEMNeC and the desired cosmology. The sigma 12 is actually the sigma 12 of the MEMNeC. Then we treat the physical density in neutrinos as a shape parameter along with $A_s$.
