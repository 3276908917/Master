% Some miscellaneous notes:
% I will indicate with my sigil the yellow-folder note pages that I've already
% digitized.

\documentclass[11pt]{article}
\usepackage{reports}

% The following block isn't actually used on the output.
% But it's easier to simply fill it out than to try
% to edit the rather fragile reports.sty file.
\newcommand*{\instr}{Ariel S\'{a}nchez}
\newcommand*{\term}{31.07.2023}
\newcommand*{\coursenum}{Master's Thesis}
\newcommand*{\hwnum}{Redshift Dependence of Neutrino Damping on the Power
Spectrum}

\usepackage{pdfpages}
\usepackage{bm}
\usepackage{listings}
\usepackage{titling}
\usepackage[normalem]{ulem} 

\IfFileExists{biblatex.sty} {
    \usepackage[style=authoryear]{biblatex}
    \addbibresource{master_thesis.bib}
}

% This code formatting sucks but I don't want to output extra spaces.
\newcommand{\cbib}[1]
{\IfFileExists{biblatex.sty}
{\cite{#1}}
{[citation ``#1'' cannot be linked in the current environment]}}

\graphicspath{{./res/}}

\begin{document}

\fontsize{12}{15}

\begin{center}
Lukas Finkbeiner: Master's Thesis
\end{center}

\tableofcontents

\begin{centering}
\section{Abstract}
\end{centering}

``The goal of this project is to produce an emulator of the linear-theory
power spectrum based on evolution mapping'' (A. G. S\'{a}nchez, private
communication). In particular, this paper seeks to extend the evolution mapping
scheme of \cbib{San21} to massive-neutrino cosmologies by applying a correction
factor to results from emulators built on massless-neutrino simulations.  We find that the scalar mode amplitude $A_s$ can be used to quantify the suppression of structure growth due to massive neutrinos. Consequently, by including this parameter, we can successfully train over the physical density of the universe in massive neutrinos.

\begin{centering}
\section{Introduction, Theory, and Background}
\end{centering}

I have a lot of different important concepts that I need to get through, so I can easily imagine this becoming a relatively long introduction for a master's thesis.

\begin{centering}
\subsection{The Matter Power Spectrum}
\end{centering}

First, I want to talk about how a primary goal of cosmology is to specify, as narrowly as possible, the parameters which define our Universe. These include, for example, the overall curvature of the Universe as well as its cold dark matter (CDM) content.

Next, I want to talk about one powerful result, the power spectrum. The power spectrum can be probed in many different ways, and it's precise shape and amplitude can tell us about several of these cosmological parameters.

Actually, the power spectra we are discussing in this thesis are linear-theory power spectra of non-neutrino matter. But anyway, here I will cover some of the tried-and-true basic explanations of what the power spectrum is and why it is interesting for the question of parameter inference. 

\begin{centering}
\subsection{Boltzmann Solvers and CAMB}
\end{centering}

To hint at what's to come, I start off this section by noting that several cosmological parameters have a fairly unique impact on the shape of the power spectrum, while others have a degenerate impact. 

This is mostly just a theoretical discussion of these solvers. The hands-on stuff comes in the non-introductory section on CAMB.

I want to talk about what a Boltzmann solver is and what kinds of equations it is solving.

I will also mention a couple of specific Boltzmann solvers, like CLASS and CAMB. I will briefly justify our use of CAMB over CLASS.

\begin{centering}
\subsection{Monte Carlo Markov Chains}
\end{centering}

This can be a very brief section, but I want to discuss a little bit of how most modern parameter inference works because it motivates the need for extremely fast power spectrum computation. It provides a sort of conceptual bridge between our ``pure'' goal (quantifying the cosmos) and the nitty-gritty bulk of the paper (optimizing emulator performance).

\begin{centering}
\label{sec: state_of_the_art}
\subsection{Emulation, Gausian Processes, and Latin Hypercube Sampling}
\end{centering}

To conduct these MCMC analyses, we need several thousands of power spectra. However, if our Boltzmann solvers take on the order of three seconds to run, then these solvers will become the bottleneck of our analysis.

This motivates the introduction of emulation, basically multi-dimensional interpolation, in order to predict the power spectra. These predictions are orders of magnitude less time-expensive. 

Most emulators are based on a Gaussian Process (GP). A GP is a Gaussian
distribution over functions\footnote
{A GP is the limit of a one-hidden-layer neural network as the number of
neurons approaches infinity.}, which can be interpreted
as the infinite-dimensional generalization of the multivariate normal
distribution. The inference of continuous values with a GP prior
is known as Gaussian process regression, or Kriging. GP regression is a
powerful non-linear multivariate interpolation tool. The computational
complexity of inference and likelihood evaluation within GP regression is cubic
in the number of points. This makes GP regression an excellent companion to
Latin hypercube sampling (LHS), which makes highly-efficient use of a limited number of samples. GP-based emulators require fewer training samples than
neural networks but provide smaller speedups. GP's allow natural propagation of
uncertainty in predictions to the final posterior distribution; neural
networks lack this feature.

Emulators interpolate across a high-dimensional parameter space. The primary
limitation is that the emulator has to be built with every possible parameter
in mind that an end-user could wish to vary. Yet there is a large number of
different cosmological parameters discussed in the modern literature.
``Currently available emulators only sample a few cosmological parameters,
often with restrictive ranges, and are not applicable to more general parameter
spaces'' (\cbib{San21}). ``Due to the high computational cost of the required
simulations, [...] current emulators leave out parameters such as the curvature
of the Universe or dynamic energy models beyond the standard CPL
parametrization'' (\cbib{San21}).

I'll talk a little about different emulators currently available, such as COMET. Some emulate non-linear power spectra, for example, but most lack this physical density in neutrinos. That is where Cassandra-linear, the emulator of this thesis, comes in.

\begin{centering}
\subsection{Evolution Mapping}
\end{centering}

This section will also include an extremely brief summary of Ariel's paper motivating the use of $\sigma_{12}$ instead of $\sigma_8$.

\footnote{
    I have two primary objectives for this section: explain the unit system
    we are using (ditch $h$ factor because it messes up everything--but
    only briefly summarize the main arguments of Sanchez 2020), and briefly
    summarize why we can funnel all of the evolution parameters through
    $\sigma_{12}$ in this way. Unfortunately, this second objective will
    almost certainly require you to bust out a few equations, and even to
    manipulate them a little to tease out relations essential to this paper.
}

Conventional emulator calibration entails the historical units of Mpc / $h$,
but if we use instead units of Mpc, then we can distill all of the evolution
parameters into one parameter, $\sigma_{12}$. Since $h$ is already its own
(shape!) parameter, the conventional $\sigma_8$ parameter is truly a mixture
of two parameters. This presents a host of misleading results  and statistical
ambiguities (\cbib{San20}) which are outside of the scope of this work but
which prompt us to abandon $\sigma_8$.
Similarly, throughout this paper we will refrain from using the conventional
fractional density parameters $\Omega_i$ in favor of the physical density
parameters $\omega_i = \Omega_i h^2$ which similarly eliminate the
dependence on $h$.

(\cbib{San21}) proposes to divide up the full set of cosmological
parameters into two categories: \textit{evolution} parameters $\mathcal{O}_E$
(such as $\omega_b$, $\omega_c$, and $\eta_s$)
affect the amplitude of the power spectrum at a particular redshift, while
\textit{shape} parameters $\mathcal{O}_S$
(such as $\omega_K$, $\omega_\text{DE}$, w(a))
affect the shape of the power
spectrum. \textcolor{orange}{One paper made it sound like shape parameters
affect both shape and amplitude. Was it one of the S\'{a}nchez papers? I think
I misunderstood it, but we have to confirm that we can now understand what was
meant.\footnote{
    I'm certain that I misread this second description, for the following.
    The main \textit{problem} with neutrinos (i.e. why they \textit{don't}
    play nicely with the categories that I am trying to describe here)
    is that they affect \textit{both} shape and evolution! As far as we know,
    the physical density in neutrinos is the \textit{only} parameter that
    is poorly behaved in this way.
}
Why does it help to have these categories in the first place?}\footnote{
    As I understand it, we split the parameters up this way because
    one category of parameters (I think the evolution parameters)
    can be entirely funneled through just one parameter
    (I think it's $\sigma_{12}$, but obviously we should double check that)
}

We take, as the evolution mapping relation for the power spectrum, equation 13
from \cbib{San21}:

\begin{equation}
\label{eq: evMapping_pSpectrum}
    \Delta^2_L (k | z, \Theta_s, \Theta_e)
    =
    \Delta_L^2 (k | \Theta_s, \sigma_{12} \left( z, \Theta_s, \Theta_e \right))
\end{equation}

Why is this scheme important? Evolution mapping\footnote{I think we need to
double check our language here, I think evolution mapping refers to more than
just the categorization scheme} greatly simplifies the emulator
implementation. Because we can
funnel all of the evolution parameters through $\sigma_{12}$, we've effectively
collapsed an entire category of parameters to just one parameter. Fewer
parameters means that we get a faster emulator.

``At the linear level, all models characterized by identical shape parameters
and the same values of the parameter combinations $b \sigma_{12}(z)$ and
$f \sigma_{12}(z)$ will be identical'' (\cbib{San21}).

Anticipating a counterargument: ``For a general cosmology, $h$ is a combination
of shape and evolution parameters. However, for a $\Lambda$CDM Universe, fixing
the values of the shape parameters $\omega_b$ and $\omega_c$ and varying $h$
corresponds to assuming different values of the purely evolutionary parameter
$\omega_\text{DE}$'' (\cbib{San21}).

From my original assignment email:
``If the dependence of the transfer function, $T(k)$, on the relevant physical
density parameters is emulated, it is possible to obtain a prediction for the
full power spectrum in Mpc units for any choice of evolution parameters and
redshift. The full shape of $P_L(k|z)$ can be obtained from the output of the
emulator of $T(k)$ and $n_s$ (and any additional shape parameter controlling
the scale dependence of the primordial power spectrum). The correct amplitude
of $P_L(k|z)$ can be obtained by computing the value of $\sigma_{12}(z)$ in
terms of the desired normalization at $z=0$ and the growth factor $D(z)$ for
the cosmology being considered. Such a recipe would be valid for a wide range
of evolution cosmological parameters, including non-standard models, as long as
the corresponding linear growth factor is scale-independent. The smaller number
of parameters required to emulate $T(k)$ would increase the overall accuracy of
the predictions while avoiding the need to sample the redshift evolution of
$P_L(k|z) P_L(k|z)$ explicitly would greatly simplify the calibration
procedure'' (A. G. S\'{a}nchez, private communication).

\textcolor{orange}{I believe that $P_L(k|z) P_L(k|z)$ is a typo in the above
paragraph, but I don't know how else to transcribe Ariel's email. Maybe he
made a mistake?}

``The time evolution of $\Delta_L^2(k)$ in models with identical shape
parameters
but different evolution parameters can be mapped from one to the other by
relabelling the redshifts that correspond to the same clustering amplitude
(i.e. $\sigma_{12}(z)$)''.
(\cbib{San21}).

Now, for the hiccup: this scheme is broken by one parameter, the Universe's
density in neutrinos. (In the next section: why this is so and what we can do
about it.)

\begin{centering}
\subsection{Neutrinos and Their Cosmological Impact}
\end{centering}

\footnote{
    Here I am violating my traditional rule of ordering sections by
    increasing complexity, because I consider the $\sigma_{12}$ argument
    decidedly more esoteric than a summary of the relevant points of neutrino
    physics. I think we should stick with this structure though, in the
    interest of narrative. Specifically, neutrinos represent a wrench in the
    plan. For that development to make sense, we first need to have established
    what
    the ``plan'' is: evolution mapping.
}

(\cbib{Kiakotou}): ``Neutrinos with masses on the eV scale or below will be a
hot component of the dark matter and will free-stream out of overdensities and
thus wipe out small-scale structures.''

``In general, a larger density of relativistic species leads to a smaller
growth of matter fluctuations'' (\cbib{Zennaro}).

``Thermal velocities dominate neutrino dynamics during the first time-steps
(after $z=99$), having a dispersion roughly five orders of magnitude larger
than the dispersion of peculiar neutrino velocities.''

The evolution of neutrino density, like that of photon density, depends on the
momentum distribution.

\begin{equation}
    p_{\nu, i}(z)
    =
    \frac{(k_B T_{\nu, 0})^4}{\pi^2}
    \,
    (1 + z)^4
    \,
    \mathcal{F} \left[ \frac{m_{\nu, i}}{k_B T_{\nu, 0} (1 + z)} \right]
\end{equation}

where $i$ indexes the neutrino species and

\begin{equation}
    \mathcal{F}(y) \equiv \int_0^\infty \frac{x^2 \sqrt{x^2 + y^2}}{1 + e^x}
\end{equation}

 (\cbib{Zennaro})

``The two-fluid approximation consists in assuming that neutrino perturbations
are well described by two variables: density and velocity divergence''
(\cbib{Zennaro}).

Remember what the point of this section is: why is $\omega_\nu$ bad for the
evolution mapping scheme? Because neutrinos exhibit redshift-dependent
damping of the power-spectrum, and therefore affect both the shape and the
amplitude of the power spectrum. Whenever massive neutrinos are present,
the growth factor becomes scale-dependent, which disrupts the
evolution-mapping scheme.

Why? I \textcolor{orange}{think} that it's because all neutrinos start off as
relativistic particles in the early Universe, acting as a type of radiation.
But as the Universe continues to expand and cool, the neutrinos behave
increasingly like dark matter.
In this way, the physical density in neutrinos impacts both the shape and the
evolution. What I don't understand is why we say that this is so when we
only ever consider the shape of the power spectrum at one redshift. By
``shape'' I was assuming that S\'{a}nchez meant the shape of $P(k)$ versus
$k$ but if $\omega_\nu$ is really a problem, then shape should also refer
to $P(k)$ through all values $z$? I don't quite understand this, but
\textcolor{green}{maybe we can ask Ariel}.

``The scale where the suppression of power (from neutrino free-streaming) sets
in is controlled by the comoving Hubble radius at the time when the neutrinos
became non-relativistic, corresponding to a comoving wave number
$k_{fs} = 0.10 \Omega_m h \sqrt{f_\nu}$.

``The popular heuristic formula for the linear theory suppression of the matter
fluctuations by free-streaming $\nu$, $\Delta P(k) / P(k) \approx -8 f_\nu$, is
valid only on very small scales $k > 0.8 h$ / Mpc, However, it is not of
practical use as this is in the strongly nonlinear regime of matter
clustering'' (\cbib{Kiakotou}).

One proposed solution is to treat the neutrinos as a small correction factor
to the results from an anologous cosmology with the same $\omega_m$ but with
$\omega_\nu = 0$. This of course limits the applicability of our emulator to
cosmologies with very small $\omega_\nu$, but this constraint agrees with
current observations (\textcolor{orange}{which?}).

I want to end this section with a vague plan of action: we want to play around with CAMB power spectra to see if there are any simple ways around this limitation in our approach.

\begin{centering}
\section{CAMB, Initial Setup}
\end{centering}
\footnote{
    At this point, we've built suspense in the reader: we have a goal and
    we have an idea about how to get to that goal. Therefore, now is the
    appropriate time to divert into laying the technical groundwork which
    will allow us to explore our idea through concrete implementations and
    to test our idea with plots and numbers.
}

CAMB is a Fortran code with a Python wrapper\footnote{
\url{https://github.com/cmbant/CAMB}
}which we will be using for the
entirety of this project.

To introduce the reader to the scope of CAMB, we will now introduce
some basic simulated power spectra along with a summary of the dynamic
parameters which will be of greatest interest to us.

Neutrino mass hierarchy: the options are degenerate, normal, and inverted.
The CAMB documentation annotates this parameter as ``(1 or 2 eigenstate
approximation),'' but this is somewhat unclear. Is the degenerate
hierarchy the single mass eigenstate approximation? Do both normal
and inverted hierarchies involve two eigenstates?

I guess we could already start putting a couple of plots here from my existing
notebooks. I mean, I'm sure none of these plots will survive to the final
version, but they might help to guide my discussion for a little while.

IN THIS NEXT SECTION WE WILL BE DISCUSSING SOME OF THE VARIOUS CAMB SETTINGS
AND THE STANDARD VALUES THAT WE USE, INCLUDING BRIEF ILLUSTRATIONS OF THE
SIGNIFICANCE OF EACH SETTING BY WAY OF EXAMPLE ERROR PLOTS WHEN NONSTANDARD
VALUES ARE USED.

In figure \ref{fig: spectrum_type}, we can see that requesting of the wrong
power spectrum type can in some low-$\omega_\nu$ cases yields errors so low
that we might accidentally overlook them. This error pattern is easily
recognizable and is a consequence of the definition of the power spectrum: the
Fourier transform  of the two-point correlation function.
\textcolor{red}{If we
have more matter overall, we expect larger correlations.
WE DON'T HAVE MORE MATTER! That's the whole point of the MEMNeC. The amount of
matter is the same, but cold dark matter clumps more on smaller scales than
neutrinos do. Wait, sorry, I'm confusing myself. We're talking about just one
model THEN we look at two different spectra types. Sorry.
}
Furthermore, the impact of decreasing redshift is only to shift the inflection
point towards larger scales.


\begin{centering}
\section{Cassandra-Linear: a Python Package}
\end{centering}

This will be a rather long, dry, and technical section with subsections based on each of the core scripts making up the Python package that I have been developing. It will in some ways paraphrase and summarize the documentation, explaining the basic use of the package as well as important limitations. Of course, fairly early on in this section, there will be a footnote linking to my GitHub repository, which will be made public once I'm ready to hand in this thesis.

Even if my code doesn't end up in a bigger repository, I nevertheless think that it's important to the scientific process that I describe in detail the code that I have written. Besides, if any readers want to experiment specifically with the ideas discussed in this thesis, they may find my code more accessible because it is, in a sense, "single-purpose"--that is to say, written almost exclusively to investigate the topics of this paper.

\begin{centering}
\section{Results and Analysis}
\end{centering}

\begin{centering}
\subsection{Quantifying the Performance of the Emulator}
\end{centering}

Before I even show any results for this emulator, I would like to motivate the challenge of gauging the accuracy and reliability of the emulator. Since we have to quantify performance over a hypervolume of parameter space, there is no concept of, for example, a chi-squared test that can applied here. So I am trying to pre-empt objections like those raised by Stella.

I would like to include a brief review of the relevant literature to explain the kinds of performance metrics that are most commonly applied in emulator papers (as I remember, largely percent-level bounds when comparing against ground truth data sets). Then, I would like to break down Ariel's arguments for why absolute error is of greater consequence to us than relative errors.

For the purpose of this paper, we use the weak and approximate performance metric of simply generating a large number of additional CAMB spectra and comparing them to the predictions of the emulator.

\begin{centering}
\subsection{Percent and Absolute Errors on Random Cosmologies}
\end{centering}

This will be a fairly short section, basically just showing the plot of 5000 error curves in these two ways. I may focus in on different k-ranges, but the error curves are currently quite flat, so I don't think that would be a good use of space.

Furthermore, I will try to select a couple of cosmologies out of the 5000 (maybe a low-error case and a high-error case) as examples of the performance on just one at a time. But I'm not sure how insightful that will be, I don't know if that will tell the reader a whole lot.

\begin{centering}
\subsection{Performance in Different Parameters}
\end{centering}

Here, I will either include color plots or, as Dante suggests, monochrome plots with error as one axis and parameter value as the other (i.e. $k$ fixed). Then, if I haven't tightened the $\sigma_{12}$ performance by the time of submission, I can talk about how this is the most promising avenue for refinement of the emulator. In any case, I plan to spend some time talking about \textit{why} parameter x is the current biggest problem for the emulator. 

\begin{centering}
\section{Discussion}
\end{centering}

\begin{centering}
\subsection{Future Work}
\end{centering}

Here I will talk about what kinds of questions we estimate will be most fruitful for further inquiries about this topic and this code.

\IfFileExists{biblatex.sty} {
    \printbibliography
}

\end{document}
