% Some miscellaneous notes:
% I will indicate with my sigil the yellow-folder note pages that I've already
% digitized.

\documentclass[11pt]{article}
\usepackage{reports}

% The following block isn't actually used on the output.
% But it's easier to simply fill it out than to try
% to edit the rather fragile reports.sty file.
\newcommand*{\instr}{Ariel S\'{a}nchez}
\newcommand*{\term}{31.07.2023}
\newcommand*{\coursenum}{Master's Thesis}
\newcommand*{\hwnum}{Redshift Dependence of Neutrino Damping on the Power
Spectrum}

\usepackage{pdfpages}
\usepackage{bm}
\usepackage{listings}
\usepackage{titling}
\usepackage[normalem]{ulem} 

\IfFileExists{biblatex.sty} {
    \usepackage[style=authoryear]{biblatex}
    \addbibresource{master_thesis.bib}
}

% This code formatting sucks but I don't want to output extra spaces.
\newcommand{\cbib}[1]
{\IfFileExists{biblatex.sty}
{\cite{#1}}
{[citation ``#1'' cannot be linked in the current environment]}}

\graphicspath{{./res/}}

\begin{document}

\fontsize{12}{15}

\begin{center}
Lukas Finkbeiner: Adapting Evolution-Mapping Emulators for Massive-Neutrino Cosmologies
\end{center}

\tableofcontents

\begin{centering}
\section{Abstract}
\end{centering}

``The goal of this project is to produce an emulator of the linear-theory
power spectrum based on evolution mapping'' (A. G. S\'{a}nchez, private
communication). In particular, this paper seeks to extend the evolution
mapping scheme of \cbib{San21} to massive-neutrino cosmologies by applying a
correction
factor to results from emulators built on massless-neutrino simulations. We
find that the scalar mode amplitude $A_s$ can be used to quantify the
suppression of structure growth due to massive neutrinos. Consequently, by
including this parameter, we can successfully train over the physical density
of the universe in massive neutrinos. We introduce a new emulation code,
Cassandra-Linear, which combines this expanded parameter space with evolution
mapping. We include various error statistics and show that the emulator
performs roughly at the level of error associated with CAMB itself
\textcolor{green}{Do I have a citation for the error associated with CAMB?}

\begin{centering}
\section{Introduction, Theory, and Background}
\end{centering}

I have a lot of different important concepts that I need to get through, so I can easily imagine this becoming a relatively long introduction compared to other master's theses.

\begin{centering}
\subsection{The Matter Power Spectrum}
\end{centering}

A primary goal of cosmology is to specify, as narrowly as possible, the parameters which define our Universe. These include, for example, the overall curvature of the Universe as well as its cold dark matter (CDM) content.

Next, I want to talk about one way of describing the matter density contrast field, the power spectrum. The power spectrum can be probed in many different ways, and its precise shape and amplitude can tell us about several of these cosmological parameters.

Actually, the power spectra we are discussing in this thesis are linear-theory power spectra of non-neutrino matter. But anyway, here I will cover some of the tried-and-true basic explanations of what the power spectrum is and why it is interesting for the question of parameter inference. I also want to discuss: why do we care about the linear-theory power spectrum? Why not jump straight to the nonlinear case?

\begin{centering}
\subsection{Boltzmann Solvers and CAMB}
\end{centering}

I want to talk about what a Boltzmann solver is and what kinds of equations it is solving.

To hint at what's to come, I start off this section by noting that several cosmological parameters have a fairly unique impact on the shape of the power spectrum, while others have a degenerate impact. Wouldn't it be great if we could know what the power spectrum would look like if we increased parameter $x$? Boltzmann solvers can help us with that.

This will mostly just be a theoretical discussion of these solvers. The hands-on stuff comes in the non-introductory section on CAMB.

I will also mention a couple of specific Boltzmann solvers, like CLASS and CAMB. I will briefly justify our use of CAMB over CLASS.

%%%

In essence, Boltzmann codes solve XXX in order to give us the power spectrum
of any universe characterized by some set of cosmological parameters. For
example, figure~\ref{fig: vary_omega_b} shows the impact of varying the
physical density in baryons, $\omega_b$. 

Several parameters have fairly unique impacts on the power spectrum.
Therefore, we can imagine building a collection of power spectra labeled by
their parameter configurations and comparing our real-world observations to
them. This should allow us to perform parameter inference.

\begin{centering}
\subsection{Monte Carlo Markov Chains}
\end{centering}

This can be a very brief section, but I want to discuss a little bit of how most modern parameter inference works because it motivates the need for extremely fast power spectrum computation. It provides a sort of conceptual bridge between our ``pure'' goal (quantifying the cosmos) and the nitty-gritty bulk of the paper (optimizing emulator performance).

Metropolis-Hastings algorithm.

We don't know what the true probability distribution of power spectra is. In order to build this distribution with simulation results, we simply draw from the distribution. \textcolor{red}{Refer to ``Data to Insights'' lecture notes in order to tighten this description.}

\begin{centering}
\subsection{Emulation: Basic Principles}
\label{sec: emulation_intro}
\end{centering}

To conduct these MCMC analyses, we need several thousands of power spectra. However, if our Boltzmann solvers take on the order of three seconds to run, then these solvers will become the bottleneck of our analysis. \textcolor{red}{Give some specific numbers for this.}

This motivates the introduction of emulation, basically multi-dimensional interpolation, in order to predict the power spectra. These predictions are orders of magnitude less time-expensive. 

Emulators interpolate across a high-dimensional parameter space. The primary
limitation is that the emulator has to be built with every possible parameter
in mind that an end-user could wish to vary. Yet there is a large number of
different cosmological parameters discussed in the modern literature.
``Currently available emulators only sample a few cosmological parameters,
often with restrictive ranges, and are not applicable to more general parameter
spaces'' (\cbib{San21}). ``Due to the high computational cost of the required
simulations, [...] current emulators leave out parameters such as the curvature
of the Universe or dynamic energy models beyond the standard CPL
parametrization'' (\cbib{San21}).

I'll talk a little about different emulators currently available, such as COMET. Some emulate non-linear power spectra, for example, and several even include massive neutrinos. But this thesis will demonstrate that massive neutrinos can be included into our evolution mapping approach, which will be introduced in section~\ref{sec: ev_mapping}.

% (This is good news because the evolution mapping approach greatly simplifies the parameter space, and enhances the accuracy), which is the subject of the next section.

\begin{centering}
\subsection{Emulation Platform: Gaussian Processes}
\end{centering}

% What is a Gaussian Process?

Most emulators are based on a Gaussian Process (GP). A GP is a Gaussian
distribution over functions\footnote
{A GP is the limit of a one-hidden-layer neural network as the number of
neurons approaches infinity.}, which can be interpreted
as the infinite-dimensional generalization of the multivariate normal
distribution. The inference of continuous values with a GP prior
is known as Gaussian process regression, or Kriging. GP regression is a
powerful non-linear multivariate interpolation tool. The computational
complexity of inference and likelihood evaluation within GP regression is cubic
in the number of points. This makes GP regression an excellent companion to
Latin hypercube sampling (LHS), which makes highly-efficient use of a limited 
number of samples and will be explained in greater depth in section~\ref{sec:
lhc_intro}.

Neural networks (NNs) generally need much larger sample sizes to reach
comparable levels of
accuracy. Due to various alterations in the Cassandra-Linear code over its
development, several regenerations of the various emulator data sets were
necessary. This practical constraint motivated the use of a GP for our
emulator. Furthermore, NNs invariably require much more complicated setup and
tuning--for example, in the precise architecture of the network (e.g. nodes
per layer, layer types) as well as the hyperparameters (e.g. learning rate).
By contrast, as we explain in section~\ref{sec: emu_training}, a Gaussian
process regression is highly straightforward to set up and modify. Therefore,
for a demonstration project such as Cassandra-Linear, we elected to base our
emulator on a GP. Please refer to the section~\ref{sec: future_work} for a
continuation of this discussion.

\begin{centering}
\subsection{Sampling Approach: Latin Hypercube}
\end{centering}

I imagine this is going to be an extremely short section. We should motivate why we're using this style of sampling.

\begin{centering}
\subsection{Evolution Mapping}
\end{centering}

This section will also include an extremely brief summary of Ariel's paper motivating the use of $\sigma_{12}$ instead of $\sigma_8$.

    I have two primary objectives for this section: explain the unit system
    we are using (ditch $h$ factor because it messes up everything--but
    only briefly summarize the main arguments of Sanchez 2020), and briefly
    summarize why we can funnel all of the evolution parameters through
    $\sigma_{12}$ in this way. Unfortunately, this second objective will
    almost certainly require you to bust out a few equations, and even to
    manipulate them a little to tease out relations essential to this paper.

Conventional emulator calibration entails the historical units of Mpc / $h$,
but if we use instead units of Mpc, then we can distill all of the evolution
parameters into one parameter, $\sigma_{12}$. Since $h$ is already its own parameter, the conventional $\sigma_8$ parameter is truly a mixture
of two parameters. This presents a host of misleading results  and statistical
ambiguities (\cbib{San20}) which are outside of the scope of this work but
which prompt us to abandon $\sigma_8$.
Similarly, throughout this paper we will refrain from using the conventional
fractional density parameters $\Omega_i$ in favor of the physical density
parameters $\omega_i = \Omega_i h^2$ which similarly eliminate the
dependence on $h$.

(\cbib{San21}) proposes to divide up the full set of cosmological
parameters into two categories: \textit{evolution} parameters $\mathcal{O}_E$
(such as $\omega_b$, $\omega_c$, and $\eta_s$)
affect the amplitude of the power spectrum at a particular redshift, while
\textit{shape} parameters $\mathcal{O}_S$
(such as $\omega_K$, $\omega_\text{DE}$, w(a))
affect the shape of the power
spectrum.

We take, as the evolution mapping relation for the power spectrum, equation 13
from \cbib{San21}:

\begin{equation}
\label{eq: evMapping_pSpectrum}
    \Delta^2_L (k | z, \Theta_s, \Theta_e)
    =
    \Delta_L^2 (k | \Theta_s, \sigma_{12} \left( z, \Theta_s, \Theta_e \right))
\end{equation}

Why is this scheme important? Evolution mapping greatly simplifies the emulator
implementation. Because we can
funnel all of the evolution parameters through $\sigma_{12}$, we've effectively
collapsed an entire category of parameters to just one parameter. Fewer
parameters means that we get a more accurate emulator.

``At the linear level, all models characterized by identical shape parameters
and the same values of the parameter combinations $b \sigma_{12}(z)$ and
$f \sigma_{12}(z)$ will be identical'' (\cbib{San21}).

Now, for the hiccup, which segues into the next section: this scheme is broken by one parameter, the Universe's
density in neutrinos. (In the next section: why this is so and what we can do
about it.)

%%%

Three shape parameters of core interest to this paper are the physical density in baryons $\omega_b$ (whose impact on the power spectrum is shown in figure~\ref{fig: omega_b_dependence}), the physical density in cold dark matter $\omega_c$ (figure~\ref{fig: omega_c_dependence}), and the spectral index $n_s$, (figure~\ref{fig: ns_dependence}). The remaining parameters $\sigma_{12}$ and $A_s$, as well as the quantities $z$ and $h$, all shift only the amplitude of the power spectrum, as illustrated in figure~\ref{eq: sig12_dependence}. 

For the sake of completion, we also show the impact of the evolution parameter $\sigma_{12}$ in figure~\ref{fig: sig12_dependence} and stress that the other quantities 


\begin{centering}
\subsection{Neutrinos and Their Cosmological Impact}
\end{centering}

(\cbib{Kiakotou}): ``Neutrinos with masses on the eV scale or below will be a
hot component of the dark matter and will free-stream out of overdensities and
thus wipe out small-scale structures.''

``In general, a larger density of relativistic species leads to a smaller
growth of matter fluctuations'' (\cbib{Zennaro}).

The point of this section is: why is $\omega_\nu$ bad for the
evolution mapping scheme? Because neutrinos exhibit redshift-dependent
damping of the power-spectrum, and therefore affect both the shape and the
amplitude of the power spectrum. Whenever massive neutrinos are present,
the growth factor becomes scale-dependent, which disrupts the
evolution-mapping scheme.

Why do they behave in this way? All neutrinos start off as
relativistic particles in the early Universe, acting as a type of radiation.
But as the Universe continues to expand and cool, the neutrinos behave
increasingly like dark matter.
In this way, the physical density in neutrinos impacts both the shape and the
evolution.

``The popular heuristic formula for the linear theory suppression of the matter
fluctuations by free-streaming $\nu$, $\Delta P(k) / P(k) \approx -8 f_\nu$, is
valid only on very small scales $k > 0.8 h$ / Mpc, However, it is not of
practical use as this is in the strongly nonlinear regime of matter
clustering'' (\cbib{Kiakotou}).

One proposed solution is to treat the neutrinos as a small correction factor
to the results from an anologous cosmology with the same $\omega_m$ but with
$\omega_\nu = 0$. This of course limits the applicability of our emulator to
cosmologies with very small $\omega_\nu$, but this constraint agrees with
current observations (\textcolor{orange}{which?}).

I want to end this section with a vague plan of action: we want to play around with CAMB power spectra to see if there are any simple ways around this limitation in our approach.

%%% New stuff

We already have an approximation for the power spectrum of a massive-neutrino cosmology within the evolution mapping scheme. The $\sigma_{12}$ value that we described earlier is actually the $\sigma_{12}$ value of the model's MEMNeC. can already be approximated within evolution-mapping by slightly altering scheme. \textcolor{red}{Is it fair to say we are adjusting, or was this actually the same scheme as it always was?} We take a MEMNeC and the desired cosmology. The sigma 12 is actually the sigma 12 of the MEMNeC. Then we treat the physical density in neutrinos as a shape parameter along with $A_s$.

\begin{centering}
\section{CAMB, Initial Setup}
\end{centering}

CAMB is a Fortran code with a Python wrapper\footnote{
\url{https://github.com/cmbant/CAMB}
}which we will be using for the
entirety of this project.

To introduce the reader to the scope of CAMB, we will now introduce
some basic simulated power spectra along with a summary of the dynamic
parameters which will be of greatest interest to us.

I hope to, in painstaking detail, cover many of the lines of the code that I have written to interface with CAMB. I will include plots to indicate, at every step, what incorrect settings cause the power spectrum to look like (or, for subtler errors, what the error curves looked like compared to Ariel's results, which I treated as a sort of "ground truth"). This should also be a good example to flex my physics interpretation skills: why does this incorrect setting produce this undesired pattern?

You might think that this is sort of an inappropriate section for a master's thesis (especially since I have in mind that this be a lengthy section), but I would like to include it unless you feel very strongly. After all, I spent several months of the project debugging at least ten different ways that slight and major errors in the various settings led to irreconcilable results.

For example, one parameter that tripped me up for a while: neutrino mass hierarchy: the options are degenerate, normal, and inverted. The CAMB documentation annotates this parameter as ``(1 or 2 eigenstate approximation),'' but this is somewhat unclear. Is the degenerate hierarchy the single mass eigenstate approximation? Do both normal and inverted hierarchies involve two eigenstates?

%In figure \ref{fig: spectrum_type}, we can see that requesting of the wrong
%power spectrum type can in some low-$\omega_\nu$ cases yields errors so low
%that we might accidentally overlook them. This error pattern is easily
%recognizable and is a consequence of the definition of the power spectrum: the
%Fourier transform  of the two-point correlation function. ...Okay, I'm still thinking about this. I don't understand %yet, but I'll be sure to ask you if I'm still struggling about it.

Another paragraph I want to have in this section: stress the part of the evolution mapping introduction, that the $\sigma_{12}$ value we're using to describe the model is actually the $\sigma_{12}$ value of the model's MEMNeC! This is so important and confusing that maybe I'll even recapitulate again later in the section on the generate\_emu\_data.script.

%%% New stuff

Beware the neutrino settings. The effective number of massive neutrinos is about 3.027

\begin{centering}
\section{Expansion of the Parameter Space}
\end{centering}

Here, I will talk about the playing-around that we did, to discover that $A_s$ assisted in the prediction of the suppression due to massive neutrinos.

Remember to explain \textit{why} the prediction of these asymptotes means that $A_s$ will capture all of the unruly behavior of neutrinos. What theory motivates such a judgment? Or are we only guessing?

\begin{centering}
\section{Cassandra-Linear: a Python Package}
\end{centering}

This will be a rather long, dry, and technical section with subsections based on each of the core scripts making up the Python package that I have been developing. It will in some ways paraphrase and summarize the documentation, explaining the basic use of the package as well as important limitations. Of course, fairly early on in this section, there will be a footnote linking to my GitHub repository, which will be made public once I'm ready to hand in this thesis.

Even if my code doesn't end up in a bigger repository, I nevertheless think that it's important to the scientific process that I describe in detail the code that I have written. Besides, if any readers want to experiment specifically with the ideas discussed in this thesis, they may find my code more accessible because it is, in a sense, "single-purpose"--that is to say, written almost exclusively to investigate the topics of this paper.

It only takes a couple of sentences, but we also need to describe how to install the package.

\textcolor{red}{Change the subsection titles, they should describe the content, not the script names}

\begin{centering}
\subsection{Building the Latin Hypercube}
\label{sec: build_lhc}
\end{centering}

% lhc.py

This section will briefly recapitulate some of the latin hypercube sampling explanation from the introduction. Then we will talk about how this sample is procured in practice, and what our ``solution'' (inelegant though it may be) is to obtaining a sample with a decent minimum separation between the points (remember to briefly repeat why this is important).

We've multithreaded a random LHC generator to allocate a large amount of compute to this brute force approach.

We'll also describe the switch from a rescaled latin hypercube to a unit latin hypercube that we later interpret differently according to the specified priors.

WE SHOULD INCLUDE SOME PLOTS: LOOK AT HOW MANY FUNCTION CALLS IT TAKES TO GET
TO MINIMUM SEPARATION X.

%! Maybe we should move get_param_ranges to lhc.py. It would be more thematically consistent if the note on priors appeared in this section

The priors that we use correspond to those currently used by COMET. \textcolor{orange}{Should I spend any time defending this choice here, or may I put all of the defense in section~\ref{sec: priors}?}

\begin{centering}
\subsection{Interfacing with CAMB}
\end{centering}

% camb\_interface.py

This section won't actually introduce much that the reader hasn't already encountered in the section "CAMB, initial setup." This will primarily be a summary of the ``correct'' settings with a couple of new sections describing the particular functions in this script.

\begin{centering}
\subsection{Integrating Evolution Mapping}
\end{centering}

% generate\_emu\_data.py

This section will describe the process going from a latin hypercube to a set of CAMB power spectra. We will describe how an array of six parameter values is fleshed out into a params object understood by CAMB. We will describe the procedure of modifying $h$ and $z$ until we arrive roughly at the $\sigma_{12}$ value that we desire, as well as the process of writing the \textit{actual} $\sigma_{12}$ value that we obtained back to the original hypercube. Why is this important? Because the latin hypercube is used again later in the user\_interface.py script for the purpose of training the Gaussian emulator object.

\begin{centering}
\subsection{Training the Emulator}
\label{sec: emu_training}
\end{centering}

% train\_emu.py

This section will focus on the particular lines of GPy that we used, as well
as the various data-cleaning and normalization statements that we used.
Normalization will be a really important topic. I want to explain some of
the theory behind why the emulator performs poorly with values outside of this
[0, 1] range.

\begin{centering}
\subsection{Accessing and Using the Emulator}
\end{centering}

% user\_interface.py

The content of this section is still relatively uncertain. Since I am still tweaking some of the emulator's settings, I haven't spent too much time on a script dedicated purely to simplifying the interaction between the user and the emulator \textit{object} itself.

Anyway, the hope is to provide some clear and simple descriptions of what functions the user should turn to in order to get started predicting power spectra using the results of this thesis. The functions can either create a new emulator object and train it or load an existing one with easy-to-understand functions for interacting with the object.

Here, we will talk about the details of the training as well as the importance of normalization. Since this is still technical, it may not be appropriate for a UI script. So, I am considering breaking out a new script (and therefore a new section), maybe called ``train\_emu.py.''

\begin{centering}
\section{Results and Analysis}
\end{centering}

\begin{centering}
\subsection{Creation of Separate Emulators}
\end{centering}

This is a brief introductory section motivating the creation of two emulators: unlike the other five parameters, the physical density in massive neutrinos has a hard lower bound. This creates some minor regression problems: TO PROVE THIS, SHOW THE PERFORMANCE OF THE MASSIVE EMULATOR ON MASSLESS-NEUTRINO COSMOLOGIES.

Conclude this section by saying that this integration of multiple emulators into one user-facing script can be further exploited with, for example, different emulators for different neutrino mass hierarchies.

\begin{centering}
\subsection{Emulation over Uncertainties}
\end{centering}

For a further step of accuracy, we can add a third data set to our pipeline
and introduce a second layer of emulation.

Up to this point, our pipeline has included a training set and a testing set.
If we add a validation set, then we can train a second emulator over the
errors associated with the first emulator's performance on this validation
set.

\begin{centering}
\subsection{Quantifying the Performance of the Emulator}
\end{centering}

Before I even show any results for this emulator, I would like to motivate the challenge of gauging the accuracy and reliability of the emulator. Since we have to quantify performance over a hypervolume of parameter space, there is no concept of, for example, a chi-squared test that can applied here. So I am trying to pre-empt objections like those raised by Stella.

I would like to include a brief review of the relevant literature to explain the kinds of performance metrics that are most commonly applied in emulator papers (as I remember, largely percent-level bounds when comparing against ground truth data sets). Then, I would like to break down Ariel's arguments for why absolute error is of greater consequence to us than relative errors.

For the purpose of this paper, we use the weak and approximate performance metric of simply generating a large number of additional CAMB spectra and comparing them to the predictions of the emulator.

\begin{centering}
\subsection{Percent versus Absolute Errors on Random Cosmologies}
\end{centering}

This will be a fairly short section, basically just showing the plot of 5000 error curves in these two ways. I may focus in on different k-ranges, but the error curves are currently quite flat, so I don't think that would be a good use of space.

Furthermore, I will try to select a couple of cosmologies out of the 5000 (maybe a low-error case and a high-error case) as examples of the performance on just one at a time. But I'm not sure how insightful that will be, I don't know if that will tell the reader a whole lot.

\begin{centering}
\subsection{Performance in Different Parameters}
\end{centering}

Here, I will either include color plots or, as Dante suggests, monochrome plots with error as one axis and parameter value as the other (i.e. $k$ fixed). Then, if I haven't tightened the $\sigma_{12}$ performance by the time of submission, I can talk about how this is the most promising avenue for refinement of the emulator. In any case, I plan to spend some time talking about \textit{why} parameter x is the current biggest problem for the emulator. 

\begin{centering}
\section{Discussion and Conclusion}
\end{centering}

\begin{centering}
\subsection{Tightness of the Priors Used}
\label{sec: priors}
\end{centering}

With this section I would like to revisit the specific values for the priors,
that I only briefly mentioned back in section~\ref{sec: build_lhc}.

First of all, from a purely practical consideration, expanding the priors was
not feasible due to the high incidence of unsolvable cells.

But second, this may not be a significant limitation to the utility of the
emulators introduced here, because they are already quite wide compared to
current state-of-the-art parameter inferences. \textcolor{green}{CITATIONS}.

\begin{centering}
\subsection{Resolution of the k Axis}
\end{centering}

This might go better in the CassL section, but I think I ought to motivate the decision to use length-300 arrays.

\begin{centering}
\subsection{Number of Training Samples}
\end{centering}

This might go better in the CassL section, but I think I ought to motivate the decision to use 5000 training arrays.

I'll have to concede that the results of this section are not entirely comprehensive; we didn't train any emulators over the uncertainties of analogous validation hypercubes. All comparisons here use the simpler pipeline of just two data sets, training and testing.

\begin{centering}
\subsection{Linear Sampling in Different Parameters}
\end{centering}

We also tried sampling in $\sigma_{12}^2$ as well as $\sqrt{\sigma_{12}}$.
Unfortunately, we were unable to conclude anything about the effectiveness of
these strategies--there appears to have been some mistake in our code, such
that the errors are much larger than can be explained on account of poor
sampling.

See figures~\ref{fig: sigsquare_sample} and~\ref{fig: sigroot_sample} for
illustrations of the problem. In a future work, it would be helpful to
investigate these problems further. We may find that a different sampling
strategy will more efficiently reduce the deltas that we see in our emulator.

\begin{centering}
\subsection{Summary of the Paper}
\end{centering}

What was the main objective of this thesis? What were the key results of this work? Why are they important? How do these results compare with results from papers on similar subjects?

\begin{centering}
\subsection{Future Work}
\label{sec: future_work}
\end{centering}

Here I will talk about what kinds of questions we estimate will be most fruitful for further inquiries about this topic and this code.

Is there a theoretically perfect LHC generator?

In order to expand the priors, it would be helpful to investigate why CAMB does not allow negative redshifts, in case this can be adapted. Alternatively, CLASS \textcolor{red}{allows} negative redshifts, so it may be worthwhile to repeat the work of this thesis using power spectra from CLASS. This would involve familiarization with a new platform, however, and so this escapes the scope of this thesis. Remember that negative redshifts would be a helpful feature for this work because it would allow us to investigate much broader priors.

Should we use a neural network instead of a Gaussian process for our emulator? AndreaP and Alex Eggemeier are already on the job: they are converting COMET to a neural network approach. We recommend that the reader follow future COMET papers for investigations into this question.

GP's allow natural propagation of
uncertainty in predictions to the final posterior distribution; neural
networks lack this feature. At the same time, NNs provide larger speedups \textcolor{green}{CITATIONS}.

\IfFileExists{biblatex.sty} {
    \printbibliography
}

\end{document}
