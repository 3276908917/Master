What is the emulator supposed to do?

When you input a cosmology, you should get a linear power spectrum back out.

For the training phase, we need a large number of linear power spectra
evaluated at the different locations in a LH.

Discrete problems to solve:
1. What are all the parameters that I absolutely need?
    a. Sigma 12 (i.e. all evolution parameters)
        Do I even need this? Or can I just post-hoc implement the evolution mapping idea?
    b. Shape parameters
2. What ranges do we want to allow for each parameter?
3. How do we implement a distance-maximizing algorithm to give us the optimal
    LH for this particular combination of parameters and ranges?
4. Now that we have our LH of samples, how do we train the GP on it?
5. Now that we have a trained GP, how do we get predictions for an arbitrary
    configuration of input parameters?
6. How do we correct the results for the presence and mass of neutrinos?

Unfortunately, even if we get the whole pipeline complete, the trained emulator will yield
wrong results so long as we train on spectra from our incorrect CAMB setup.

Work-arounds:
1. You could make up a small list, say {om_b, om_c, h} and hold everything else
    to a Lambda-CDM model
2. You could make up ranges by setting min = value - 10% and max = value + 10%.
3. You could potentially set up a non-optimal LH just as a proof of concept.
    Maybe divide the ranges up into a grid and try throwing darts at a board.
4. Training with some particular set up of samples should only require some documentation-reading.
5. Ditto
6. That's your job!

Status as of 15:42 on January 10, 2023: (using work-arounds i.e. proof of concept, rather than full solution)
1. Done!
2. Done!
3. Done! It seems like the CAKE21 workshop already handled looking for optimal hypercubes,
    although the approach that they used is possibly inefficient
    (is the best solution really to just regenerate the cube a bunch of times?)

    -> Let's ask Ariel.

4. This is the next step. We have all of the samples and now we want to train a GP
    to find the relationship between the parameter values and the power spectrum.

But not so fast!
    I think that our current setup is probably not going to work very well, since we would have to train
the GP to also predict the k-axis. That would not only waste a lot of compute but it would achieve
nothing, since we assign no intrinsic meaning to the particular k-axis returned by CAMB.
    Instead, I propose the following: let's arbitrarily pick some baseline power spectrum (maybe just
first one in the list?) and use its k-axis to interpolate all of the other power spectra. This shouldn't
be too hard since we already have code for this in a few of our Jupyter notebooks--all we need to change
is scaling it up to interpolate all ninety-nine remaining spectra.
    Once we've accomplished that, the k-axis is now worthless to the GP. Of course, we want to hold onto
it, because it will allow us to plot the GP's final predictions. But the k-axis data will play no role
in the training of the GP.
    Once we've accomplished all of the above, we want to study the GPy documentation until we understand
how to get the computer to learn a mapping from a triple (omega_b, omega_c, h) to a length-10000 array
consisting of the power spectrum evaluated at different k.
