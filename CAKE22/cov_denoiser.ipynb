{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc4f7cf",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53aead2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukas\\anaconda3\\envs\\lmu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\lukas\\anaconda3\\envs\\lmu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\lukas\\anaconda3\\envs\\lmu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\lukas\\anaconda3\\envs\\lmu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\lukas\\anaconda3\\envs\\lmu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\lukas\\anaconda3\\envs\\lmu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\lukas\\anaconda3\\envs\\lmu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\lukas\\anaconda3\\envs\\lmu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\lukas\\anaconda3\\envs\\lmu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\lukas\\anaconda3\\envs\\lmu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\lukas\\anaconda3\\envs\\lmu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\lukas\\anaconda3\\envs\\lmu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edac0c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(ns = 50, N_train = 70, N_test = 29, cov_len = 126):\n",
    "    \"\"\"\n",
    "    Load the covariance matrices\n",
    "    \"\"\"\n",
    "    name_root = \"samples/ns{}/xiell_cov_noisy_ns{}_\".format(ns, ns)\n",
    "    \n",
    "    train_data = np.zeros((N_train, cov_len, cov_len))\n",
    "    test_data = np.zeros((N_test, cov_len, cov_len))\n",
    "    \n",
    "    for i in range(N_train):\n",
    "        train_data[i] = np.loadtxt(name_root+\"{:04d}.dat\".format(i+1))\n",
    "    for i in range(N_test):\n",
    "        test_data[i] = np.loadtxt(name_root+\"{:04d}.dat\".format(N_train+i+1))\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f94b45c-d587-4b91-9680-6676d4d8a47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData_tri(ns = 50, N_train = 60, N_val = 20, N_test = 19, cov_len = 126):\n",
    "    \"\"\"\n",
    "    Load the covariance matrices\n",
    "    \"\"\"\n",
    "    name_root = \"samples/ns{}/xiell_cov_noisy_ns{}_\".format(ns, ns)\n",
    "    \n",
    "    train_data = np.zeros((N_train, cov_len, cov_len))\n",
    "    val_data = np.zeros((N_val, cov_len, cov_len))\n",
    "    test_data = np.zeros((N_test, cov_len, cov_len))\n",
    "    \n",
    "    for i in range(N_train):\n",
    "        train_data[i] = np.loadtxt(name_root+\"{:04d}.dat\".format(i+1))\n",
    "    for i in range(N_val):\n",
    "        val_data[i] = np.loadtxt(name_root+\"{:04d}.dat\".format(N_train+i+1))\n",
    "    for i in range(N_test):\n",
    "        test_data[i] = np.loadtxt(name_root+\"{:04d}.dat\".format(N_train+N_val+i+1))\n",
    "    \n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d46fa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_cov(array, theory_cov):\n",
    "    array = copy.deepcopy(array)\n",
    "    cov_len = len(theory_cov)\n",
    "    for i in range(cov_len):\n",
    "        for j in range(cov_len):\n",
    "            array[:, i, j] /= np.sqrt(theory_cov[i, i] * theory_cov[j, j])\n",
    "    array = np.reshape(array, (len(array), cov_len, cov_len, 1))\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3d6d0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_alpha(covs, theory_cov):\n",
    "    \"\"\"\n",
    "    Normalizes the supplied array and reshapes it into the appropriate format.\n",
    "    \"\"\"\n",
    "    covs_norm = []\n",
    "    for cov in covs:\n",
    "      theory_diag = np.diagonal(theory_cov)\n",
    "      cov_ii = cov/np.sqrt(theory_diag)\n",
    "      cov_jj = np.transpose(cov_ii)/np.sqrt(theory_diag)\n",
    "      covs_norm.append(np.transpose(cov_jj))\n",
    "\n",
    "    return np.array(covs_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "001ee102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_theory(theory_cov, N = 70):\n",
    "    cov_len = len(theory_cov)\n",
    "    array = np.zeros((N, cov_len, cov_len))\n",
    "    for i in range(cov_len):\n",
    "        for j in range(cov_len):\n",
    "            array[:, i, j] = theory_cov[i, j] / np.sqrt(theory_cov[i, i] * theory_cov[j, j])\n",
    "    array = np.reshape(array, (len(array), cov_len, cov_len, 1))\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbb8013",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccaaa659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4000 averages of 50\n",
    "# 4000 averages of 1000\n",
    "\n",
    "# Since we only need images from the dataset to encode and decode, we\n",
    "# won't use the labels.\n",
    "\n",
    "# 2000 training, 1000 validation, 1000 test\n",
    "_train_noisy, _val_noisy, _test_noisy = \\\n",
    "    loadData_tri(ns=50, N_train=2000, N_val=1000, N_test=1000)\n",
    "_train_target, _val_target, _test_target = \\\n",
    "    loadData_tri(ns=1000, N_train=2000,  N_val=1000, N_test=1000)\n",
    "\n",
    "theory_cov = np.loadtxt(\"samples2/xi_new_boss_zs_z0p61_lin_ximulti_covar.dat\")\n",
    "\n",
    "# Normalize and reshape the data\n",
    "train_noisy = preprocess_cov(_train_noisy, theory_cov=theory_cov)\n",
    "val_noisy = preprocess_cov(_val_noisy, theory_cov=theory_cov)\n",
    "\n",
    "train_target = preprocess_cov(_train_target, theory_cov=theory_cov)\n",
    "val_target = preprocess_cov(_val_target, theory_cov=theory_cov)\n",
    "\n",
    "#target_data = preprocess_theory(theory_cov)\n",
    "\n",
    "# Display the train data and a version of it with added noise\n",
    "# display(train_data, train_data)\n",
    "\n",
    "cov_len = len(theory_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78957b33",
   "metadata": {},
   "source": [
    "## Build the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2528d158-c345-497e-9159-3c2b0541344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Technically Keras already provides this,\n",
    "but the API is a mess when it comes to\n",
    "the layer / activation function distinction\"\"\"\n",
    "from tensorflow.keras import backend as K\n",
    "def ReLU(x):\n",
    "    return K.relu(x, alpha=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff32614e-11a4-4f5c-91f0-f9d12cd120c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Aliases to keep the code compact\n",
    "universal_kernel = (3, 3)\n",
    "universal_padding = \"same\"\n",
    "universal_dropRate = 0.05\n",
    "universal_numStrides = 1\n",
    "\n",
    "def conv(in_, filters, activation_in):\n",
    "    return layers.Conv2D(filters, universal_kernel,\n",
    "        activation=activation_in, padding=universal_padding)(in_)\n",
    "\n",
    "def tConv(in_, filters, activation_in):\n",
    "    return layers.Conv2DTranspose(filters, universal_kernel,\n",
    "        strides=universal_numStrides, activation=activation_in,\n",
    "        padding=universal_padding)(in_)\n",
    "\n",
    "def drop(in_):\n",
    "    return tf.keras.layers.Dropout(universal_dropRate)(in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c89170f-6698-4863-ae35-f435523f731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = layers.Input(shape=(cov_len, cov_len, 1))\n",
    "\n",
    "def get_autoencoders(function):\n",
    "    print(\"Initializing self autoencoder...\")\n",
    "    \n",
    "    a_self = Model(input_, function)\n",
    "    a_self.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "\n",
    "    a_self.fit(\n",
    "        x=train_noisy,\n",
    "        y=train_noisy,\n",
    "        epochs=5,\n",
    "        batch_size=15,\n",
    "        shuffle=True,\n",
    "        validation_data=(val_noisy, val_noisy),\n",
    "    )\n",
    "    \n",
    "    print(\"Initializing target autoencoder...\")\n",
    "    \n",
    "    a_target = Model(input_, function)\n",
    "    a_target.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "    \n",
    "    a_target.fit(\n",
    "        x=train_noisy,\n",
    "        y=train_target,\n",
    "        epochs=5,\n",
    "        batch_size=15,\n",
    "        shuffle=True,\n",
    "        validation_data=(val_noisy, val_targete),\n",
    "    )\n",
    "    \n",
    "    return a_self, a_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d2ac5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lukas\\anaconda3\\envs\\lmu\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "\n",
      "\n",
      "Old function:\n",
      "Initializing self autoencoder...\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/5\n",
      "2000/2000 [==============================] - 189s 95ms/sample - loss: 0.0784 - val_loss: 0.0709\n",
      "Epoch 2/5\n",
      "2000/2000 [==============================] - 181s 90ms/sample - loss: 0.0704 - val_loss: 0.0699\n",
      "Epoch 3/5\n",
      "2000/2000 [==============================] - 176s 88ms/sample - loss: 0.0719 - val_loss: 0.0711\n",
      "Epoch 4/5\n",
      "2000/2000 [==============================] - 176s 88ms/sample - loss: 0.0709 - val_loss: 0.0711\n",
      "Epoch 5/5\n",
      "1530/2000 [=====================>........] - ETA: 37s - loss: 0.0711"
     ]
    }
   ],
   "source": [
    "def build_layers(inner_act, last_act, drops=True):\n",
    "    # Encoder\n",
    "    x = conv(input_, 64, last_act)\n",
    "    if drops:\n",
    "        x = drop(x)\n",
    "    x = conv(x, 32, inner_act)\n",
    "    if drops:\n",
    "        x = drop(x)\n",
    "    # Decoder\n",
    "    x = tConv(x, 32, inner_act)\n",
    "    if drops:\n",
    "        x = drop(x)\n",
    "    x = tConv(x, 64, inner_act)\n",
    "    if drops:\n",
    "        x = drop(x)\n",
    "    x = conv(x, 1, last_act)\n",
    "    return x\n",
    "\n",
    "old = build_layers(\"relu\", \"sigmoid\", drops=False)\n",
    "paper = build_layers(ReLU, \"tanh\", drops=True)\n",
    "nondropper = build_layers(ReLU, \"tanh\", drops=False)\n",
    "sigmoidLast = build_layers(ReLU, \"sigmoid\", drops=True)\n",
    "nonleaker = build_layers(\"relu\", \"tanh\", drops=True)\n",
    "\n",
    "# Autoencoder\n",
    "print(\"\\n\\nOld function:\")\n",
    "a_old_self, a_old_target = get_autoencoders(old)\n",
    "\n",
    "print(\"\\n\\nPaper's function:\")\n",
    "a_paper_self, a_paper_target = get_autoencoders(paper)\n",
    "\n",
    "print(\"\\n\\nNondropper function:\")\n",
    "a_nondropper_self, a_nondropper_target = get_autoencoders(nondropper)\n",
    "\n",
    "print(\"\\n\\nSigmoid as last activation function:\")\n",
    "a_sigmoidLast_self, a_sigmoidLast_target = get_autoencoders(sigmoidLast)\n",
    "\n",
    "print(\"\\n\\nNonleaker function:\")\n",
    "a_nonleaker_self, a_nonleaker_target = get_autoencoders(nonleaker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a5a1be",
   "metadata": {},
   "source": [
    "## Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6337a45c-e212-4db2-acb6-1a3b858dda01",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = a_paper_self.predict(train_data)\n",
    "# display(test_data, predictions, n=2)\n",
    "plt.imshow(predictions[0].reshape(cov_len, cov_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d227db",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = a_paper_target.predict(val_data)\n",
    "plt.imshow(predictions[0].reshape(cov_len, cov_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81634c1-7fac-4d89-bd7e-fc40b7df7f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "23262661808f7a123023321b882c61e6062caf1b3faf8a2c27cc43e1b4060f86"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
